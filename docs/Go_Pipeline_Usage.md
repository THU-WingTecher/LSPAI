# Running Go Test Pipeline on AI-Generated Tests

## Overview

This guide explains how to run the complete test pipeline on Go test files generated by AI tools (like DeepSeek, GPT-4, etc.). The pipeline handles directories that contain both test files (`*_test.go`) and source files (`*.go`) mixed together.

## Quick Start

### Prerequisites

1. **Go toolchain installed**
   ```bash
   go version  # Should show go1.15 or later
   ```

2. **Test directory structure**
   - Directory containing Go test files: `*_test.go`
   - May also contain source files: `*.go` (will be filtered out)
   - Must have `go.mod` file in the directory

3. **Test file map** (JSON file mapping tests to source files)
   ```json
   {
     "test_file_name.go": {
       "project_name": "cobra",
       "file_name": "source_file.go",
       "symbol_name": "FunctionName"
     }
   }
   ```

## Example: Cobra Project

### Directory Structure

```
/LSPRAG/experiments/data/main_result/cobra/.../final-clean/
├── go.mod                                    # Go module file
├── args.go                                   # Source file (will be ignored)
├── bash_completions.go                       # Source file (will be ignored)
├── args_LegacyArgs_2388_test.go             # Test file ✓
├── args_OnlyValidArgs_9597_test.go          # Test file ✓
├── bash_completions_WriteRequiredFlag_3200_test.go  # Test file ✓
└── ...                                       # More files
```

**Statistics:**
- 40 test files (`*_test.go`)
- 19 source files (`*.go`)
- **Collector automatically filters**: Only the 40 test files will be executed

### Configuration

```typescript
const testsDir = '/path/to/cobra/final-clean';
const outputDir = '/path/to/output';
const testFileMapPath = '/path/to/cobra_test_file_map.json';
const projectPath = '/path/to/cobra/final-clean';
```

### Running the Pipeline

#### Method 1: Integration Test (Recommended)

```bash
cd /LSPRAG
npm run compile
npm test -- --grep "EXECUTE - Go \(Cobra\)"
```

This will:
1. ✓ Check if Go is installed
2. ✓ Verify test directory exists
3. ✓ Verify test file map exists
4. ✓ Filter test files from source files automatically
5. ✓ Run all test files with GoExecutor
6. ✓ Analyze results
7. ✓ Generate reports

#### Method 2: Programmatic Usage

```typescript
import { runPipeline } from './ut_runner/runner';
import { getConfigInstance } from './config';

// Set workspace
getConfigInstance().updateConfig({
  workspace: projectPath,
});

// Run pipeline
await runPipeline(testsDir, outputDir, testFileMapPath, {
  language: 'go',
  include: ['*_test.go'], // Only collect test files
  timeoutSec: 30,         // 30 second timeout per test
  jobs: 4,                // Run 4 tests concurrently
});
```

## How It Works

### Phase 1: Collection
The `Collector` class automatically filters test files:

```typescript
// Collector defaults for Go language
includePatterns: ['*_test.go']
```

- ✓ Matches: `args_LegacyArgs_2388_test.go`
- ✗ Ignores: `args.go`, `bash_completions.go`

### Phase 2: Execution
The `GoExecutor` runs each test file:

```typescript
{
  logsDir: outputDir + '/logs',
  timeout: 30,
  cleanCache: false,
  verbose: true,
}
```

For each test file:
1. Find module root (locates `go.mod`)
2. Extract test names (`Test*`, `Benchmark*`, `Example*`)
3. Run `go test -json -count=1 -v`
4. Capture JSON output
5. Validate and detect issues

### Phase 3: Analysis
The `Analyzer` processes test results:
- Parse Go test JSON output
- Map tests to source files (using test file map)
- Classify errors (build errors, panics, failures)
- Generate statistics

### Phase 4: Output
The `Writer` produces reports:
- `logs/*.log` - Individual test logs
- `test_results.json` - All test cases with status
- `file_results.json` - Per-file statistics
- `pytest_output.log` - Unified log

## Output Example

### test_results.json
```json
{
  "tests": {
    "cobra_test.go::TestArgsValidation": {
      "code_name": "cobra_test.go::TestArgsValidation",
      "status": "Passed",
      "error_type": null,
      "detail": "",
      "test_file": "/path/to/cobra_test.go",
      "log_path": "/path/to/logs/cobra_test.go.log"
    }
  }
}
```

### file_results.json
```json
{
  "files": {
    "/path/to/args_test.go": {
      "counts": {
        "Passed": 3,
        "Assertion Errors": 1,
        "Runtime Errors": 0
      },
      "testcases": [...],
      "note": "Matched source: args.go"
    }
  }
}
```

### Log File Format
```
================================================================================
GO TEST EXECUTION LOG
================================================================================
Test File:        /path/to/args_test.go
Language:         go
Module Root:      /path/to/cobra
Started:          2024-10-13 12:00:00
Timeout:          30s
Command:          go test -json -count=1 -v ./
================================================================================

{"Time":"...","Action":"run","Package":"cobra","Test":"TestArgsValidation"}
{"Time":"...","Action":"pass","Package":"cobra","Test":"TestArgsValidation"}

================================================================================
EXECUTION SUMMARY
================================================================================
Exit Code:        0
Duration:         123ms (0.12s)
Status:           SUCCESS
================================================================================
```

## Key Features

### Automatic File Filtering ✅
- Only test files (`*_test.go`) are executed
- Source files (`*.go`) are automatically ignored
- Uses glob pattern matching on filename

### Robust Execution ✅
- Module root detection (finds `go.mod`)
- Test name extraction (Test*, Benchmark*, Example*)
- Timeout handling per test
- Concurrent execution (configurable jobs)
- Build cache cleanup (optional)

### Comprehensive Logging ✅
- Structured log format
- JSON output validation
- Error detection (build errors, panics, timeouts)
- Post-execution validation

### Test Mapping ✅
- Maps test files to source files
- Tracks which function is being tested
- Supports multi-file projects

## Troubleshooting

### Issue: "No test files found"

**Cause**: Test files don't match the pattern

**Solution**: Check file naming:
```bash
# Should end with _test.go
ls /path/to/tests/*_test.go

# Update include pattern if needed
include: ['*_test.go', 'test_*.go']
```

### Issue: "go.mod not found"

**Cause**: Test directory lacks Go module

**Solution**: Create go.mod in test directory:
```bash
cd /path/to/tests
cat go.mod  # Check if it exists

# If missing, tests won't compile
```

### Issue: "Build errors in tests"

**Cause**: Test files have compilation errors

**Solution**: Check individual logs:
```bash
cat outputDir/logs/failing_test.go.log

# Look for build errors like:
# undefined: SomeFunction
# cannot use X (type Y) as type Z
```

### Issue: "Some tests timeout"

**Cause**: Tests run longer than timeout limit

**Solution**: Increase timeout:
```typescript
await runPipeline(testsDir, outputDir, testFileMapPath, {
  language: 'go',
  timeoutSec: 60, // Increase from 30 to 60 seconds
  jobs: 4,
});
```

## Best Practices

### 1. Start with Small Batch
Test with a few files first:
```bash
# Copy a few test files to a test directory
mkdir /tmp/test-run
cp /path/to/tests/file1_test.go /tmp/test-run/
cp /path/to/tests/go.mod /tmp/test-run/

# Run pipeline on small batch
```

### 2. Use Appropriate Timeout
- Simple unit tests: 10-30 seconds
- Integration tests: 30-60 seconds
- Complex tests: 60-120 seconds

### 3. Adjust Concurrency
- Fast tests: jobs = 8-16
- Memory-intensive tests: jobs = 2-4
- Mixed: jobs = 4-8

### 4. Monitor Resource Usage
```bash
# While tests are running
top  # Monitor CPU and memory
du -sh outputDir/logs  # Monitor log size
```

### 5. Review Logs Regularly
```bash
# Check for patterns in failures
grep -r "panic:" outputDir/logs/
grep -r "error:" outputDir/logs/
grep -r "undefined:" outputDir/logs/
```

## Performance Tips

### Reduce Execution Time
1. **Increase concurrency**: `jobs: 8`
2. **Reduce timeout**: `timeoutSec: 15`
3. **Skip slow tests**: Filter out known slow tests
4. **Use cache**: Don't clean build cache on every run

### Reduce Disk Usage
1. **Cleanup old runs**: Remove old output directories
2. **Compress logs**: `tar -czf logs.tar.gz outputDir/logs/`
3. **Filter verbose output**: Reduce logging level

## Next Steps

After running the pipeline:

1. **Analyze Results**
   ```bash
   # View summary
   cat outputDir/file_results.json | jq '.files | length'
   
   # Count passed/failed
   cat outputDir/test_results.json | jq '[.tests[] | .status] | group_by(.) | map({status: .[0], count: length})'
   ```

2. **Fix Failing Tests**
   - Review logs in `outputDir/logs/`
   - Fix build errors first
   - Then fix runtime errors
   - Finally fix assertion errors

3. **Iterate**
   - Re-run pipeline after fixes
   - Compare results
   - Track improvements

## Related Documentation

- [GoExecutor Guide](./GoExecutor.md) - Detailed executor documentation
- [GoExecutor Quick Start](./GoExecutor_QuickStart.md) - Quick start guide
- [Implementation Summary](../IMPLEMENTATION_SUMMARY.md) - Technical details

## Example Test Execution

### Cobra Project Results

```
================================================================================
Cobra Go Tests - Pipeline Execution
================================================================================
  Tests Directory: /LSPRAG/experiments/.../final-clean
  Output Directory: /LSPRAG/experiments/.../pipeline-output

Directory contents:
  Test files (*_test.go): 40
  Source files (*.go): 19
  Total Go files: 59

Note: Collector will automatically filter and only run 40 test files

================================================================================
Starting Pipeline Execution
================================================================================
[RUNNER] Phase 1: Collecting test files
[RUNNER] Found 40 test files
[RUNNER] Phase 2: Analyzing cache
[RUNNER] Tests to run: 40
[RUNNER] Phase 3: Test execution
[EXECUTOR][GO] Executing 40 test files (jobs: 4)
...
[RUNNER] Phase 4: Analysis
[ANALYZER] Analyzed 120 test cases from 40 test files
[RUNNER] Phase 5: Writing results

================================================================================
Pipeline Execution Complete
================================================================================
Total execution time: 45000ms (45.00s)

Overall Statistics:
  Passed: 85
  Failed: 25
  Errors: 10
  Total: 120
  Success Rate: 70.8%
```

---

**Status**: ✅ Ready for Use
**Last Updated**: October 13, 2025

