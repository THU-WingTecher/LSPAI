[
    {
      "symbolName": "from_configuration",
      "sourceCode": "@classmethod\n    def from_configuration(\n        cls, *, check: bool, diff: bool, color: bool = False\n    ) -> \"WriteBack\":\n        if check and not diff:\n            return cls.CHECK\n\n        if diff and color:\n            return cls.COLOR_DIFF\n\n        return cls.DIFF if diff else cls.YES",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 10,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "read_pyproject_toml",
      "sourceCode": "def read_pyproject_toml(\n    ctx: click.Context, param: click.Parameter, value: Optional[str]\n) -> Optional[str]:\n    \"\"\"Inject Black configuration from \"pyproject.toml\" into defaults in `ctx`.\n\n    Returns the path to a successfully found and read configuration file, None\n    otherwise.\n    \"\"\"\n    if not value:\n        value = find_pyproject_toml(\n            ctx.params.get(\"src\", ()), ctx.params.get(\"stdin_filename\", None)\n        )\n        if value is None:\n            return None\n\n    try:\n        config = parse_pyproject_toml(value)\n    except (OSError, ValueError) as e:\n        raise click.FileError(\n            filename=value, hint=f\"Error reading configuration file: {e}\"\n        ) from None\n\n    if not config:\n        return None\n    else:\n        spellcheck_pyproject_toml_keys(ctx, list(config), value)\n        # Sanitize the values to be Click friendly. For more information please see:\n        # https://github.com/psf/black/issues/1458\n        # https://github.com/pallets/click/issues/1567\n        config = {\n            k: str(v) if not isinstance(v, (list, dict)) else v\n            for k, v in config.items()\n        }\n\n    target_version = config.get(\"target_version\")\n    if target_version is not None and not isinstance(target_version, list):\n        raise click.BadOptionUsage(\n            \"target-version\", \"Config key target-version must be a list\"\n        )\n\n    exclude = config.get(\"exclude\")\n    if exclude is not None and not isinstance(exclude, str):\n        raise click.BadOptionUsage(\"exclude\", \"Config key exclude must be a string\")\n\n    extend_exclude = config.get(\"extend_exclude\")\n    if extend_exclude is not None and not isinstance(extend_exclude, str):\n        raise click.BadOptionUsage(\n            \"extend-exclude\", \"Config key extend-exclude must be a string\"\n        )\n\n    line_ranges = config.get(\"line_ranges\")\n    if line_ranges is not None:\n        raise click.BadOptionUsage(\n            \"line-ranges\", \"Cannot use line-ranges in the pyproject.toml file.\"\n        )\n\n    default_map: dict[str, Any] = {}\n    if ctx.default_map:\n        default_map.update(ctx.default_map)\n    default_map.update(config)\n\n    ctx.default_map = default_map\n    return value",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 62,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "spellcheck_pyproject_toml_keys",
      "sourceCode": "def spellcheck_pyproject_toml_keys(\n    ctx: click.Context, config_keys: list[str], config_file_path: str\n) -> None:\n    invalid_keys: list[str] = []\n    available_config_options = {param.name for param in ctx.command.params}\n    for key in config_keys:\n        if key not in available_config_options:\n            invalid_keys.append(key)\n    if invalid_keys:\n        keys_str = \", \".join(map(repr, invalid_keys))\n        out(\n            f\"Invalid config keys detected: {keys_str} (in {config_file_path})\",\n            fg=\"red\",\n        )",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 13,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "main",
      "sourceCode": "@click.command(\n    context_settings={\"help_option_names\": [\"-h\", \"--help\"]},\n    # While Click does set this field automatically using the docstring, mypyc\n    # (annoyingly) strips 'em so we need to set it here too.\n    help=\"The uncompromising code formatter.\",\n)\n@click.option(\"-c\", \"--code\", type=str, help=\"Format the code passed in as a string.\")\n@click.option(\n    \"-l\",\n    \"--line-length\",\n    type=int,\n    default=DEFAULT_LINE_LENGTH,\n    help=\"How many characters per line to allow.\",\n    show_default=True,\n)\n@click.option(\n    \"-t\",\n    \"--target-version\",\n    type=click.Choice([v.name.lower() for v in TargetVersion]),\n    callback=target_version_option_callback,\n    multiple=True,\n    help=(\n        \"Python versions that should be supported by Black's output. You should\"\n        \" include all versions that your code supports. By default, Black will infer\"\n        \" target versions from the project metadata in pyproject.toml. If this does\"\n        \" not yield conclusive results, Black will use per-file auto-detection.\"\n    ),\n)\n@click.option(\n    \"--pyi\",\n    is_flag=True,\n    help=(\n        \"Format all input files like typing stubs regardless of file extension. This\"\n        \" is useful when piping source on standard input.\"\n    ),\n)\n@click.option(\n    \"--ipynb\",\n    is_flag=True,\n    help=(\n        \"Format all input files like Jupyter Notebooks regardless of file extension.\"\n        \" This is useful when piping source on standard input.\"\n    ),\n)\n@click.option(\n    \"--python-cell-magics\",\n    multiple=True,\n    help=(\n        \"When processing Jupyter Notebooks, add the given magic to the list\"\n        f\" of known python-magics ({', '.join(sorted(PYTHON_CELL_MAGICS))}).\"\n        \" Useful for formatting cells with custom python magics.\"\n    ),\n    default=[],\n)\n@click.option(\n    \"-x\",\n    \"--skip-source-first-line\",\n    is_flag=True,\n    help=\"Skip the first line of the source code.\",\n)\n@click.option(\n    \"-S\",\n    \"--skip-string-normalization\",\n    is_flag=True,\n    help=\"Don't normalize string quotes or prefixes.\",\n)\n@click.option(\n    \"-C\",\n    \"--skip-magic-trailing-comma\",\n    is_flag=True,\n    help=\"Don't use trailing commas as a reason to split lines.\",\n)\n@click.option(\n    \"--preview\",\n    is_flag=True,\n    help=(\n        \"Enable potentially disruptive style changes that may be added to Black's main\"\n        \" functionality in the next major release.\"\n    ),\n)\n@click.option(\n    \"--unstable\",\n    is_flag=True,\n    help=(\n        \"Enable potentially disruptive style changes that have known bugs or are not\"\n        \" currently expected to make it into the stable style Black's next major\"\n        \" release. Implies --preview.\"\n    ),\n)\n@click.option(\n    \"--enable-unstable-feature\",\n    type=click.Choice([v.name for v in Preview]),\n    callback=enable_unstable_feature_callback,\n    multiple=True,\n    help=(\n        \"Enable specific features included in the `--unstable` style. Requires\"\n        \" `--preview`. No compatibility guarantees are provided on the behavior\"\n        \" or existence of any unstable features.\"\n    ),\n)\n@click.option(\n    \"--check\",\n    is_flag=True,\n    help=(\n        \"Don't write the files back, just return the status. Return code 0 means\"\n        \" nothing would change. Return code 1 means some files would be reformatted.\"\n        \" Return code 123 means there was an internal error.\"\n    ),\n)\n@click.option(\n    \"--diff\",\n    is_flag=True,\n    help=(\n        \"Don't write the files back, just output a diff to indicate what changes\"\n        \" Black would've made. They are printed to stdout so capturing them is simple.\"\n    ),\n)\n@click.option(\n    \"--color/--no-color\",\n    is_flag=True,\n    help=\"Show (or do not show) colored diff. Only applies when --diff is given.\",\n)\n@click.option(\n    \"--line-ranges\",\n    multiple=True,\n    metavar=\"START-END\",\n    help=(\n        \"When specified, Black will try its best to only format these lines. This\"\n        \" option can be specified multiple times, and a union of the lines will be\"\n        \" formatted. Each range must be specified as two integers connected by a `-`:\"\n        \" `<START>-<END>`. The `<START>` and `<END>` integer indices are 1-based and\"\n        \" inclusive on both ends.\"\n    ),\n    default=(),\n)\n@click.option(\n    \"--fast/--safe\",\n    is_flag=True,\n    help=(\n        \"By default, Black performs an AST safety check after formatting your code.\"\n        \" The --fast flag turns off this check and the --safe flag explicitly enables\"\n        \" it. [default: --safe]\"\n    ),\n)\n@click.option(\n    \"--required-version\",\n    type=str,\n    help=(\n        \"Require a specific version of Black to be running. This is useful for\"\n        \" ensuring that all contributors to your project are using the same\"\n        \" version, because different versions of Black may format code a little\"\n        \" differently. This option can be set in a configuration file for consistent\"\n        \" results across environments.\"\n    ),\n)\n@click.option(\n    \"--exclude\",\n    type=str,\n    callback=validate_regex,\n    help=(\n        \"A regular expression that matches files and directories that should be\"\n        \" excluded on recursive searches. An empty value means no paths are excluded.\"\n        \" Use forward slashes for directories on all platforms (Windows, too).\"\n        \" By default, Black also ignores all paths listed in .gitignore. Changing this\"\n        f\" value will override all default exclusions. [default: {DEFAULT_EXCLUDES}]\"\n    ),\n    show_default=False,\n)\n@click.option(\n    \"--extend-exclude\",\n    type=str,\n    callback=validate_regex,\n    help=(\n        \"Like --exclude, but adds additional files and directories on top of the\"\n        \" default values instead of overriding them.\"\n    ),\n)\n@click.option(\n    \"--force-exclude\",\n    type=str,\n    callback=validate_regex,\n    help=(\n        \"Like --exclude, but files and directories matching this regex will be excluded\"\n        \" even when they are passed explicitly as arguments. This is useful when\"\n        \" invoking Black programmatically on changed files, such as in a pre-commit\"\n        \" hook or editor plugin.\"\n    ),\n)\n@click.option(\n    \"--stdin-filename\",\n    type=str,\n    is_eager=True,\n    help=(\n        \"The name of the file when passing it through stdin. Useful to make sure Black\"\n        \" will respect the --force-exclude option on some editors that rely on using\"\n        \" stdin.\"\n    ),\n)\n@click.option(\n    \"--include\",\n    type=str,\n    default=DEFAULT_INCLUDES,\n    callback=validate_regex,\n    help=(\n        \"A regular expression that matches files and directories that should be\"\n        \" included on recursive searches. An empty value means all files are included\"\n        \" regardless of the name. Use forward slashes for directories on all platforms\"\n        \" (Windows, too). Overrides all exclusions, including from .gitignore and\"\n        \" command line options.\"\n    ),\n    show_default=True,\n)\n@click.option(\n    \"-W\",\n    \"--workers\",\n    type=click.IntRange(min=1),\n    default=None,\n    help=(\n        \"When Black formats multiple files, it may use a process pool to speed up\"\n        \" formatting. This option controls the number of parallel workers. This can\"\n        \" also be specified via the BLACK_NUM_WORKERS environment variable. Defaults\"\n        \" to the number of CPUs in the system.\"\n    ),\n)\n@click.option(\n    \"-q\",\n    \"--quiet\",\n    is_flag=True,\n    help=(\n        \"Stop emitting all non-critical output. Error messages will still be emitted\"\n        \" (which can silenced by 2>/dev/null).\"\n    ),\n)\n@click.option(\n    \"-v\",\n    \"--verbose\",\n    is_flag=True,\n    help=(\n        \"Emit messages about files that were not changed or were ignored due to\"\n        \" exclusion patterns. If Black is using a configuration file, a message\"\n        \" detailing which one it is using will be emitted.\"\n    ),\n)\n@click.version_option(\n    version=__version__,\n    message=(\n        f\"%(prog)s, %(version)s (compiled: {'yes' if COMPILED else 'no'})\\n\"\n        f\"Python ({platform.python_implementation()}) {platform.python_version()}\"\n    ),\n)\n@click.argument(\n    \"src\",\n    nargs=-1,\n    type=click.Path(\n        exists=True, file_okay=True, dir_okay=True, readable=True, allow_dash=True\n    ),\n    is_eager=True,\n    metavar=\"SRC ...\",\n)\n@click.option(\n    \"--config\",\n    type=click.Path(\n        exists=True,\n        file_okay=True,\n        dir_okay=False,\n        readable=True,\n        allow_dash=False,\n        path_type=str,\n    ),\n    is_eager=True,\n    callback=read_pyproject_toml,\n    help=\"Read configuration options from a configuration file.\",\n)\n@click.pass_context\ndef main(  # noqa: C901\n    ctx: click.Context,\n    code: Optional[str],\n    line_length: int,\n    target_version: list[TargetVersion],\n    check: bool,\n    diff: bool,\n    line_ranges: Sequence[str],\n    color: bool,\n    fast: bool,\n    pyi: bool,\n    ipynb: bool,\n    python_cell_magics: Sequence[str],\n    skip_source_first_line: bool,\n    skip_string_normalization: bool,\n    skip_magic_trailing_comma: bool,\n    preview: bool,\n    unstable: bool,\n    enable_unstable_feature: list[Preview],\n    quiet: bool,\n    verbose: bool,\n    required_version: Optional[str],\n    include: Pattern[str],\n    exclude: Optional[Pattern[str]],\n    extend_exclude: Optional[Pattern[str]],\n    force_exclude: Optional[Pattern[str]],\n    stdin_filename: Optional[str],\n    workers: Optional[int],\n    src: tuple[str, ...],\n    config: Optional[str],\n) -> None:\n    \"\"\"The uncompromising code formatter.\"\"\"\n    ctx.ensure_object(dict)\n\n    assert sys.version_info >= (3, 9), \"Black requires Python 3.9+\"\n    if sys.version_info[:3] == (3, 12, 5):\n        out(\n            \"Python 3.12.5 has a memory safety issue that can cause Black's \"\n            \"AST safety checks to fail. \"\n            \"Please upgrade to Python 3.12.6 or downgrade to Python 3.12.4\"\n        )\n        ctx.exit(1)\n\n    if src and code is not None:\n        out(\n            main.get_usage(ctx)\n            + \"\\n\\n'SRC' and 'code' cannot be passed simultaneously.\"\n        )\n        ctx.exit(1)\n    if not src and code is None:\n        out(main.get_usage(ctx) + \"\\n\\nOne of 'SRC' or 'code' is required.\")\n        ctx.exit(1)\n\n    # It doesn't do anything if --unstable is also passed, so just allow it.\n    if enable_unstable_feature and not (preview or unstable):\n        out(\n            main.get_usage(ctx)\n            + \"\\n\\n'--enable-unstable-feature' requires '--preview'.\"\n        )\n        ctx.exit(1)\n\n    root, method = (\n        find_project_root(src, stdin_filename) if code is None else (None, None)\n    )\n    ctx.obj[\"root\"] = root\n\n    if verbose:\n        if root:\n            out(\n                f\"Identified `{root}` as project root containing a {method}.\",\n                fg=\"blue\",\n            )\n\n        if config:\n            config_source = ctx.get_parameter_source(\"config\")\n            user_level_config = str(find_user_pyproject_toml())\n            if config == user_level_config:\n                out(\n                    \"Using configuration from user-level config at \"\n                    f\"'{user_level_config}'.\",\n                    fg=\"blue\",\n                )\n            elif config_source in (\n                ParameterSource.DEFAULT,\n                ParameterSource.DEFAULT_MAP,\n            ):\n                out(\"Using configuration from project root.\", fg=\"blue\")\n            else:\n                out(f\"Using configuration in '{config}'.\", fg=\"blue\")\n            if ctx.default_map:\n                for param, value in ctx.default_map.items():\n                    out(f\"{param}: {value}\")\n\n    error_msg = \"Oh no! ðŸ’¥ ðŸ’” ðŸ’¥\"\n    if (\n        required_version\n        and required_version != __version__\n        and required_version != __version__.split(\".\")[0]\n    ):\n        err(\n            f\"{error_msg} The required version `{required_version}` does not match\"\n            f\" the running version `{__version__}`!\"\n        )\n        ctx.exit(1)\n    if ipynb and pyi:\n        err(\"Cannot pass both `pyi` and `ipynb` flags!\")\n        ctx.exit(1)\n\n    write_back = WriteBack.from_configuration(check=check, diff=diff, color=color)\n    if target_version:\n        versions = set(target_version)\n    else:\n        # We'll autodetect later.\n        versions = set()\n    mode = Mode(\n        target_versions=versions,\n        line_length=line_length,\n        is_pyi=pyi,\n        is_ipynb=ipynb,\n        skip_source_first_line=skip_source_first_line,\n        string_normalization=not skip_string_normalization,\n        magic_trailing_comma=not skip_magic_trailing_comma,\n        preview=preview,\n        unstable=unstable,\n        python_cell_magics=set(python_cell_magics),\n        enabled_features=set(enable_unstable_feature),\n    )\n\n    lines: list[tuple[int, int]] = []\n    if line_ranges:\n        if ipynb:\n            err(\"Cannot use --line-ranges with ipynb files.\")\n            ctx.exit(1)\n\n        try:\n            lines = parse_line_ranges(line_ranges)\n        except ValueError as e:\n            err(str(e))\n            ctx.exit(1)\n\n    if code is not None:\n        # Run in quiet mode by default with -c; the extra output isn't useful.\n        # You can still pass -v to get verbose output.\n        quiet = True\n\n    report = Report(check=check, diff=diff, quiet=quiet, verbose=verbose)\n\n    if code is not None:\n        reformat_code(\n            content=code,\n            fast=fast,\n            write_back=write_back,\n            mode=mode,\n            report=report,\n            lines=lines,\n        )\n    else:\n        assert root is not None  # root is only None if code is not None\n        try:\n            sources = get_sources(\n                root=root,\n                src=src,\n                quiet=quiet,\n                verbose=verbose,\n                include=include,\n                exclude=exclude,\n                extend_exclude=extend_exclude,\n                force_exclude=force_exclude,\n                report=report,\n                stdin_filename=stdin_filename,\n            )\n        except GitWildMatchPatternError:\n            ctx.exit(1)\n\n        path_empty(\n            sources,\n            \"No Python files are present to be formatted. Nothing to do ðŸ˜´\",\n            quiet,\n            verbose,\n            ctx,\n        )\n\n        if len(sources) == 1:\n            reformat_one(\n                src=sources.pop(),\n                fast=fast,\n                write_back=write_back,\n                mode=mode,\n                report=report,\n                lines=lines,\n            )\n        else:\n            from black.concurrency import reformat_many\n\n            if lines:\n                err(\"Cannot use --line-ranges to format multiple files.\")\n                ctx.exit(1)\n            reformat_many(\n                sources=sources,\n                fast=fast,\n                write_back=write_back,\n                mode=mode,\n                report=report,\n                workers=workers,\n            )\n\n    if verbose or not quiet:\n        if code is None and (verbose or report.change_count or report.failure_count):\n            out()\n        out(error_msg if report.return_code else \"All done! âœ¨ ðŸ° âœ¨\")\n        if code is None:\n            click.echo(str(report), err=True)\n    ctx.exit(report.return_code)",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 486,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "get_sources",
      "sourceCode": "def get_sources(\n    *,\n    root: Path,\n    src: tuple[str, ...],\n    quiet: bool,\n    verbose: bool,\n    include: Pattern[str],\n    exclude: Optional[Pattern[str]],\n    extend_exclude: Optional[Pattern[str]],\n    force_exclude: Optional[Pattern[str]],\n    report: \"Report\",\n    stdin_filename: Optional[str],\n) -> set[Path]:\n    \"\"\"Compute the set of files to be formatted.\"\"\"\n    sources: set[Path] = set()\n\n    assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n    using_default_exclude = exclude is None\n    exclude = re_compile_maybe_verbose(DEFAULT_EXCLUDES) if exclude is None else exclude\n    gitignore: Optional[dict[Path, PathSpec]] = None\n    root_gitignore = get_gitignore(root)\n\n    for s in src:\n        if s == \"-\" and stdin_filename:\n            path = Path(stdin_filename)\n            if path_is_excluded(stdin_filename, force_exclude):\n                report.path_ignored(\n                    path,\n                    \"--stdin-filename matches the --force-exclude regular expression\",\n                )\n                continue\n            is_stdin = True\n        else:\n            path = Path(s)\n            is_stdin = False\n\n        # Compare the logic here to the logic in `gen_python_files`.\n        if is_stdin or path.is_file():\n            if resolves_outside_root_or_cannot_stat(path, root, report):\n                if verbose:\n                    out(f'Skipping invalid source: \"{path}\"', fg=\"red\")\n                continue\n\n            root_relative_path = best_effort_relative_path(path, root).as_posix()\n            root_relative_path = \"/\" + root_relative_path\n\n            # Hard-exclude any files that matches the `--force-exclude` regex.\n            if path_is_excluded(root_relative_path, force_exclude):\n                report.path_ignored(\n                    path, \"matches the --force-exclude regular expression\"\n                )\n                continue\n\n            if is_stdin:\n                path = Path(f\"{STDIN_PLACEHOLDER}{str(path)}\")\n\n            if path.suffix == \".ipynb\" and not jupyter_dependencies_are_installed(\n                warn=verbose or not quiet\n            ):\n                continue\n\n            if verbose:\n                out(f'Found input source: \"{path}\"', fg=\"blue\")\n            sources.add(path)\n        elif path.is_dir():\n            path = root / (path.resolve().relative_to(root))\n            if verbose:\n                out(f'Found input source directory: \"{path}\"', fg=\"blue\")\n\n            if using_default_exclude:\n                gitignore = {\n                    root: root_gitignore,\n                    path: get_gitignore(path),\n                }\n            sources.update(\n                gen_python_files(\n                    path.iterdir(),\n                    root,\n                    include,\n                    exclude,\n                    extend_exclude,\n                    force_exclude,\n                    report,\n                    gitignore,\n                    verbose=verbose,\n                    quiet=quiet,\n                )\n            )\n        elif s == \"-\":\n            if verbose:\n                out(\"Found input source stdin\", fg=\"blue\")\n            sources.add(path)\n        else:\n            err(f\"invalid path: {s}\")\n\n    return sources",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 95,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "reformat_code",
      "sourceCode": "def reformat_code(\n    content: str,\n    fast: bool,\n    write_back: WriteBack,\n    mode: Mode,\n    report: Report,\n    *,\n    lines: Collection[tuple[int, int]] = (),\n) -> None:\n    \"\"\"\n    Reformat and print out `content` without spawning child processes.\n    Similar to `reformat_one`, but for string content.\n\n    `fast`, `write_back`, and `mode` options are passed to\n    :func:`format_file_in_place` or :func:`format_stdin_to_stdout`.\n    \"\"\"\n    path = Path(\"<string>\")\n    try:\n        changed = Changed.NO\n        if format_stdin_to_stdout(\n            content=content, fast=fast, write_back=write_back, mode=mode, lines=lines\n        ):\n            changed = Changed.YES\n        report.done(path, changed)\n    except Exception as exc:\n        if report.verbose:\n            traceback.print_exc()\n        report.failed(path, str(exc))",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 27,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "reformat_one",
      "sourceCode": "@mypyc_attr(patchable=True)\ndef reformat_one(\n    src: Path,\n    fast: bool,\n    write_back: WriteBack,\n    mode: Mode,\n    report: \"Report\",\n    *,\n    lines: Collection[tuple[int, int]] = (),\n) -> None:\n    \"\"\"Reformat a single file under `src` without spawning child processes.\n\n    `fast`, `write_back`, and `mode` options are passed to\n    :func:`format_file_in_place` or :func:`format_stdin_to_stdout`.\n    \"\"\"\n    try:\n        changed = Changed.NO\n\n        if str(src) == \"-\":\n            is_stdin = True\n        elif str(src).startswith(STDIN_PLACEHOLDER):\n            is_stdin = True\n            # Use the original name again in case we want to print something\n            # to the user\n            src = Path(str(src)[len(STDIN_PLACEHOLDER) :])\n        else:\n            is_stdin = False\n\n        if is_stdin:\n            if src.suffix == \".pyi\":\n                mode = replace(mode, is_pyi=True)\n            elif src.suffix == \".ipynb\":\n                mode = replace(mode, is_ipynb=True)\n            if format_stdin_to_stdout(\n                fast=fast, write_back=write_back, mode=mode, lines=lines\n            ):\n                changed = Changed.YES\n        else:\n            cache = Cache.read(mode)\n            if write_back not in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\n                if not cache.is_changed(src):\n                    changed = Changed.CACHED\n            if changed is not Changed.CACHED and format_file_in_place(\n                src, fast=fast, write_back=write_back, mode=mode, lines=lines\n            ):\n                changed = Changed.YES\n            if (write_back is WriteBack.YES and changed is not Changed.CACHED) or (\n                write_back is WriteBack.CHECK and changed is Changed.NO\n            ):\n                cache.write([src])\n        report.done(src, changed)\n    except Exception as exc:\n        if report.verbose:\n            traceback.print_exc()\n        report.failed(src, str(exc))",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 54,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "format_file_in_place",
      "sourceCode": "def format_file_in_place(\n    src: Path,\n    fast: bool,\n    mode: Mode,\n    write_back: WriteBack = WriteBack.NO,\n    lock: Any = None,  # multiprocessing.Manager().Lock() is some crazy proxy\n    *,\n    lines: Collection[tuple[int, int]] = (),\n) -> bool:\n    \"\"\"Format file under `src` path. Return True if changed.\n\n    If `write_back` is DIFF, write a diff to stdout. If it is YES, write reformatted\n    code to the file.\n    `mode` and `fast` options are passed to :func:`format_file_contents`.\n    \"\"\"\n    if src.suffix == \".pyi\":\n        mode = replace(mode, is_pyi=True)\n    elif src.suffix == \".ipynb\":\n        mode = replace(mode, is_ipynb=True)\n\n    then = datetime.fromtimestamp(src.stat().st_mtime, timezone.utc)\n    header = b\"\"\n    with open(src, \"rb\") as buf:\n        if mode.skip_source_first_line:\n            header = buf.readline()\n        src_contents, encoding, newline = decode_bytes(buf.read())\n    try:\n        dst_contents = format_file_contents(\n            src_contents, fast=fast, mode=mode, lines=lines\n        )\n    except NothingChanged:\n        return False\n    except JSONDecodeError:\n        raise ValueError(\n            f\"File '{src}' cannot be parsed as valid Jupyter notebook.\"\n        ) from None\n    src_contents = header.decode(encoding) + src_contents\n    dst_contents = header.decode(encoding) + dst_contents\n\n    if write_back == WriteBack.YES:\n        with open(src, \"w\", encoding=encoding, newline=newline) as f:\n            f.write(dst_contents)\n    elif write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\n        now = datetime.now(timezone.utc)\n        src_name = f\"{src}\\t{then}\"\n        dst_name = f\"{src}\\t{now}\"\n        if mode.is_ipynb:\n            diff_contents = ipynb_diff(src_contents, dst_contents, src_name, dst_name)\n        else:\n            diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n\n        if write_back == WriteBack.COLOR_DIFF:\n            diff_contents = color_diff(diff_contents)\n\n        with lock or nullcontext():\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f = wrap_stream_for_windows(f)\n            f.write(diff_contents)\n            f.detach()\n\n    return True",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 65,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "format_stdin_to_stdout",
      "sourceCode": "def format_stdin_to_stdout(\n    fast: bool,\n    *,\n    content: Optional[str] = None,\n    write_back: WriteBack = WriteBack.NO,\n    mode: Mode,\n    lines: Collection[tuple[int, int]] = (),\n) -> bool:\n    \"\"\"Format file on stdin. Return True if changed.\n\n    If content is None, it's read from sys.stdin.\n\n    If `write_back` is YES, write reformatted code back to stdout. If it is DIFF,\n    write a diff to stdout. The `mode` argument is passed to\n    :func:`format_file_contents`.\n    \"\"\"\n    then = datetime.now(timezone.utc)\n\n    if content is None:\n        src, encoding, newline = decode_bytes(sys.stdin.buffer.read())\n    else:\n        src, encoding, newline = content, \"utf-8\", \"\"\n\n    dst = src\n    try:\n        dst = format_file_contents(src, fast=fast, mode=mode, lines=lines)\n        return True\n\n    except NothingChanged:\n        return False\n\n    finally:\n        f = io.TextIOWrapper(\n            sys.stdout.buffer, encoding=encoding, newline=newline, write_through=True\n        )\n        if write_back == WriteBack.YES:\n            # Make sure there's a newline after the content\n            if dst and dst[-1] != \"\\n\":\n                dst += \"\\n\"\n            f.write(dst)\n        elif write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\n            now = datetime.now(timezone.utc)\n            src_name = f\"STDIN\\t{then}\"\n            dst_name = f\"STDOUT\\t{now}\"\n            d = diff(src, dst, src_name, dst_name)\n            if write_back == WriteBack.COLOR_DIFF:\n                d = color_diff(d)\n                f = wrap_stream_for_windows(f)\n            f.write(d)\n        f.detach()",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 49,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "check_stability_and_equivalence",
      "sourceCode": "def check_stability_and_equivalence(\n    src_contents: str,\n    dst_contents: str,\n    *,\n    mode: Mode,\n    lines: Collection[tuple[int, int]] = (),\n) -> None:\n    \"\"\"Perform stability and equivalence checks.\n\n    Raise AssertionError if source and destination contents are not\n    equivalent, or if a second pass of the formatter would format the\n    content differently.\n    \"\"\"\n    assert_equivalent(src_contents, dst_contents)\n    assert_stable(src_contents, dst_contents, mode=mode, lines=lines)",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "format_file_contents",
      "sourceCode": "def format_file_contents(\n    src_contents: str,\n    *,\n    fast: bool,\n    mode: Mode,\n    lines: Collection[tuple[int, int]] = (),\n) -> FileContent:\n    \"\"\"Reformat contents of a file and return new contents.\n\n    If `fast` is False, additionally confirm that the reformatted code is\n    valid by calling :func:`assert_equivalent` and :func:`assert_stable` on it.\n    `mode` is passed to :func:`format_str`.\n    \"\"\"\n    if mode.is_ipynb:\n        dst_contents = format_ipynb_string(src_contents, fast=fast, mode=mode)\n    else:\n        dst_contents = format_str(src_contents, mode=mode, lines=lines)\n    if src_contents == dst_contents:\n        raise NothingChanged\n\n    if not fast and not mode.is_ipynb:\n        # Jupyter notebooks will already have been checked above.\n        check_stability_and_equivalence(\n            src_contents, dst_contents, mode=mode, lines=lines\n        )\n    return dst_contents",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 25,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "format_cell",
      "sourceCode": "def format_cell(src: str, *, fast: bool, mode: Mode) -> str:\n    \"\"\"Format code in given cell of Jupyter notebook.\n\n    General idea is:\n\n      - if cell has trailing semicolon, remove it;\n      - if cell has IPython magics, mask them;\n      - format cell;\n      - reinstate IPython magics;\n      - reinstate trailing semicolon (if originally present);\n      - strip trailing newlines.\n\n    Cells with syntax errors will not be processed, as they\n    could potentially be automagics or multi-line magics, which\n    are currently not supported.\n    \"\"\"\n    validate_cell(src, mode)\n    src_without_trailing_semicolon, has_trailing_semicolon = remove_trailing_semicolon(\n        src\n    )\n    try:\n        masked_src, replacements = mask_cell(src_without_trailing_semicolon)\n    except SyntaxError:\n        raise NothingChanged from None\n    masked_dst = format_str(masked_src, mode=mode)\n    if not fast:\n        check_stability_and_equivalence(masked_src, masked_dst, mode=mode)\n    dst_without_trailing_semicolon = unmask_cell(masked_dst, replacements)\n    dst = put_trailing_semicolon_back(\n        dst_without_trailing_semicolon, has_trailing_semicolon\n    )\n    dst = dst.rstrip(\"\\n\")\n    if dst == src:\n        raise NothingChanged from None\n    return dst",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 34,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "format_ipynb_string",
      "sourceCode": "def format_ipynb_string(src_contents: str, *, fast: bool, mode: Mode) -> FileContent:\n    \"\"\"Format Jupyter notebook.\n\n    Operate cell-by-cell, only on code cells, only for Python notebooks.\n    If the ``.ipynb`` originally had a trailing newline, it'll be preserved.\n    \"\"\"\n    if not src_contents:\n        raise NothingChanged\n\n    trailing_newline = src_contents[-1] == \"\\n\"\n    modified = False\n    nb = json.loads(src_contents)\n    validate_metadata(nb)\n    for cell in nb[\"cells\"]:\n        if cell.get(\"cell_type\", None) == \"code\":\n            try:\n                src = \"\".join(cell[\"source\"])\n                dst = format_cell(src, fast=fast, mode=mode)\n            except NothingChanged:\n                pass\n            else:\n                cell[\"source\"] = dst.splitlines(keepends=True)\n                modified = True\n    if modified:\n        dst_contents = json.dumps(nb, indent=1, ensure_ascii=False)\n        if trailing_newline:\n            dst_contents = dst_contents + \"\\n\"\n        return dst_contents\n    else:\n        raise NothingChanged",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 29,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "format_str",
      "sourceCode": "def format_str(\n    src_contents: str, *, mode: Mode, lines: Collection[tuple[int, int]] = ()\n) -> str:\n    \"\"\"Reformat a string and return new contents.\n\n    `mode` determines formatting options, such as how many characters per line are\n    allowed.  Example:\n\n    >>> import black\n    >>> print(black.format_str(\"def f(arg:str='')->None:...\", mode=black.Mode()))\n    def f(arg: str = \"\") -> None:\n        ...\n\n    A more complex example:\n\n    >>> print(\n    ...   black.format_str(\n    ...     \"def f(arg:str='')->None: hey\",\n    ...     mode=black.Mode(\n    ...       target_versions={black.TargetVersion.PY36},\n    ...       line_length=10,\n    ...       string_normalization=False,\n    ...       is_pyi=False,\n    ...     ),\n    ...   ),\n    ... )\n    def f(\n        arg: str = '',\n    ) -> None:\n        hey\n\n    \"\"\"\n    if lines:\n        lines = sanitized_lines(lines, src_contents)\n        if not lines:\n            return src_contents  # Nothing to format\n    dst_contents = _format_str_once(src_contents, mode=mode, lines=lines)\n    # Forced second pass to work around optional trailing commas (becoming\n    # forced trailing commas on pass 2) interacting differently with optional\n    # parentheses.  Admittedly ugly.\n    if src_contents != dst_contents:\n        if lines:\n            lines = adjusted_lines(lines, src_contents, dst_contents)\n        return _format_str_once(dst_contents, mode=mode, lines=lines)\n    return dst_contents",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 44,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "_format_str_once",
      "sourceCode": "def _format_str_once(\n    src_contents: str, *, mode: Mode, lines: Collection[tuple[int, int]] = ()\n) -> str:\n    src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)\n    dst_blocks: list[LinesBlock] = []\n    if mode.target_versions:\n        versions = mode.target_versions\n    else:\n        future_imports = get_future_imports(src_node)\n        versions = detect_target_versions(src_node, future_imports=future_imports)\n\n    context_manager_features = {\n        feature\n        for feature in {Feature.PARENTHESIZED_CONTEXT_MANAGERS}\n        if supports_feature(versions, feature)\n    }\n    normalize_fmt_off(src_node, mode, lines)\n    if lines:\n        # This should be called after normalize_fmt_off.\n        convert_unchanged_lines(src_node, lines)\n\n    line_generator = LineGenerator(mode=mode, features=context_manager_features)\n    elt = EmptyLineTracker(mode=mode)\n    split_line_features = {\n        feature\n        for feature in {\n            Feature.TRAILING_COMMA_IN_CALL,\n            Feature.TRAILING_COMMA_IN_DEF,\n        }\n        if supports_feature(versions, feature)\n    }\n    block: Optional[LinesBlock] = None\n    for current_line in line_generator.visit(src_node):\n        block = elt.maybe_empty_lines(current_line)\n        dst_blocks.append(block)\n        for line in transform_line(\n            current_line, mode=mode, features=split_line_features\n        ):\n            block.content_lines.append(str(line))\n    if dst_blocks:\n        dst_blocks[-1].after = 0\n    dst_contents = []\n    for block in dst_blocks:\n        dst_contents.extend(block.all_lines())\n    if not dst_contents:\n        # Use decode_bytes to retrieve the correct source newline (CRLF or LF),\n        # and check if normalized_content has more than one line\n        normalized_content, _, newline = decode_bytes(src_contents.encode(\"utf-8\"))\n        if \"\\n\" in normalized_content:\n            return newline\n        return \"\"\n    return \"\".join(dst_contents)",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 51,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "decode_bytes",
      "sourceCode": "def decode_bytes(src: bytes) -> tuple[FileContent, Encoding, NewLine]:\n    \"\"\"Return a tuple of (decoded_contents, encoding, newline).\n\n    `newline` is either CRLF or LF but `decoded_contents` is decoded with\n    universal newlines (i.e. only contains LF).\n    \"\"\"\n    srcbuf = io.BytesIO(src)\n    encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n    if not lines:\n        return \"\", encoding, \"\\n\"\n\n    newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n    srcbuf.seek(0)\n    with io.TextIOWrapper(srcbuf, encoding) as tiow:\n        return tiow.read(), encoding, newline",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "get_features_used",
      "sourceCode": "def get_features_used(  # noqa: C901\n    node: Node, *, future_imports: Optional[set[str]] = None\n) -> set[Feature]:\n    \"\"\"Return a set of (relatively) new Python features used in this file.\n\n    Currently looking for:\n    - f-strings;\n    - self-documenting expressions in f-strings (f\"{x=}\");\n    - underscores in numeric literals;\n    - trailing commas after * or ** in function signatures and calls;\n    - positional only arguments in function signatures and lambdas;\n    - assignment expression;\n    - relaxed decorator syntax;\n    - usage of __future__ flags (annotations);\n    - print / exec statements;\n    - parenthesized context managers;\n    - match statements;\n    - except* clause;\n    - variadic generics;\n    \"\"\"\n    features: set[Feature] = set()\n    if future_imports:\n        features |= {\n            FUTURE_FLAG_TO_FEATURE[future_import]\n            for future_import in future_imports\n            if future_import in FUTURE_FLAG_TO_FEATURE\n        }\n\n    for n in node.pre_order():\n        if n.type == token.FSTRING_START:\n            features.add(Feature.F_STRINGS)\n        elif (\n            n.type == token.RBRACE\n            and n.parent is not None\n            and any(child.type == token.EQUAL for child in n.parent.children)\n        ):\n            features.add(Feature.DEBUG_F_STRINGS)\n\n        elif is_number_token(n):\n            if \"_\" in n.value:\n                features.add(Feature.NUMERIC_UNDERSCORES)\n\n        elif n.type == token.SLASH:\n            if n.parent and n.parent.type in {\n                syms.typedargslist,\n                syms.arglist,\n                syms.varargslist,\n            }:\n                features.add(Feature.POS_ONLY_ARGUMENTS)\n\n        elif n.type == token.COLONEQUAL:\n            features.add(Feature.ASSIGNMENT_EXPRESSIONS)\n\n        elif n.type == syms.decorator:\n            if len(n.children) > 1 and not is_simple_decorator_expression(\n                n.children[1]\n            ):\n                features.add(Feature.RELAXED_DECORATORS)\n\n        elif (\n            n.type in {syms.typedargslist, syms.arglist}\n            and n.children\n            and n.children[-1].type == token.COMMA\n        ):\n            if n.type == syms.typedargslist:\n                feature = Feature.TRAILING_COMMA_IN_DEF\n            else:\n                feature = Feature.TRAILING_COMMA_IN_CALL\n\n            for ch in n.children:\n                if ch.type in STARS:\n                    features.add(feature)\n\n                if ch.type == syms.argument:\n                    for argch in ch.children:\n                        if argch.type in STARS:\n                            features.add(feature)\n\n        elif (\n            n.type in {syms.return_stmt, syms.yield_expr}\n            and len(n.children) >= 2\n            and n.children[1].type == syms.testlist_star_expr\n            and any(child.type == syms.star_expr for child in n.children[1].children)\n        ):\n            features.add(Feature.UNPACKING_ON_FLOW)\n\n        elif (\n            n.type == syms.annassign\n            and len(n.children) >= 4\n            and n.children[3].type == syms.testlist_star_expr\n        ):\n            features.add(Feature.ANN_ASSIGN_EXTENDED_RHS)\n\n        elif (\n            n.type == syms.with_stmt\n            and len(n.children) > 2\n            and n.children[1].type == syms.atom\n        ):\n            atom_children = n.children[1].children\n            if (\n                len(atom_children) == 3\n                and atom_children[0].type == token.LPAR\n                and _contains_asexpr(atom_children[1])\n                and atom_children[2].type == token.RPAR\n            ):\n                features.add(Feature.PARENTHESIZED_CONTEXT_MANAGERS)\n\n        elif n.type == syms.match_stmt:\n            features.add(Feature.PATTERN_MATCHING)\n\n        elif (\n            n.type == syms.except_clause\n            and len(n.children) >= 2\n            and n.children[1].type == token.STAR\n        ):\n            features.add(Feature.EXCEPT_STAR)\n\n        elif n.type in {syms.subscriptlist, syms.trailer} and any(\n            child.type == syms.star_expr for child in n.children\n        ):\n            features.add(Feature.VARIADIC_GENERICS)\n\n        elif (\n            n.type == syms.tname_star\n            and len(n.children) == 3\n            and n.children[2].type == syms.star_expr\n        ):\n            features.add(Feature.VARIADIC_GENERICS)\n\n        elif n.type in (syms.type_stmt, syms.typeparams):\n            features.add(Feature.TYPE_PARAMS)\n\n        elif (\n            n.type in (syms.typevartuple, syms.paramspec, syms.typevar)\n            and n.children[-2].type == token.EQUAL\n        ):\n            features.add(Feature.TYPE_PARAM_DEFAULTS)\n\n    return features",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 138,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "_contains_asexpr",
      "sourceCode": "def _contains_asexpr(node: Union[Node, Leaf]) -> bool:\n    \"\"\"Return True if `node` contains an as-pattern.\"\"\"\n    if node.type == syms.asexpr_test:\n        return True\n    elif node.type == syms.atom:\n        if (\n            len(node.children) == 3\n            and node.children[0].type == token.LPAR\n            and node.children[2].type == token.RPAR\n        ):\n            return _contains_asexpr(node.children[1])\n    elif node.type == syms.testlist_gexp:\n        return any(_contains_asexpr(child) for child in node.children)\n    return False",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 13,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "get_future_imports",
      "sourceCode": "def get_future_imports(node: Node) -> set[str]:\n    \"\"\"Return a set of __future__ imports in the file.\"\"\"\n    imports: set[str] = set()\n\n    def get_imports_from_children(children: list[LN]) -> Generator[str, None, None]:\n        for child in children:\n            if isinstance(child, Leaf):\n                if child.type == token.NAME:\n                    yield child.value\n\n            elif child.type == syms.import_as_name:\n                orig_name = child.children[0]\n                assert isinstance(orig_name, Leaf), \"Invalid syntax parsing imports\"\n                assert orig_name.type == token.NAME, \"Invalid syntax parsing imports\"\n                yield orig_name.value\n\n            elif child.type == syms.import_as_names:\n                yield from get_imports_from_children(child.children)\n\n            else:\n                raise AssertionError(\"Invalid syntax parsing imports\")\n\n    for child in node.children:\n        if child.type != syms.simple_stmt:\n            break\n\n        first_child = child.children[0]\n        if isinstance(first_child, Leaf):\n            # Continue looking if we see a docstring; otherwise stop.\n            if (\n                len(child.children) == 2\n                and first_child.type == token.STRING\n                and child.children[1].type == token.NEWLINE\n            ):\n                continue\n\n            break\n\n        elif first_child.type == syms.import_from:\n            module_name = first_child.children[1]\n            if not isinstance(module_name, Leaf) or module_name.value != \"__future__\":\n                break\n\n            imports |= set(get_imports_from_children(first_child.children[3:]))\n        else:\n            break\n\n    return imports",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 47,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "get_imports_from_children",
      "sourceCode": "def get_imports_from_children(children: list[LN]) -> Generator[str, None, None]:\n        for child in children:\n            if isinstance(child, Leaf):\n                if child.type == token.NAME:\n                    yield child.value\n\n            elif child.type == syms.import_as_name:\n                orig_name = child.children[0]\n                assert isinstance(orig_name, Leaf), \"Invalid syntax parsing imports\"\n                assert orig_name.type == token.NAME, \"Invalid syntax parsing imports\"\n                yield orig_name.value\n\n            elif child.type == syms.import_as_names:\n                yield from get_imports_from_children(child.children)\n\n            else:\n                raise AssertionError(\"Invalid syntax parsing imports\")",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 16,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "assert_equivalent",
      "sourceCode": "def assert_equivalent(src: str, dst: str) -> None:\n    \"\"\"Raise AssertionError if `src` and `dst` aren't equivalent.\"\"\"\n    try:\n        src_ast = parse_ast(src)\n    except Exception as exc:\n        raise ASTSafetyError(\n            \"cannot use --safe with this file; failed to parse source file AST: \"\n            f\"{exc}\\n\"\n            \"This could be caused by running Black with an older Python version \"\n            \"that does not support new syntax used in your source file.\"\n        ) from exc\n\n    try:\n        dst_ast = parse_ast(dst)\n    except Exception as exc:\n        log = dump_to_file(\"\".join(traceback.format_tb(exc.__traceback__)), dst)\n        raise ASTSafetyError(\n            f\"INTERNAL ERROR: {_black_info()} produced invalid code: {exc}. \"\n            \"Please report a bug on https://github.com/psf/black/issues.  \"\n            f\"This invalid output might be helpful: {log}\"\n        ) from None\n\n    src_ast_str = \"\\n\".join(stringify_ast(src_ast))\n    dst_ast_str = \"\\n\".join(stringify_ast(dst_ast))\n    if src_ast_str != dst_ast_str:\n        log = dump_to_file(diff(src_ast_str, dst_ast_str, \"src\", \"dst\"))\n        raise ASTSafetyError(\n            f\"INTERNAL ERROR: {_black_info()} produced code that is not equivalent to\"\n            \" the source.  Please report a bug on https://github.com/psf/black/issues.\"\n            f\"  This diff might be helpful: {log}\"\n        ) from None",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 30,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "assert_stable",
      "sourceCode": "def assert_stable(\n    src: str, dst: str, mode: Mode, *, lines: Collection[tuple[int, int]] = ()\n) -> None:\n    \"\"\"Raise AssertionError if `dst` reformats differently the second time.\"\"\"\n    if lines:\n        # Formatting specified lines requires `adjusted_lines` to map original lines\n        # to the formatted lines before re-formatting the previously formatted result.\n        # Due to less-ideal diff algorithm, some edge cases produce incorrect new line\n        # ranges. Hence for now, we skip the stable check.\n        # See https://github.com/psf/black/issues/4033 for context.\n        return\n    # We shouldn't call format_str() here, because that formats the string\n    # twice and may hide a bug where we bounce back and forth between two\n    # versions.\n    newdst = _format_str_once(dst, mode=mode, lines=lines)\n    if dst != newdst:\n        log = dump_to_file(\n            str(mode),\n            diff(src, dst, \"source\", \"first pass\"),\n            diff(dst, newdst, \"first pass\", \"second pass\"),\n        )\n        raise AssertionError(\n            f\"INTERNAL ERROR: {_black_info()} produced different code on the second\"\n            \" pass of the formatter.  Please report a bug on\"\n            f\" https://github.com/psf/black/issues.  This diff might be helpful: {log}\"\n        ) from None",
      "importString": "import io\nimport json\nimport platform\nimport re\nimport sys\nimport tokenize\nimport traceback\nfrom collections.abc import (\nCollection\nGenerator\nIterator\nMutableMapping\nSequence\nSized\n)\nfrom contextlib import contextmanager\nfrom dataclasses import replace\nfrom datetime import datetime, timezone\nfrom enum import Enum\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import Any, Optional, Union\n\nimport click\nfrom click.core import ParameterSource\nfrom mypy_extensions import mypyc_attr\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\n\nfrom _black_version import version as __version__\nfrom black.cache import Cache\nfrom black.comments import normalize_fmt_off\nfrom black.const import (\nDEFAULT_EXCLUDES\nDEFAULT_INCLUDES\nDEFAULT_LINE_LENGTH\nSTDIN_PLACEHOLDER\n)\nfrom black.files import (\nbest_effort_relative_path\nfind_project_root\nfind_pyproject_toml\nfind_user_pyproject_toml\ngen_python_files\nget_gitignore\nparse_pyproject_toml\npath_is_excluded\nresolves_outside_root_or_cannot_stat\nwrap_stream_for_windows\n)\nfrom black.handle_ipynb_magics import (\nPYTHON_CELL_MAGICS\njupyter_dependencies_are_installed\nmask_cell\nput_trailing_semicolon_back\nremove_trailing_semicolon\nunmask_cell\nvalidate_cell\n)\nfrom black.linegen import LN, LineGenerator, transform_line\nfrom black.lines import EmptyLineTracker, LinesBlock\nfrom black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature\nfrom black.mode import Mode as Mode\nfrom black.mode import Preview, TargetVersion, supports_feature\nfrom black.nodes import STARS, is_number_token, is_simple_decorator_expression, syms\nfrom black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out\nfrom black.ranges import (\nadjusted_lines\nconvert_unchanged_lines\nparse_line_ranges\nsanitized_lines\n)\nfrom black.report import Changed, NothingChanged, Report\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 25,
      "relativeDocumentPath": "src/black/__init__.py"
    },
    {
      "symbolName": "mark",
      "sourceCode": "def mark(self, leaf: Leaf) -> None:\n        \"\"\"Mark `leaf` with bracket-related metadata. Keep track of delimiters.\n\n        All leaves receive an int `bracket_depth` field that stores how deep\n        within brackets a given leaf is. 0 means there are no enclosing brackets\n        that started on this line.\n\n        If a leaf is itself a closing bracket and there is a matching opening\n        bracket earlier, it receives an `opening_bracket` field with which it forms a\n        pair. This is a one-directional link to avoid reference cycles. Closing\n        bracket without opening happens on lines continued from previous\n        breaks, e.g. `) -> \"ReturnType\":` as part of a funcdef where we place\n        the return type annotation on its own line of the previous closing RPAR.\n\n        If a leaf is a delimiter (a token on which Black can split the line if\n        needed) and it's on depth 0, its `id()` is stored in the tracker's\n        `delimiters` field.\n        \"\"\"\n        if leaf.type == token.COMMENT:\n            return\n\n        if (\n            self.depth == 0\n            and leaf.type in CLOSING_BRACKETS\n            and (self.depth, leaf.type) not in self.bracket_match\n        ):\n            return\n\n        self.maybe_decrement_after_for_loop_variable(leaf)\n        self.maybe_decrement_after_lambda_arguments(leaf)\n        if leaf.type in CLOSING_BRACKETS:\n            self.depth -= 1\n            try:\n                opening_bracket = self.bracket_match.pop((self.depth, leaf.type))\n            except KeyError as e:\n                raise BracketMatchError(\n                    \"Unable to match a closing bracket to the following opening\"\n                    f\" bracket: {leaf}\"\n                ) from e\n            leaf.opening_bracket = opening_bracket\n            if not leaf.value:\n                self.invisible.append(leaf)\n        leaf.bracket_depth = self.depth\n        if self.depth == 0:\n            delim = is_split_before_delimiter(leaf, self.previous)\n            if delim and self.previous is not None:\n                self.delimiters[id(self.previous)] = delim\n            else:\n                delim = is_split_after_delimiter(leaf)\n                if delim:\n                    self.delimiters[id(leaf)] = delim\n        if leaf.type in OPENING_BRACKETS:\n            self.bracket_match[self.depth, BRACKET[leaf.type]] = leaf\n            self.depth += 1\n            if not leaf.value:\n                self.invisible.append(leaf)\n        self.previous = leaf\n        self.maybe_increment_lambda_arguments(leaf)\n        self.maybe_increment_for_loop_variable(leaf)",
      "importString": "from collections.abc import Iterable, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Final, Optional, Union\nfrom black.nodes import (\nBRACKET\nCLOSING_BRACKETS\nCOMPARATORS\nLOGIC_OPERATORS\nMATH_OPERATORS\nOPENING_BRACKETS\nUNPACKING_PARENTS\nVARARGS_PARENTS\nis_vararg\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 58,
      "relativeDocumentPath": "src/black/brackets.py"
    },
    {
      "symbolName": "maybe_increment_for_loop_variable",
      "sourceCode": "def maybe_increment_for_loop_variable(self, leaf: Leaf) -> bool:\n        \"\"\"In a for loop, or comprehension, the variables are often unpacks.\n\n        To avoid splitting on the comma in this situation, increase the depth of\n        tokens between `for` and `in`.\n        \"\"\"\n        if leaf.type == token.NAME and leaf.value == \"for\":\n            self.depth += 1\n            self._for_loop_depths.append(self.depth)\n            return True\n\n        return False",
      "importString": "from collections.abc import Iterable, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Final, Optional, Union\nfrom black.nodes import (\nBRACKET\nCLOSING_BRACKETS\nCOMPARATORS\nLOGIC_OPERATORS\nMATH_OPERATORS\nOPENING_BRACKETS\nUNPACKING_PARENTS\nVARARGS_PARENTS\nis_vararg\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/brackets.py"
    },
    {
      "symbolName": "maybe_decrement_after_for_loop_variable",
      "sourceCode": "def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:\n        \"\"\"See `maybe_increment_for_loop_variable` above for explanation.\"\"\"\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n            self.depth -= 1\n            self._for_loop_depths.pop()\n            return True\n\n        return False",
      "importString": "from collections.abc import Iterable, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Final, Optional, Union\nfrom black.nodes import (\nBRACKET\nCLOSING_BRACKETS\nCOMPARATORS\nLOGIC_OPERATORS\nMATH_OPERATORS\nOPENING_BRACKETS\nUNPACKING_PARENTS\nVARARGS_PARENTS\nis_vararg\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 12,
      "relativeDocumentPath": "src/black/brackets.py"
    },
    {
      "symbolName": "maybe_increment_lambda_arguments",
      "sourceCode": "def maybe_increment_lambda_arguments(self, leaf: Leaf) -> bool:\n        \"\"\"In a lambda expression, there might be more than one argument.\n\n        To avoid splitting on the comma in this situation, increase the depth of\n        tokens between `lambda` and `:`.\n        \"\"\"\n        if leaf.type == token.NAME and leaf.value == \"lambda\":\n            self.depth += 1\n            self._lambda_argument_depths.append(self.depth)\n            return True\n\n        return False",
      "importString": "from collections.abc import Iterable, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Final, Optional, Union\nfrom black.nodes import (\nBRACKET\nCLOSING_BRACKETS\nCOMPARATORS\nLOGIC_OPERATORS\nMATH_OPERATORS\nOPENING_BRACKETS\nUNPACKING_PARENTS\nVARARGS_PARENTS\nis_vararg\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/brackets.py"
    },
    {
      "symbolName": "maybe_decrement_after_lambda_arguments",
      "sourceCode": "def maybe_decrement_after_lambda_arguments(self, leaf: Leaf) -> bool:\n        \"\"\"See `maybe_increment_lambda_arguments` above for explanation.\"\"\"\n        if (\n            self._lambda_argument_depths\n            and self._lambda_argument_depths[-1] == self.depth\n            and leaf.type == token.COLON\n        ):\n            self.depth -= 1\n            self._lambda_argument_depths.pop()\n            return True\n\n        return False",
      "importString": "from collections.abc import Iterable, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Final, Optional, Union\nfrom black.nodes import (\nBRACKET\nCLOSING_BRACKETS\nCOMPARATORS\nLOGIC_OPERATORS\nMATH_OPERATORS\nOPENING_BRACKETS\nUNPACKING_PARENTS\nVARARGS_PARENTS\nis_vararg\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/brackets.py"
    },
    {
      "symbolName": "is_split_before_delimiter",
      "sourceCode": "def is_split_before_delimiter(leaf: Leaf, previous: Optional[Leaf] = None) -> Priority:\n    \"\"\"Return the priority of the `leaf` delimiter, given a line break before it.\n\n    The delimiter priorities returned here are from those delimiters that would\n    cause a line break before themselves.\n\n    Higher numbers are higher priority.\n    \"\"\"\n    if is_vararg(leaf, within=VARARGS_PARENTS | UNPACKING_PARENTS):\n        # * and ** might also be MATH_OPERATORS but in this case they are not.\n        # Don't treat them as a delimiter.\n        return 0\n\n    if (\n        leaf.type == token.DOT\n        and leaf.parent\n        and leaf.parent.type not in {syms.import_from, syms.dotted_name}\n        and (previous is None or previous.type in CLOSING_BRACKETS)\n    ):\n        return DOT_PRIORITY\n    # ... existing code ...\n    if (\n        leaf.type in MATH_OPERATORS\n        and leaf.parent\n        and leaf.parent.type not in {syms.factor, syms.star_expr}\n    ):\n        return MATH_PRIORITIES[leaf.type]\n\n    if leaf.type in COMPARATORS:\n        return COMPARATOR_PRIORITY\n\n    if (\n        leaf.type == token.STRING\n        and previous is not None\n        and previous.type == token.STRING\n    ):\n        return STRING_PRIORITY\n\n    if leaf.type not in {token.NAME, token.ASYNC}:\n        return 0\n\n    if (\n        leaf.value == \"for\"\n        and leaf.parent\n        and leaf.parent.type in {syms.comp_for, syms.old_comp_for}\n        or leaf.type == token.ASYNC\n    ):\n        if (\n            not isinstance(leaf.prev_sibling, Leaf)\n            or leaf.prev_sibling.value != \"async\"\n        ):\n            return COMPREHENSION_PRIORITY\n\n    if (\n        leaf.value == \"if\"\n        and leaf.parent\n        and leaf.parent.type in {syms.comp_if, syms.old_comp_if}\n    ):\n        return COMPREHENSION_PRIORITY\n\n    if leaf.value in {\"if\", \"else\"} and leaf.parent and leaf.parent.type == syms.test:\n        return TERNARY_PRIORITY\n\n    if leaf.value == \"is\":\n        return COMPARATOR_PRIORITY\n\n    if (\n        leaf.value == \"in\"\n        and leaf.parent\n        and leaf.parent.type in {syms.comp_op, syms.comparison}\n        and not (\n            previous is not None\n            and previous.type == token.NAME\n            and previous.value == \"not\"\n        )\n    ):\n        return COMPARATOR_PRIORITY\n\n    if (\n        leaf.value == \"not\"\n        and leaf.parent\n        and leaf.parent.type == syms.comp_op\n        and not (\n            previous is not None\n            and previous.type == token.NAME\n            and previous.value == \"is\"\n        )\n    ):\n        return COMPARATOR_PRIORITY\n\n    if leaf.value in LOGIC_OPERATORS and leaf.parent:\n        return LOGIC_PRIORITY\n\n    return 0",
      "importString": "from collections.abc import Iterable, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Final, Optional, Union\nfrom black.nodes import (\nBRACKET\nCLOSING_BRACKETS\nCOMPARATORS\nLOGIC_OPERATORS\nMATH_OPERATORS\nOPENING_BRACKETS\nUNPACKING_PARENTS\nVARARGS_PARENTS\nis_vararg\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 93,
      "relativeDocumentPath": "src/black/brackets.py"
    },
    {
      "symbolName": "max_delimiter_priority_in_atom",
      "sourceCode": "def max_delimiter_priority_in_atom(node: LN) -> Priority:\n    \"\"\"Return maximum delimiter priority inside `node`.\n\n    This is specific to atoms with contents contained in a pair of parentheses.\n    If `node` isn't an atom or there are no enclosing parentheses, returns 0.\n    \"\"\"\n    if node.type != syms.atom:\n        return 0\n\n    first = node.children[0]\n    last = node.children[-1]\n    if not (first.type == token.LPAR and last.type == token.RPAR):\n        return 0\n\n    bt = BracketTracker()\n    for c in node.children[1:-1]:\n        if isinstance(c, Leaf):\n            bt.mark(c)\n        else:\n            for leaf in c.leaves():\n                bt.mark(leaf)\n    try:\n        return bt.max_delimiter_priority()\n\n    except ValueError:\n        return 0",
      "importString": "from collections.abc import Iterable, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Final, Optional, Union\nfrom black.nodes import (\nBRACKET\nCLOSING_BRACKETS\nCOMPARATORS\nLOGIC_OPERATORS\nMATH_OPERATORS\nOPENING_BRACKETS\nUNPACKING_PARENTS\nVARARGS_PARENTS\nis_vararg\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 25,
      "relativeDocumentPath": "src/black/brackets.py"
    },
    {
      "symbolName": "get_leaves_inside_matching_brackets",
      "sourceCode": "def get_leaves_inside_matching_brackets(leaves: Sequence[Leaf]) -> set[LeafID]:\n    \"\"\"Return leaves that are inside matching brackets.\n\n    The input `leaves` can have non-matching brackets at the head or tail parts.\n    Matching brackets are included.\n    \"\"\"\n    try:\n        # Start with the first opening bracket and ignore closing brackets before.\n        start_index = next(\n            i for i, l in enumerate(leaves) if l.type in OPENING_BRACKETS\n        )\n    except StopIteration:\n        return set()\n    bracket_stack = []\n    ids = set()\n    for i in range(start_index, len(leaves)):\n        leaf = leaves[i]\n        if leaf.type in OPENING_BRACKETS:\n            bracket_stack.append((BRACKET[leaf.type], i))\n        if leaf.type in CLOSING_BRACKETS:\n            if bracket_stack and leaf.type == bracket_stack[-1][0]:\n                _, start = bracket_stack.pop()\n                for j in range(start, i + 1):\n                    ids.add(id(leaves[j]))\n            else:\n                break\n    return ids",
      "importString": "from collections.abc import Iterable, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Final, Optional, Union\nfrom black.nodes import (\nBRACKET\nCLOSING_BRACKETS\nCOMPARATORS\nLOGIC_OPERATORS\nMATH_OPERATORS\nOPENING_BRACKETS\nUNPACKING_PARENTS\nVARARGS_PARENTS\nis_vararg\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 26,
      "relativeDocumentPath": "src/black/brackets.py"
    },
    {
      "symbolName": "get_cache_dir",
      "sourceCode": "def get_cache_dir() -> Path:\n    \"\"\"Get the cache directory used by black.\n\n    Users can customize this directory on all systems using `BLACK_CACHE_DIR`\n    environment variable. By default, the cache directory is the user cache directory\n    under the black application.\n\n    This result is immediately set to a constant `black.cache.CACHE_DIR` as to avoid\n    repeated calls.\n    \"\"\"\n    # NOTE: Function mostly exists as a clean way to test getting the cache directory.\n    default_cache_dir = user_cache_dir(\"black\")\n    cache_dir = Path(os.environ.get(\"BLACK_CACHE_DIR\", default_cache_dir))\n    cache_dir = cache_dir / __version__\n    return cache_dir",
      "importString": "import hashlib\nimport os\nimport pickle\nimport sys\nimport tempfile\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import NamedTuple\n\nfrom platformdirs import user_cache_dir\n\nfrom _black_version import version as __version__\nfrom black.mode import Mode\nfrom black.output import err",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/cache.py"
    },
    {
      "symbolName": "read",
      "sourceCode": "@classmethod\n    def read(cls, mode: Mode) -> Self:\n        \"\"\"Read the cache if it exists and is well-formed.\n\n        If it is not well-formed, the call to write later should\n        resolve the issue.\n        \"\"\"\n        cache_file = get_cache_file(mode)\n        try:\n            exists = cache_file.exists()\n        except OSError as e:\n            # Likely file too long; see #4172 and #4174\n            err(f\"Unable to read cache file {cache_file} due to {e}\")\n            return cls(mode, cache_file)\n        if not exists:\n            return cls(mode, cache_file)\n\n        with cache_file.open(\"rb\") as fobj:\n            try:\n                data: dict[str, tuple[float, int, str]] = pickle.load(fobj)\n                file_data = {k: FileData(*v) for k, v in data.items()}\n            except (pickle.UnpicklingError, ValueError, IndexError):\n                return cls(mode, cache_file)\n\n        return cls(mode, cache_file, file_data)",
      "importString": "import hashlib\nimport os\nimport pickle\nimport sys\nimport tempfile\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import NamedTuple\n\nfrom platformdirs import user_cache_dir\n\nfrom _black_version import version as __version__\nfrom black.mode import Mode\nfrom black.output import err",
      "lineNum": 24,
      "relativeDocumentPath": "src/black/cache.py"
    },
    {
      "symbolName": "is_changed",
      "sourceCode": "def is_changed(self, source: Path) -> bool:\n        \"\"\"Check if source has changed compared to cached version.\"\"\"\n        res_src = source.resolve()\n        old = self.file_data.get(str(res_src))\n        if old is None:\n            return True\n\n        st = res_src.stat()\n        if st.st_size != old.st_size:\n            return True\n        if st.st_mtime != old.st_mtime:\n            new_hash = Cache.hash_digest(res_src)\n            if new_hash != old.hash:\n                return True\n        return False",
      "importString": "import hashlib\nimport os\nimport pickle\nimport sys\nimport tempfile\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import NamedTuple\n\nfrom platformdirs import user_cache_dir\n\nfrom _black_version import version as __version__\nfrom black.mode import Mode\nfrom black.output import err",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/cache.py"
    },
    {
      "symbolName": "filtered_cached",
      "sourceCode": "def filtered_cached(self, sources: Iterable[Path]) -> tuple[set[Path], set[Path]]:\n        \"\"\"Split an iterable of paths in `sources` into two sets.\n\n        The first contains paths of files that modified on disk or are not in the\n        cache. The other contains paths to non-modified files.\n        \"\"\"\n        changed: set[Path] = set()\n        done: set[Path] = set()\n        for src in sources:\n            if self.is_changed(src):\n                changed.add(src)\n            else:\n                done.add(src)\n        return changed, done",
      "importString": "import hashlib\nimport os\nimport pickle\nimport sys\nimport tempfile\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import NamedTuple\n\nfrom platformdirs import user_cache_dir\n\nfrom _black_version import version as __version__\nfrom black.mode import Mode\nfrom black.output import err",
      "lineNum": 13,
      "relativeDocumentPath": "src/black/cache.py"
    },
    {
      "symbolName": "write",
      "sourceCode": "def write(self, sources: Iterable[Path]) -> None:\n        \"\"\"Update the cache file data and write a new cache file.\"\"\"\n        self.file_data.update(\n            **{str(src.resolve()): Cache.get_file_data(src) for src in sources}\n        )\n        try:\n            CACHE_DIR.mkdir(parents=True, exist_ok=True)\n            with tempfile.NamedTemporaryFile(\n                dir=str(self.cache_file.parent), delete=False\n            ) as f:\n                # We store raw tuples in the cache because it's faster.\n                data: dict[str, tuple[float, int, str]] = {\n                    k: (*v,) for k, v in self.file_data.items()\n                }\n                pickle.dump(data, f, protocol=4)\n            os.replace(f.name, self.cache_file)\n        except OSError:\n            pass",
      "importString": "import hashlib\nimport os\nimport pickle\nimport sys\nimport tempfile\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import NamedTuple\n\nfrom platformdirs import user_cache_dir\n\nfrom _black_version import version as __version__\nfrom black.mode import Mode\nfrom black.output import err",
      "lineNum": 17,
      "relativeDocumentPath": "src/black/cache.py"
    },
    {
      "symbolName": "generate_comments",
      "sourceCode": "def generate_comments(leaf: LN) -> Iterator[Leaf]:\n    \"\"\"Clean the prefix of the `leaf` and generate comments from it, if any.\n\n    Comments in lib2to3 are shoved into the whitespace prefix.  This happens\n    in `pgen2/driver.py:Driver.parse_tokens()`.  This was a brilliant implementation\n    move because it does away with modifying the grammar to include all the\n    possible places in which comments can be placed.\n\n    The sad consequence for us though is that comments don't \"belong\" anywhere.\n    This is why this function generates simple parentless Leaf objects for\n    comments.  We simply don't know what the correct parent should be.\n\n    No matter though, we can live without this.  We really only need to\n    differentiate between inline and standalone comments.  The latter don't\n    share the line with any code.\n\n    Inline comments are emitted as regular token.COMMENT leaves.  Standalone\n    are emitted with a fake STANDALONE_COMMENT token identifier.\n    \"\"\"\n    total_consumed = 0\n    for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):\n        total_consumed = pc.consumed\n        prefix = make_simple_prefix(pc.newlines, pc.form_feed)\n        yield Leaf(pc.type, pc.value, prefix=prefix)\n    normalize_trailing_prefix(leaf, total_consumed)",
      "importString": "import re\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import Final, Optional, Union\n\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nSTANDALONE_COMMENT\nWHITESPACE\ncontainer_of\nfirst_leaf_of\nmake_simple_prefix\npreceding_leaf\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 24,
      "relativeDocumentPath": "src/black/comments.py"
    },
    {
      "symbolName": "list_comments",
      "sourceCode": "@lru_cache(maxsize=4096)\ndef list_comments(prefix: str, *, is_endmarker: bool) -> list[ProtoComment]:\n    \"\"\"Return a list of :class:`ProtoComment` objects parsed from the given `prefix`.\"\"\"\n    result: list[ProtoComment] = []\n    if not prefix or \"#\" not in prefix:\n        return result\n\n    consumed = 0\n    nlines = 0\n    ignored_lines = 0\n    form_feed = False\n    for index, full_line in enumerate(re.split(\"\\r?\\n\", prefix)):\n        consumed += len(full_line) + 1  # adding the length of the split '\\n'\n        match = re.match(r\"^(\\s*)(\\S.*|)$\", full_line)\n        assert match\n        whitespace, line = match.groups()\n        if not line:\n            nlines += 1\n            if \"\\f\" in full_line:\n                form_feed = True\n        if not line.startswith(\"#\"):\n            # Escaped newlines outside of a comment are not really newlines at\n            # all. We treat a single-line comment following an escaped newline\n            # as a simple trailing comment.\n            if line.endswith(\"\\\\\"):\n                ignored_lines += 1\n            continue\n\n        if index == ignored_lines and not is_endmarker:\n            comment_type = token.COMMENT  # simple trailing comment\n        else:\n            comment_type = STANDALONE_COMMENT\n        comment = make_comment(line)\n        result.append(\n            ProtoComment(\n                type=comment_type,\n                value=comment,\n                newlines=nlines,\n                consumed=consumed,\n                form_feed=form_feed,\n                leading_whitespace=whitespace,\n            )\n        )\n        form_feed = False\n        nlines = 0\n    return result",
      "importString": "import re\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import Final, Optional, Union\n\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nSTANDALONE_COMMENT\nWHITESPACE\ncontainer_of\nfirst_leaf_of\nmake_simple_prefix\npreceding_leaf\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 45,
      "relativeDocumentPath": "src/black/comments.py"
    },
    {
      "symbolName": "normalize_trailing_prefix",
      "sourceCode": "def normalize_trailing_prefix(leaf: LN, total_consumed: int) -> None:\n    \"\"\"Normalize the prefix that's left over after generating comments.\n\n    Note: don't use backslashes for formatting or you'll lose your voting rights.\n    \"\"\"\n    remainder = leaf.prefix[total_consumed:]\n    if \"\\\\\" not in remainder:\n        nl_count = remainder.count(\"\\n\")\n        form_feed = \"\\f\" in remainder and remainder.endswith(\"\\n\")\n        leaf.prefix = make_simple_prefix(nl_count, form_feed)\n        return\n\n    leaf.prefix = \"\"",
      "importString": "import re\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import Final, Optional, Union\n\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nSTANDALONE_COMMENT\nWHITESPACE\ncontainer_of\nfirst_leaf_of\nmake_simple_prefix\npreceding_leaf\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 12,
      "relativeDocumentPath": "src/black/comments.py"
    },
    {
      "symbolName": "make_comment",
      "sourceCode": "def make_comment(content: str) -> str:\n    \"\"\"Return a consistently formatted comment from the given `content` string.\n\n    All comments (except for \"##\", \"#!\", \"#:\", '#'\") should have a single\n    space between the hash sign and the content.\n\n    If `content` didn't start with a hash sign, one is provided.\n    \"\"\"\n    content = content.rstrip()\n    if not content:\n        return \"#\"\n\n    if content[0] == \"#\":\n        content = content[1:]\n    NON_BREAKING_SPACE = \"Â \"\n    if (\n        content\n        and content[0] == NON_BREAKING_SPACE\n        and not content.lstrip().startswith(\"type:\")\n    ):\n        content = \" \" + content[1:]  # Replace NBSP by a simple space\n    if content and content[0] not in COMMENT_EXCEPTIONS:\n        content = \" \" + content\n    return \"#\" + content",
      "importString": "import re\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import Final, Optional, Union\n\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nSTANDALONE_COMMENT\nWHITESPACE\ncontainer_of\nfirst_leaf_of\nmake_simple_prefix\npreceding_leaf\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 23,
      "relativeDocumentPath": "src/black/comments.py"
    },
    {
      "symbolName": "convert_one_fmt_off_pair",
      "sourceCode": "def convert_one_fmt_off_pair(\n    node: Node, mode: Mode, lines: Collection[tuple[int, int]]\n) -> bool:\n    \"\"\"Convert content of a single `# fmt: off`/`# fmt: on` into a standalone comment.\n\n    Returns True if a pair was converted.\n    \"\"\"\n    for leaf in node.leaves():\n        previous_consumed = 0\n        for comment in list_comments(leaf.prefix, is_endmarker=False):\n            is_fmt_off = comment.value in FMT_OFF\n            is_fmt_skip = _contains_fmt_skip_comment(comment.value, mode)\n            if (not is_fmt_off and not is_fmt_skip) or (\n                # Invalid use when `# fmt: off` is applied before a closing bracket.\n                is_fmt_off\n                and leaf.type in CLOSING_BRACKETS\n            ):\n                previous_consumed = comment.consumed\n                continue\n            # We only want standalone comments. If there's no previous leaf or\n            # the previous leaf is indentation, it's a standalone comment in\n            # disguise.\n            if comment.type != STANDALONE_COMMENT:\n                prev = preceding_leaf(leaf)\n                if prev:\n                    if is_fmt_off and prev.type not in WHITESPACE:\n                        continue\n                    if is_fmt_skip and prev.type in WHITESPACE:\n                        continue\n\n            ignored_nodes = list(generate_ignored_nodes(leaf, comment, mode))\n            if not ignored_nodes:\n                continue\n\n            first = ignored_nodes[0]  # Can be a container node with the `leaf`.\n            parent = first.parent\n            prefix = first.prefix\n            if comment.value in FMT_OFF:\n                first.prefix = prefix[comment.consumed :]\n            if is_fmt_skip:\n                first.prefix = \"\"\n                standalone_comment_prefix = prefix\n            else:\n                standalone_comment_prefix = (\n                    prefix[:previous_consumed] + \"\\n\" * comment.newlines\n                )\n            hidden_value = \"\".join(str(n) for n in ignored_nodes)\n            comment_lineno = leaf.lineno - comment.newlines\n            if comment.value in FMT_OFF:\n                fmt_off_prefix = \"\"\n                if len(lines) > 0 and not any(\n                    line[0] <= comment_lineno <= line[1] for line in lines\n                ):\n                    # keeping indentation of comment by preserving original whitespaces.\n                    fmt_off_prefix = prefix.split(comment.value)[0]\n                    if \"\\n\" in fmt_off_prefix:\n                        fmt_off_prefix = fmt_off_prefix.split(\"\\n\")[-1]\n                standalone_comment_prefix += fmt_off_prefix\n                hidden_value = comment.value + \"\\n\" + hidden_value\n            if is_fmt_skip:\n                hidden_value += (\n                    comment.leading_whitespace\n                    if Preview.no_normalize_fmt_skip_whitespace in mode\n                    else \"  \"\n                ) + comment.value\n            if hidden_value.endswith(\"\\n\"):\n                # That happens when one of the `ignored_nodes` ended with a NEWLINE\n                # leaf (possibly followed by a DEDENT).\n                hidden_value = hidden_value[:-1]\n            first_idx: Optional[int] = None\n            for ignored in ignored_nodes:\n                index = ignored.remove()\n                if first_idx is None:\n                    first_idx = index\n            assert parent is not None, \"INTERNAL ERROR: fmt: on/off handling (1)\"\n            assert first_idx is not None, \"INTERNAL ERROR: fmt: on/off handling (2)\"\n            parent.insert_child(\n                first_idx,\n                Leaf(\n                    STANDALONE_COMMENT,\n                    hidden_value,\n                    prefix=standalone_comment_prefix,\n                    fmt_pass_converted_first_leaf=first_leaf_of(first),\n                ),\n            )\n            return True\n\n    return False",
      "importString": "import re\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import Final, Optional, Union\n\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nSTANDALONE_COMMENT\nWHITESPACE\ncontainer_of\nfirst_leaf_of\nmake_simple_prefix\npreceding_leaf\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 87,
      "relativeDocumentPath": "src/black/comments.py"
    },
    {
      "symbolName": "generate_ignored_nodes",
      "sourceCode": "def generate_ignored_nodes(\n    leaf: Leaf, comment: ProtoComment, mode: Mode\n) -> Iterator[LN]:\n    \"\"\"Starting from the container of `leaf`, generate all leaves until `# fmt: on`.\n\n    If comment is skip, returns leaf only.\n    Stops at the end of the block.\n    \"\"\"\n    if _contains_fmt_skip_comment(comment.value, mode):\n        yield from _generate_ignored_nodes_from_fmt_skip(leaf, comment)\n        return\n    container: Optional[LN] = container_of(leaf)\n    while container is not None and container.type != token.ENDMARKER:\n        if is_fmt_on(container):\n            return\n\n        # fix for fmt: on in children\n        if children_contains_fmt_on(container):\n            for index, child in enumerate(container.children):\n                if isinstance(child, Leaf) and is_fmt_on(child):\n                    if child.type in CLOSING_BRACKETS:\n                        # This means `# fmt: on` is placed at a different bracket level\n                        # than `# fmt: off`. This is an invalid use, but as a courtesy,\n                        # we include this closing bracket in the ignored nodes.\n                        # The alternative is to fail the formatting.\n                        yield child\n                    return\n                if (\n                    child.type == token.INDENT\n                    and index < len(container.children) - 1\n                    and children_contains_fmt_on(container.children[index + 1])\n                ):\n                    # This means `# fmt: on` is placed right after an indentation\n                    # level, and we shouldn't swallow the previous INDENT token.\n                    return\n                if children_contains_fmt_on(child):\n                    return\n                yield child\n        else:\n            if container.type == token.DEDENT and container.next_sibling is None:\n                # This can happen when there is no matching `# fmt: on` comment at the\n                # same level as `# fmt: on`. We need to keep this DEDENT.\n                return\n            yield container\n            container = container.next_sibling",
      "importString": "import re\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import Final, Optional, Union\n\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nSTANDALONE_COMMENT\nWHITESPACE\ncontainer_of\nfirst_leaf_of\nmake_simple_prefix\npreceding_leaf\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 44,
      "relativeDocumentPath": "src/black/comments.py"
    },
    {
      "symbolName": "_generate_ignored_nodes_from_fmt_skip",
      "sourceCode": "def _generate_ignored_nodes_from_fmt_skip(\n    leaf: Leaf, comment: ProtoComment\n) -> Iterator[LN]:\n    \"\"\"Generate all leaves that should be ignored by the `# fmt: skip` from `leaf`.\"\"\"\n    prev_sibling = leaf.prev_sibling\n    parent = leaf.parent\n    # Need to properly format the leaf prefix to compare it to comment.value,\n    # which is also formatted\n    comments = list_comments(leaf.prefix, is_endmarker=False)\n    if not comments or comment.value != comments[0].value:\n        return\n    if prev_sibling is not None:\n        leaf.prefix = \"\"\n        siblings = [prev_sibling]\n        while \"\\n\" not in prev_sibling.prefix and prev_sibling.prev_sibling is not None:\n            prev_sibling = prev_sibling.prev_sibling\n            siblings.insert(0, prev_sibling)\n        yield from siblings\n    elif (\n        parent is not None and parent.type == syms.suite and leaf.type == token.NEWLINE\n    ):\n        # The `# fmt: skip` is on the colon line of the if/while/def/class/...\n        # statements. The ignored nodes should be previous siblings of the\n        # parent suite node.\n        leaf.prefix = \"\"\n        ignored_nodes: list[LN] = []\n        parent_sibling = parent.prev_sibling\n        while parent_sibling is not None and parent_sibling.type != syms.suite:\n            ignored_nodes.insert(0, parent_sibling)\n            parent_sibling = parent_sibling.prev_sibling\n        # Special case for `async_stmt` where the ASYNC token is on the\n        # grandparent node.\n        grandparent = parent.parent\n        if (\n            grandparent is not None\n            and grandparent.prev_sibling is not None\n            and grandparent.prev_sibling.type == token.ASYNC\n        ):\n            ignored_nodes.insert(0, grandparent.prev_sibling)\n        yield from iter(ignored_nodes)",
      "importString": "import re\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import Final, Optional, Union\n\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nSTANDALONE_COMMENT\nWHITESPACE\ncontainer_of\nfirst_leaf_of\nmake_simple_prefix\npreceding_leaf\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 39,
      "relativeDocumentPath": "src/black/comments.py"
    },
    {
      "symbolName": "is_fmt_on",
      "sourceCode": "def is_fmt_on(container: LN) -> bool:\n    \"\"\"Determine whether formatting is switched on within a container.\n    Determined by whether the last `# fmt:` comment is `on` or `off`.\n    \"\"\"\n    fmt_on = False\n    for comment in list_comments(container.prefix, is_endmarker=False):\n        if comment.value in FMT_ON:\n            fmt_on = True\n        elif comment.value in FMT_OFF:\n            fmt_on = False\n    return fmt_on",
      "importString": "import re\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import Final, Optional, Union\n\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nSTANDALONE_COMMENT\nWHITESPACE\ncontainer_of\nfirst_leaf_of\nmake_simple_prefix\npreceding_leaf\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 10,
      "relativeDocumentPath": "src/black/comments.py"
    },
    {
      "symbolName": "contains_pragma_comment",
      "sourceCode": "def contains_pragma_comment(comment_list: list[Leaf]) -> bool:\n    \"\"\"\n    Returns:\n        True iff one of the comments in @comment_list is a pragma used by one\n        of the more common static analysis tools for python (e.g. mypy, flake8,\n        pylint).\n    \"\"\"\n    for comment in comment_list:\n        if comment.value.startswith((\"# type:\", \"# noqa\", \"# pylint:\")):\n            return True\n\n    return False",
      "importString": "import re\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import Final, Optional, Union\n\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nSTANDALONE_COMMENT\nWHITESPACE\ncontainer_of\nfirst_leaf_of\nmake_simple_prefix\npreceding_leaf\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/comments.py"
    },
    {
      "symbolName": "_contains_fmt_skip_comment",
      "sourceCode": "def _contains_fmt_skip_comment(comment_line: str, mode: Mode) -> bool:\n    \"\"\"\n    Checks if the given comment contains FMT_SKIP alone or paired with other comments.\n    Matching styles:\n      # fmt:skip                           <-- single comment\n      # noqa:XXX # fmt:skip # a nice line  <-- multiple comments (Preview)\n      # pylint:XXX; fmt:skip               <-- list of comments (; separated, Preview)\n    \"\"\"\n    semantic_comment_blocks = [\n        comment_line,\n        *[\n            _COMMENT_PREFIX + comment.strip()\n            for comment in comment_line.split(_COMMENT_PREFIX)[1:]\n        ],\n        *[\n            _COMMENT_PREFIX + comment.strip()\n            for comment in comment_line.strip(_COMMENT_PREFIX).split(\n                _COMMENT_LIST_SEPARATOR\n            )\n        ],\n    ]\n\n    return any(comment in FMT_SKIP for comment in semantic_comment_blocks)",
      "importString": "import re\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import Final, Optional, Union\n\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nSTANDALONE_COMMENT\nWHITESPACE\ncontainer_of\nfirst_leaf_of\nmake_simple_prefix\npreceding_leaf\nsyms\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 22,
      "relativeDocumentPath": "src/black/comments.py"
    },
    {
      "symbolName": "maybe_install_uvloop",
      "sourceCode": "def maybe_install_uvloop() -> None:\n    \"\"\"If our environment has uvloop installed we use it.\n\n    This is called only from command-line entry points to avoid\n    interfering with the parent process if Black is used as a library.\n    \"\"\"\n    try:\n        import uvloop\n\n        uvloop.install()\n    except ImportError:\n        pass",
      "importString": "import logging\nimport os\nimport signal\nimport sys\nimport traceback\nfrom collections.abc import Iterable\nfrom concurrent.futures import Executor, ProcessPoolExecutor, ThreadPoolExecutor\nfrom multiprocessing import Manager\nfrom pathlib import Path\nfrom typing import Any, Optional\n\nfrom mypy_extensions import mypyc_attr\n\nfrom black import WriteBack, format_file_in_place\nfrom black.cache import Cache\nfrom black.mode import Mode\nfrom black.output import err\nfrom black.report import Changed, Report",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/concurrency.py"
    },
    {
      "symbolName": "shutdown",
      "sourceCode": "def shutdown(loop: asyncio.AbstractEventLoop) -> None:\n    \"\"\"Cancel all pending tasks on `loop`, wait for them, and close the loop.\"\"\"\n    try:\n        # This part is borrowed from asyncio/runners.py in Python 3.7b2.\n        to_cancel = [task for task in asyncio.all_tasks(loop) if not task.done()]\n        if not to_cancel:\n            return\n\n        for task in to_cancel:\n            task.cancel()\n        loop.run_until_complete(asyncio.gather(*to_cancel, return_exceptions=True))\n    finally:\n        # `concurrent.futures.Future` objects cannot be cancelled once they\n        # are already running. There might be some when the `shutdown()` happened.\n        # Silence their logger's spew about the event loop being closed.\n        cf_logger = logging.getLogger(\"concurrent.futures\")\n        cf_logger.setLevel(logging.CRITICAL)\n        loop.close()",
      "importString": "import logging\nimport os\nimport signal\nimport sys\nimport traceback\nfrom collections.abc import Iterable\nfrom concurrent.futures import Executor, ProcessPoolExecutor, ThreadPoolExecutor\nfrom multiprocessing import Manager\nfrom pathlib import Path\nfrom typing import Any, Optional\n\nfrom mypy_extensions import mypyc_attr\n\nfrom black import WriteBack, format_file_in_place\nfrom black.cache import Cache\nfrom black.mode import Mode\nfrom black.output import err\nfrom black.report import Changed, Report",
      "lineNum": 17,
      "relativeDocumentPath": "src/black/concurrency.py"
    },
    {
      "symbolName": "reformat_many",
      "sourceCode": "@mypyc_attr(patchable=True)\ndef reformat_many(\n    sources: set[Path],\n    fast: bool,\n    write_back: WriteBack,\n    mode: Mode,\n    report: Report,\n    workers: Optional[int],\n) -> None:\n    \"\"\"Reformat multiple files using a ProcessPoolExecutor.\"\"\"\n    maybe_install_uvloop()\n\n    executor: Executor\n    if workers is None:\n        workers = int(os.environ.get(\"BLACK_NUM_WORKERS\", 0))\n        workers = workers or os.cpu_count() or 1\n    if sys.platform == \"win32\":\n        # Work around https://bugs.python.org/issue26903\n        workers = min(workers, 60)\n    try:\n        executor = ProcessPoolExecutor(max_workers=workers)\n    except (ImportError, NotImplementedError, OSError):\n        # we arrive here if the underlying system does not support multi-processing\n        # like in AWS Lambda or Termux, in which case we gracefully fallback to\n        # a ThreadPoolExecutor with just a single worker (more workers would not do us\n        # any good due to the Global Interpreter Lock)\n        executor = ThreadPoolExecutor(max_workers=1)\n\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        loop.run_until_complete(\n            schedule_formatting(\n                sources=sources,\n                fast=fast,\n                write_back=write_back,\n                mode=mode,\n                report=report,\n                loop=loop,\n                executor=executor,\n            )\n        )\n    finally:\n        try:\n            shutdown(loop)\n        finally:\n            asyncio.set_event_loop(None)\n        if executor is not None:\n            executor.shutdown()",
      "importString": "import logging\nimport os\nimport signal\nimport sys\nimport traceback\nfrom collections.abc import Iterable\nfrom concurrent.futures import Executor, ProcessPoolExecutor, ThreadPoolExecutor\nfrom multiprocessing import Manager\nfrom pathlib import Path\nfrom typing import Any, Optional\n\nfrom mypy_extensions import mypyc_attr\n\nfrom black import WriteBack, format_file_in_place\nfrom black.cache import Cache\nfrom black.mode import Mode\nfrom black.output import err\nfrom black.report import Changed, Report",
      "lineNum": 48,
      "relativeDocumentPath": "src/black/concurrency.py"
    },
    {
      "symbolName": "schedule_formatting",
      "sourceCode": "async def schedule_formatting(\n    sources: set[Path],\n    fast: bool,\n    write_back: WriteBack,\n    mode: Mode,\n    report: \"Report\",\n    loop: asyncio.AbstractEventLoop,\n    executor: \"Executor\",\n) -> None:\n    \"\"\"Run formatting of `sources` in parallel using the provided `executor`.\n\n    (Use ProcessPoolExecutors for actual parallelism.)\n\n    `write_back`, `fast`, and `mode` options are passed to\n    :func:`format_file_in_place`.\n    \"\"\"\n    cache = Cache.read(mode)\n    if write_back not in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\n        sources, cached = cache.filtered_cached(sources)\n        for src in sorted(cached):\n            report.done(src, Changed.CACHED)\n    if not sources:\n        return\n\n    cancelled = []\n    sources_to_cache = []\n    lock = None\n    if write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):\n        # For diff output, we need locks to ensure we don't interleave output\n        # from different processes.\n        manager = Manager()\n        lock = manager.Lock()\n    tasks = {\n        asyncio.ensure_future(\n            loop.run_in_executor(\n                executor, format_file_in_place, src, fast, mode, write_back, lock\n            )\n        ): src\n        for src in sorted(sources)\n    }\n    pending = tasks.keys()\n    try:\n        loop.add_signal_handler(signal.SIGINT, cancel, pending)\n        loop.add_signal_handler(signal.SIGTERM, cancel, pending)\n    except NotImplementedError:\n        # There are no good alternatives for these on Windows.\n        pass\n    while pending:\n        done, _ = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n        for task in done:\n            src = tasks.pop(task)\n            if task.cancelled():\n                cancelled.append(task)\n            elif exc := task.exception():\n                if report.verbose:\n                    traceback.print_exception(type(exc), exc, exc.__traceback__)\n                report.failed(src, str(exc))\n            else:\n                changed = Changed.YES if task.result() else Changed.NO\n                # If the file was written back or was successfully checked as\n                # well-formatted, store this information in the cache.\n                if write_back is WriteBack.YES or (\n                    write_back is WriteBack.CHECK and changed is Changed.NO\n                ):\n                    sources_to_cache.append(src)\n                report.done(src, changed)\n    if cancelled:\n        await asyncio.gather(*cancelled, return_exceptions=True)\n    if sources_to_cache:\n        cache.write(sources_to_cache)",
      "importString": "import logging\nimport os\nimport signal\nimport sys\nimport traceback\nfrom collections.abc import Iterable\nfrom concurrent.futures import Executor, ProcessPoolExecutor, ThreadPoolExecutor\nfrom multiprocessing import Manager\nfrom pathlib import Path\nfrom typing import Any, Optional\n\nfrom mypy_extensions import mypyc_attr\n\nfrom black import WriteBack, format_file_in_place\nfrom black.cache import Cache\nfrom black.mode import Mode\nfrom black.output import err\nfrom black.report import Changed, Report",
      "lineNum": 69,
      "relativeDocumentPath": "src/black/concurrency.py"
    },
    {
      "symbolName": "visit_default",
      "sourceCode": "def visit_default(self, node: LN) -> Iterator[T]:\n        indent = \" \" * (2 * self.tree_depth)\n        if isinstance(node, Node):\n            _type = type_repr(node.type)\n            self.out(f\"{indent}{_type}\", fg=\"yellow\")\n            self.tree_depth += 1\n            for child in node.children:\n                yield from self.visit(child)\n\n            self.tree_depth -= 1\n            self.out(f\"{indent}/{_type}\", fg=\"yellow\", bold=False)\n        else:\n            _type = token.tok_name.get(node.type, str(node.type))\n            self.out(f\"{indent}{_type}\", fg=\"blue\", nl=False)\n            if node.prefix:\n                # We don't have to handle prefixes for `Node` objects since\n                # that delegates to the first child anyway.\n                self.out(f\" {node.prefix!r}\", fg=\"green\", bold=False, nl=False)\n            self.out(f\" {node.value!r}\", fg=\"blue\", bold=False)",
      "importString": "from collections.abc import Iterator\nfrom dataclasses import dataclass, field\nfrom typing import Any, TypeVar, Union\n\nfrom black.nodes import Visitor\nfrom black.output import out\nfrom black.parsing import lib2to3_parse\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node, type_repr",
      "lineNum": 18,
      "relativeDocumentPath": "src/black/debug.py"
    },
    {
      "symbolName": "find_project_root",
      "sourceCode": "@lru_cache\ndef find_project_root(\n    srcs: Sequence[str], stdin_filename: Optional[str] = None\n) -> tuple[Path, str]:\n    \"\"\"Return a directory containing .git, .hg, or pyproject.toml.\n\n    pyproject.toml files are only considered if they contain a [tool.black]\n    section and are ignored otherwise.\n\n    That directory will be a common parent of all files and directories\n    passed in `srcs`.\n\n    If no directory in the tree contains a marker that would specify it's the\n    project root, the root of the file system is returned.\n\n    Returns a two-tuple with the first element as the project root path and\n    the second element as a string describing the method by which the\n    project root was discovered.\n    \"\"\"\n    if stdin_filename is not None:\n        srcs = tuple(stdin_filename if s == \"-\" else s for s in srcs)\n    if not srcs:\n        srcs = [str(_cached_resolve(Path.cwd()))]\n\n    path_srcs = [_cached_resolve(Path(Path.cwd(), src)) for src in srcs]\n\n    # A list of lists of parents for each 'src'. 'src' is included as a\n    # \"parent\" of itself if it is a directory\n    src_parents = [\n        list(path.parents) + ([path] if path.is_dir() else []) for path in path_srcs\n    ]\n\n    common_base = max(\n        set.intersection(*(set(parents) for parents in src_parents)),\n        key=lambda path: path.parts,\n    )\n\n    for directory in (common_base, *common_base.parents):\n        if (directory / \".git\").exists():\n            return directory, \".git directory\"\n\n        if (directory / \".hg\").is_dir():\n            return directory, \".hg directory\"\n\n        if (directory / \"pyproject.toml\").is_file():\n            pyproject_toml = _load_toml(directory / \"pyproject.toml\")\n            if \"black\" in pyproject_toml.get(\"tool\", {}):\n                return directory, \"pyproject.toml\"\n\n    return directory, \"file system root\"",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 49,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "find_pyproject_toml",
      "sourceCode": "def find_pyproject_toml(\n    path_search_start: tuple[str, ...], stdin_filename: Optional[str] = None\n) -> Optional[str]:\n    \"\"\"Find the absolute filepath to a pyproject.toml if it exists\"\"\"\n    path_project_root, _ = find_project_root(path_search_start, stdin_filename)\n    path_pyproject_toml = path_project_root / \"pyproject.toml\"\n    if path_pyproject_toml.is_file():\n        return str(path_pyproject_toml)\n\n    try:\n        path_user_pyproject_toml = find_user_pyproject_toml()\n        return (\n            str(path_user_pyproject_toml)\n            if path_user_pyproject_toml.is_file()\n            else None\n        )\n    except (PermissionError, RuntimeError) as e:\n        # We do not have access to the user-level config directory, so ignore it.\n        err(f\"Ignoring user configuration directory due to {e!r}\")\n        return None",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 19,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "parse_pyproject_toml",
      "sourceCode": "@mypyc_attr(patchable=True)\ndef parse_pyproject_toml(path_config: str) -> dict[str, Any]:\n    \"\"\"Parse a pyproject toml file, pulling out relevant parts for Black.\n\n    If parsing fails, will raise a tomllib.TOMLDecodeError.\n    \"\"\"\n    pyproject_toml = _load_toml(path_config)\n    config: dict[str, Any] = pyproject_toml.get(\"tool\", {}).get(\"black\", {})\n    config = {k.replace(\"--\", \"\").replace(\"-\", \"_\"): v for k, v in config.items()}\n\n    if \"target_version\" not in config:\n        inferred_target_version = infer_target_version(pyproject_toml)\n        if inferred_target_version is not None:\n            config[\"target_version\"] = [v.name.lower() for v in inferred_target_version]\n\n    return config",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "infer_target_version",
      "sourceCode": "def infer_target_version(\n    pyproject_toml: dict[str, Any],\n) -> Optional[list[TargetVersion]]:\n    \"\"\"Infer Black's target version from the project metadata in pyproject.toml.\n\n    Supports the PyPA standard format (PEP 621):\n    https://packaging.python.org/en/latest/specifications/declaring-project-metadata/#requires-python\n\n    If the target version cannot be inferred, returns None.\n    \"\"\"\n    project_metadata = pyproject_toml.get(\"project\", {})\n    requires_python = project_metadata.get(\"requires-python\", None)\n    if requires_python is not None:\n        try:\n            return parse_req_python_version(requires_python)\n        except InvalidVersion:\n            pass\n        try:\n            return parse_req_python_specifier(requires_python)\n        except (InvalidSpecifier, InvalidVersion):\n            pass\n\n    return None",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 22,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "parse_req_python_version",
      "sourceCode": "def parse_req_python_version(requires_python: str) -> Optional[list[TargetVersion]]:\n    \"\"\"Parse a version string (i.e. ``\"3.7\"``) to a list of TargetVersion.\n\n    If parsing fails, will raise a packaging.version.InvalidVersion error.\n    If the parsed version cannot be mapped to a valid TargetVersion, returns None.\n    \"\"\"\n    version = Version(requires_python)\n    if version.release[0] != 3:\n        return None\n    try:\n        return [TargetVersion(version.release[1])]\n    except (IndexError, ValueError):\n        return None",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 12,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "parse_req_python_specifier",
      "sourceCode": "def parse_req_python_specifier(requires_python: str) -> Optional[list[TargetVersion]]:\n    \"\"\"Parse a specifier string (i.e. ``\">=3.7,<3.10\"``) to a list of TargetVersion.\n\n    If parsing fails, will raise a packaging.specifiers.InvalidSpecifier error.\n    If the parsed specifier cannot be mapped to a valid TargetVersion, returns None.\n    \"\"\"\n    specifier_set = strip_specifier_set(SpecifierSet(requires_python))\n    if not specifier_set:\n        return None\n\n    target_version_map = {f\"3.{v.value}\": v for v in TargetVersion}\n    compatible_versions: list[str] = list(specifier_set.filter(target_version_map))\n    if compatible_versions:\n        return [target_version_map[v] for v in compatible_versions]\n    return None",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "strip_specifier_set",
      "sourceCode": "def strip_specifier_set(specifier_set: SpecifierSet) -> SpecifierSet:\n    \"\"\"Strip minor versions for some specifiers in the specifier set.\n\n    For background on version specifiers, see PEP 440:\n    https://peps.python.org/pep-0440/#version-specifiers\n    \"\"\"\n    specifiers = []\n    for s in specifier_set:\n        if \"*\" in str(s):\n            specifiers.append(s)\n        elif s.operator in [\"~=\", \"==\", \">=\", \"===\"]:\n            version = Version(s.version)\n            stripped = Specifier(f\"{s.operator}{version.major}.{version.minor}\")\n            specifiers.append(stripped)\n        elif s.operator == \">\":\n            version = Version(s.version)\n            if len(version.release) > 2:\n                s = Specifier(f\">={version.major}.{version.minor}\")\n            specifiers.append(s)\n        else:\n            specifiers.append(s)\n\n    return SpecifierSet(\",\".join(str(s) for s in specifiers))",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 22,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "find_user_pyproject_toml",
      "sourceCode": "@lru_cache\ndef find_user_pyproject_toml() -> Path:\n    r\"\"\"Return the path to the top-level user configuration for black.\n\n    This looks for ~\\.black on Windows and ~/.config/black on Linux and other\n    Unix systems.\n\n    May raise:\n    - RuntimeError: if the current user has no homedir\n    - PermissionError: if the current process cannot access the user's homedir\n    \"\"\"\n    if sys.platform == \"win32\":\n        # Windows\n        user_config_path = Path.home() / \".black\"\n    else:\n        config_root = os.environ.get(\"XDG_CONFIG_HOME\", \"~/.config\")\n        user_config_path = Path(config_root).expanduser() / \"black\"\n    return _cached_resolve(user_config_path)",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 17,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "get_gitignore",
      "sourceCode": "@lru_cache\ndef get_gitignore(root: Path) -> PathSpec:\n    \"\"\"Return a PathSpec matching gitignore content if present.\"\"\"\n    gitignore = root / \".gitignore\"\n    lines: list[str] = []\n    if gitignore.is_file():\n        with gitignore.open(encoding=\"utf-8\") as gf:\n            lines = gf.readlines()\n    try:\n        return PathSpec.from_lines(\"gitwildmatch\", lines)\n    except GitWildMatchPatternError as e:\n        err(f\"Could not parse {gitignore}: {e}\")\n        raise",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 12,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "resolves_outside_root_or_cannot_stat",
      "sourceCode": "def resolves_outside_root_or_cannot_stat(\n    path: Path,\n    root: Path,\n    report: Optional[Report] = None,\n) -> bool:\n    \"\"\"\n    Returns whether the path is a symbolic link that points outside the\n    root directory. Also returns True if we failed to resolve the path.\n    \"\"\"\n    try:\n        resolved_path = _cached_resolve(path)\n    except OSError as e:\n        if report:\n            report.path_ignored(path, f\"cannot be read because {e}\")\n        return True\n    try:\n        resolved_path.relative_to(root)\n    except ValueError:\n        if report:\n            report.path_ignored(path, f\"is a symbolic link that points outside {root}\")\n        return True\n    return False",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 21,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "best_effort_relative_path",
      "sourceCode": "def best_effort_relative_path(path: Path, root: Path) -> Path:\n    # Precondition: resolves_outside_root_or_cannot_stat(path, root) is False\n    try:\n        return path.absolute().relative_to(root)\n    except ValueError:\n        pass\n    root_parent = next((p for p in path.parents if _cached_resolve(p) == root), None)\n    if root_parent is not None:\n        return path.relative_to(root_parent)\n    # something adversarial, fallback to path guaranteed by precondition\n    return _cached_resolve(path).relative_to(root)",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 10,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "_path_is_ignored",
      "sourceCode": "def _path_is_ignored(\n    root_relative_path: str,\n    root: Path,\n    gitignore_dict: dict[Path, PathSpec],\n) -> bool:\n    path = root / root_relative_path\n    # Note that this logic is sensitive to the ordering of gitignore_dict. Callers must\n    # ensure that gitignore_dict is ordered from least specific to most specific.\n    for gitignore_path, pattern in gitignore_dict.items():\n        try:\n            relative_path = path.relative_to(gitignore_path).as_posix()\n            if path.is_dir():\n                relative_path = relative_path + \"/\"\n        except ValueError:\n            break\n        if pattern.match_file(relative_path):\n            return True\n    return False",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 17,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "gen_python_files",
      "sourceCode": "def gen_python_files(\n    paths: Iterable[Path],\n    root: Path,\n    include: Pattern[str],\n    exclude: Pattern[str],\n    extend_exclude: Optional[Pattern[str]],\n    force_exclude: Optional[Pattern[str]],\n    report: Report,\n    gitignore_dict: Optional[dict[Path, PathSpec]],\n    *,\n    verbose: bool,\n    quiet: bool,\n) -> Iterator[Path]:\n    \"\"\"Generate all files under `path` whose paths are not excluded by the\n    `exclude_regex`, `extend_exclude`, or `force_exclude` regexes,\n    but are included by the `include` regex.\n\n    Symbolic links pointing outside of the `root` directory are ignored.\n\n    `report` is where output about exclusions goes.\n    \"\"\"\n\n    assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n    for child in paths:\n        assert child.is_absolute()\n        root_relative_path = child.relative_to(root).as_posix()\n\n        # First ignore files matching .gitignore, if passed\n        if gitignore_dict and _path_is_ignored(\n            root_relative_path, root, gitignore_dict\n        ):\n            report.path_ignored(child, \"matches a .gitignore file content\")\n            continue\n\n        # Then ignore with `--exclude` `--extend-exclude` and `--force-exclude` options.\n        root_relative_path = \"/\" + root_relative_path\n        if child.is_dir():\n            root_relative_path += \"/\"\n\n        if path_is_excluded(root_relative_path, exclude):\n            report.path_ignored(child, \"matches the --exclude regular expression\")\n            continue\n\n        if path_is_excluded(root_relative_path, extend_exclude):\n            report.path_ignored(\n                child, \"matches the --extend-exclude regular expression\"\n            )\n            continue\n\n        if path_is_excluded(root_relative_path, force_exclude):\n            report.path_ignored(child, \"matches the --force-exclude regular expression\")\n            continue\n\n        if resolves_outside_root_or_cannot_stat(child, root, report):\n            continue\n\n        if child.is_dir():\n            # If gitignore is None, gitignore usage is disabled, while a Falsey\n            # gitignore is when the directory doesn't have a .gitignore file.\n            if gitignore_dict is not None:\n                new_gitignore_dict = {\n                    **gitignore_dict,\n                    root / child: get_gitignore(child),\n                }\n            else:\n                new_gitignore_dict = None\n            yield from gen_python_files(\n                child.iterdir(),\n                root,\n                include,\n                exclude,\n                extend_exclude,\n                force_exclude,\n                report,\n                new_gitignore_dict,\n                verbose=verbose,\n                quiet=quiet,\n            )\n\n        elif child.is_file():\n            if child.suffix == \".ipynb\" and not jupyter_dependencies_are_installed(\n                warn=verbose or not quiet\n            ):\n                continue\n            include_match = include.search(root_relative_path) if include else True\n            if include_match:\n                yield child",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 86,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "wrap_stream_for_windows",
      "sourceCode": "def wrap_stream_for_windows(\n    f: io.TextIOWrapper,\n) -> Union[io.TextIOWrapper, \"colorama.AnsiToWin32\"]:\n    \"\"\"\n    Wrap stream with colorama's wrap_stream so colors are shown on Windows.\n\n    If `colorama` is unavailable, the original stream is returned unmodified.\n    Otherwise, the `wrap_stream()` function determines whether the stream needs\n    to be wrapped for a Windows environment and will accordingly either return\n    an `AnsiToWin32` wrapper or the original stream.\n    \"\"\"\n    try:\n        from colorama.initialise import wrap_stream\n    except ImportError:\n        return f\n    else:\n        # Set `strip=False` to avoid needing to modify test_express_diff_with_color.\n        return wrap_stream(f, convert=None, strip=False, autoreset=False, wrap=True)",
      "importString": "import io\nimport os\nimport sys\nfrom collections.abc import Iterable, Iterator, Sequence\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom re import Pattern\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom mypy_extensions import mypyc_attr\nfrom packaging.specifiers import InvalidSpecifier, Specifier, SpecifierSet\nfrom packaging.version import InvalidVersion, Version\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPatternError\nfrom black.handle_ipynb_magics import jupyter_dependencies_are_installed\nfrom black.mode import TargetVersion\nfrom black.output import err\nfrom black.report import Report",
      "lineNum": 17,
      "relativeDocumentPath": "src/black/files.py"
    },
    {
      "symbolName": "jupyter_dependencies_are_installed",
      "sourceCode": "@lru_cache\ndef jupyter_dependencies_are_installed(*, warn: bool) -> bool:\n    installed = (\n        find_spec(\"tokenize_rt\") is not None and find_spec(\"IPython\") is not None\n    )\n    if not installed and warn:\n        msg = (\n            \"Skipping .ipynb files as Jupyter dependencies are not installed.\\n\"\n            'You can fix this by running ``pip install \"black[jupyter]\"``'\n        )\n        out(msg)\n    return installed",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "validate_cell",
      "sourceCode": "def validate_cell(src: str, mode: Mode) -> None:\n    \"\"\"Check that cell does not already contain TransformerManager transformations,\n    or non-Python cell magics, which might cause tokenizer_rt to break because of\n    indentations.\n\n    If a cell contains ``!ls``, then it'll be transformed to\n    ``get_ipython().system('ls')``. However, if the cell originally contained\n    ``get_ipython().system('ls')``, then it would get transformed in the same way:\n\n        >>> TransformerManager().transform_cell(\"get_ipython().system('ls')\")\n        \"get_ipython().system('ls')\\n\"\n        >>> TransformerManager().transform_cell(\"!ls\")\n        \"get_ipython().system('ls')\\n\"\n\n    Due to the impossibility of safely roundtripping in such situations, cells\n    containing transformed magics will be ignored.\n    \"\"\"\n    if any(transformed_magic in src for transformed_magic in TRANSFORMED_MAGICS):\n        raise NothingChanged\n\n    line = _get_code_start(src)\n    if line.startswith(\"%%\") and (\n        line.split(maxsplit=1)[0][2:]\n        not in PYTHON_CELL_MAGICS | mode.python_cell_magics\n    ):\n        raise NothingChanged",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 25,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "remove_trailing_semicolon",
      "sourceCode": "def remove_trailing_semicolon(src: str) -> tuple[str, bool]:\n    \"\"\"Remove trailing semicolon from Jupyter notebook cell.\n\n    For example,\n\n        fig, ax = plt.subplots()\n        ax.plot(x_data, y_data);  # plot data\n\n    would become\n\n        fig, ax = plt.subplots()\n        ax.plot(x_data, y_data)  # plot data\n\n    Mirrors the logic in `quiet` from `IPython.core.displayhook`, but uses\n    ``tokenize_rt`` so that round-tripping works fine.\n    \"\"\"\n    from tokenize_rt import reversed_enumerate, src_to_tokens, tokens_to_src\n\n    tokens = src_to_tokens(src)\n    trailing_semicolon = False\n    for idx, token in reversed_enumerate(tokens):\n        if token.name in TOKENS_TO_IGNORE:\n            continue\n        if token.name == \"OP\" and token.src == \";\":\n            del tokens[idx]\n            trailing_semicolon = True\n        break\n    if not trailing_semicolon:\n        return src, False\n    return tokens_to_src(tokens), True",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 29,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "put_trailing_semicolon_back",
      "sourceCode": "def put_trailing_semicolon_back(src: str, has_trailing_semicolon: bool) -> str:\n    \"\"\"Put trailing semicolon back if cell originally had it.\n\n    Mirrors the logic in `quiet` from `IPython.core.displayhook`, but uses\n    ``tokenize_rt`` so that round-tripping works fine.\n    \"\"\"\n    if not has_trailing_semicolon:\n        return src\n    from tokenize_rt import reversed_enumerate, src_to_tokens, tokens_to_src\n\n    tokens = src_to_tokens(src)\n    for idx, token in reversed_enumerate(tokens):\n        if token.name in TOKENS_TO_IGNORE:\n            continue\n        tokens[idx] = token._replace(src=token.src + \";\")\n        break\n    else:  # pragma: nocover\n        raise AssertionError(\n            \"INTERNAL ERROR: Was not able to reinstate trailing semicolon. \"\n            \"Please report a bug on https://github.com/psf/black/issues.  \"\n        ) from None\n    return str(tokens_to_src(tokens))",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 21,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "mask_cell",
      "sourceCode": "def mask_cell(src: str) -> tuple[str, list[Replacement]]:\n    \"\"\"Mask IPython magics so content becomes parseable Python code.\n\n    For example,\n\n        %matplotlib inline\n        'foo'\n\n    becomes\n\n        \"25716f358c32750e\"\n        'foo'\n\n    The replacements are returned, along with the transformed code.\n    \"\"\"\n    replacements: list[Replacement] = []\n    try:\n        ast.parse(src)\n    except SyntaxError:\n        # Might have IPython magics, will process below.\n        pass\n    else:\n        # Syntax is fine, nothing to mask, early return.\n        return src, replacements\n\n    from IPython.core.inputtransformer2 import TransformerManager\n\n    transformer_manager = TransformerManager()\n    # A side effect of the following transformation is that it also removes any\n    # empty lines at the beginning of the cell.\n    transformed = transformer_manager.transform_cell(src)\n    transformed, cell_magic_replacements = replace_cell_magics(transformed)\n    replacements += cell_magic_replacements\n    transformed = transformer_manager.transform_cell(transformed)\n    transformed, magic_replacements = replace_magics(transformed)\n    if len(transformed.strip().splitlines()) != len(src.strip().splitlines()):\n        # Multi-line magic, not supported.\n        raise NothingChanged\n    replacements += magic_replacements\n    return transformed, replacements",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 39,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "get_token",
      "sourceCode": "def get_token(src: str, magic: str) -> str:\n    \"\"\"Return randomly generated token to mask IPython magic with.\n\n    For example, if 'magic' was `%matplotlib inline`, then a possible\n    token to mask it with would be `\"43fdd17f7e5ddc83\"`. The token\n    will be the same length as the magic, and we make sure that it was\n    not already present anywhere else in the cell.\n    \"\"\"\n    assert magic\n    nbytes = max(len(magic) // 2 - 1, 1)\n    token = TOKEN_HEX(nbytes)\n    counter = 0\n    while token in src:\n        token = TOKEN_HEX(nbytes)\n        counter += 1\n        if counter > 100:\n            raise AssertionError(\n                \"INTERNAL ERROR: Black was not able to replace IPython magic. \"\n                \"Please report a bug on https://github.com/psf/black/issues.  \"\n                f\"The magic might be helpful: {magic}\"\n            ) from None\n    if len(token) + 2 < len(magic):\n        token = f\"{token}.\"\n    return f'\"{token}\"'",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 23,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "replace_cell_magics",
      "sourceCode": "def replace_cell_magics(src: str) -> tuple[str, list[Replacement]]:\n    \"\"\"Replace cell magic with token.\n\n    Note that 'src' will already have been processed by IPython's\n    TransformerManager().transform_cell.\n\n    Example,\n\n        get_ipython().run_cell_magic('t', '-n1', 'ls =!ls\\\\n')\n\n    becomes\n\n        \"a794.\"\n        ls =!ls\n\n    The replacement, along with the transformed code, is returned.\n    \"\"\"\n    replacements: list[Replacement] = []\n\n    tree = ast.parse(src)\n\n    cell_magic_finder = CellMagicFinder()\n    cell_magic_finder.visit(tree)\n    if cell_magic_finder.cell_magic is None:\n        return src, replacements\n    header = cell_magic_finder.cell_magic.header\n    mask = get_token(src, header)\n    replacements.append(Replacement(mask=mask, src=header))\n    return f\"{mask}\\n{cell_magic_finder.cell_magic.body}\", replacements",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 28,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "replace_magics",
      "sourceCode": "def replace_magics(src: str) -> tuple[str, list[Replacement]]:\n    \"\"\"Replace magics within body of cell.\n\n    Note that 'src' will already have been processed by IPython's\n    TransformerManager().transform_cell.\n\n    Example, this\n\n        get_ipython().run_line_magic('matplotlib', 'inline')\n        'foo'\n\n    becomes\n\n        \"5e67db56d490fd39\"\n        'foo'\n\n    The replacement, along with the transformed code, are returned.\n    \"\"\"\n    replacements = []\n    magic_finder = MagicFinder()\n    magic_finder.visit(ast.parse(src))\n    new_srcs = []\n    for i, line in enumerate(src.split(\"\\n\"), start=1):\n        if i in magic_finder.magics:\n            offsets_and_magics = magic_finder.magics[i]\n            if len(offsets_and_magics) != 1:  # pragma: nocover\n                raise AssertionError(\n                    f\"Expecting one magic per line, got: {offsets_and_magics}\\n\"\n                    \"Please report a bug on https://github.com/psf/black/issues.\"\n                )\n            col_offset, magic = (\n                offsets_and_magics[0].col_offset,\n                offsets_and_magics[0].magic,\n            )\n            mask = get_token(src, magic)\n            replacements.append(Replacement(mask=mask, src=magic))\n            line = line[:col_offset] + mask\n        new_srcs.append(line)\n    return \"\\n\".join(new_srcs), replacements",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 38,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "unmask_cell",
      "sourceCode": "def unmask_cell(src: str, replacements: list[Replacement]) -> str:\n    \"\"\"Remove replacements from cell.\n\n    For example\n\n        \"9b20\"\n        foo = bar\n\n    becomes\n\n        %%time\n        foo = bar\n    \"\"\"\n    for replacement in replacements:\n        src = src.replace(replacement.mask, replacement.src)\n    return src",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "_get_code_start",
      "sourceCode": "def _get_code_start(src: str) -> str:\n    \"\"\"Provides the first line where the code starts.\n\n    Iterates over lines of code until it finds the first line that doesn't\n    contain only empty spaces and comments. It removes any empty spaces at the\n    start of the line and returns it. If such line doesn't exist, it returns an\n    empty string.\n    \"\"\"\n    for match in re.finditer(\".+\", src):\n        line = match.group(0).lstrip()\n        if line and not line.startswith(\"#\"):\n            return line\n    return \"\"",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 12,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "_is_ipython_magic",
      "sourceCode": "def _is_ipython_magic(node: ast.expr) -> TypeGuard[ast.Attribute]:\n    \"\"\"Check if attribute is IPython magic.\n\n    Note that the source of the abstract syntax tree\n    will already have been processed by IPython's\n    TransformerManager().transform_cell.\n    \"\"\"\n    return (\n        isinstance(node, ast.Attribute)\n        and isinstance(node.value, ast.Call)\n        and isinstance(node.value.func, ast.Name)\n        and node.value.func.id == \"get_ipython\"\n    )",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 12,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "visit_Assign",
      "sourceCode": "def visit_Assign(self, node: ast.Assign) -> None:\n        \"\"\"Look for system assign magics.\n\n        For example,\n\n            black_version = !black --version\n            env = %env var\n\n        would have been (respectively) transformed to\n\n            black_version = get_ipython().getoutput('black --version')\n            env = get_ipython().run_line_magic('env', 'var')\n\n        and we look for instances of any of the latter.\n        \"\"\"\n        if isinstance(node.value, ast.Call) and _is_ipython_magic(node.value.func):\n            args = _get_str_args(node.value.args)\n            if node.value.func.attr == \"getoutput\":\n                src = f\"!{args[0]}\"\n            elif node.value.func.attr == \"run_line_magic\":\n                src = f\"%{args[0]}\"\n                if args[1]:\n                    src += f\" {args[1]}\"\n            else:\n                raise AssertionError(\n                    f\"Unexpected IPython magic {node.value.func.attr!r} found. \"\n                    \"Please report a bug on https://github.com/psf/black/issues.\"\n                ) from None\n            self.magics[node.value.lineno].append(\n                OffsetAndMagic(node.value.col_offset, src)\n            )\n        self.generic_visit(node)",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 31,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "visit_Expr",
      "sourceCode": "def visit_Expr(self, node: ast.Expr) -> None:\n        \"\"\"Look for magics in body of cell.\n\n        For examples,\n\n            !ls\n            !!ls\n            ?ls\n            ??ls\n\n        would (respectively) get transformed to\n\n            get_ipython().system('ls')\n            get_ipython().getoutput('ls')\n            get_ipython().run_line_magic('pinfo', 'ls')\n            get_ipython().run_line_magic('pinfo2', 'ls')\n\n        and we look for instances of any of the latter.\n        \"\"\"\n        if isinstance(node.value, ast.Call) and _is_ipython_magic(node.value.func):\n            args = _get_str_args(node.value.args)\n            if node.value.func.attr == \"run_line_magic\":\n                if args[0] == \"pinfo\":\n                    src = f\"?{args[1]}\"\n                elif args[0] == \"pinfo2\":\n                    src = f\"??{args[1]}\"\n                else:\n                    src = f\"%{args[0]}\"\n                    if args[1]:\n                        src += f\" {args[1]}\"\n            elif node.value.func.attr == \"system\":\n                src = f\"!{args[0]}\"\n            elif node.value.func.attr == \"getoutput\":\n                src = f\"!!{args[0]}\"\n            else:\n                raise NothingChanged  # unsupported magic.\n            self.magics[node.value.lineno].append(\n                OffsetAndMagic(node.value.col_offset, src)\n            )\n        self.generic_visit(node)",
      "importString": "import collections\nimport dataclasses\nimport re\nimport secrets\nimport sys\nfrom functools import lru_cache\nfrom importlib.util import find_spec\nfrom typing import Optional\nfrom black.mode import Mode\nfrom black.output import out\nfrom black.report import NothingChanged",
      "lineNum": 39,
      "relativeDocumentPath": "src/black/handle_ipynb_magics.py"
    },
    {
      "symbolName": "line",
      "sourceCode": "def line(self, indent: int = 0) -> Iterator[Line]:\n        \"\"\"Generate a line.\n\n        If the line is empty, only emit if it makes sense.\n        If the line is too long, split it first and then generate.\n\n        If any lines were generated, set up a new current_line.\n        \"\"\"\n        if not self.current_line:\n            self.current_line.depth += indent\n            return  # Line is empty, don't emit. Creating a new one unnecessary.\n\n        if len(self.current_line.leaves) == 1 and is_async_stmt_or_funcdef(\n            self.current_line.leaves[0]\n        ):\n            # Special case for async def/for/with statements. `visit_async_stmt`\n            # adds an `ASYNC` leaf then visits the child def/for/with statement\n            # nodes. Line yields from those nodes shouldn't treat the former\n            # `ASYNC` leaf as a complete line.\n            return\n\n        complete_line = self.current_line\n        self.current_line = Line(mode=self.mode, depth=complete_line.depth + indent)\n        yield complete_line",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 23,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_default",
      "sourceCode": "def visit_default(self, node: LN) -> Iterator[Line]:\n        \"\"\"Default `visit_*()` implementation. Recurses to children of `node`.\"\"\"\n        if isinstance(node, Leaf):\n            any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            for comment in generate_comments(node):\n                if any_open_brackets:\n                    # any comment within brackets is subject to splitting\n                    self.current_line.append(comment)\n                elif comment.type == token.COMMENT:\n                    # regular trailing comment\n                    self.current_line.append(comment)\n                    yield from self.line()\n\n                else:\n                    # regular standalone comment\n                    yield from self.line()\n\n                    self.current_line.append(comment)\n                    yield from self.line()\n\n            if any_open_brackets:\n                node.prefix = \"\"\n            if node.type not in WHITESPACE:\n                self.current_line.append(node)\n        yield from super().visit_default(node)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 24,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_test",
      "sourceCode": "def visit_test(self, node: Node) -> Iterator[Line]:\n        \"\"\"Visit an `x if y else z` test\"\"\"\n\n        already_parenthesized = (\n            node.prev_sibling and node.prev_sibling.type == token.LPAR\n        )\n\n        if not already_parenthesized:\n            # Similar to logic in wrap_in_parentheses\n            lpar = Leaf(token.LPAR, \"\")\n            rpar = Leaf(token.RPAR, \"\")\n            prefix = node.prefix\n            node.prefix = \"\"\n            lpar.prefix = prefix\n            node.insert_child(0, lpar)\n            node.append_child(rpar)\n\n        yield from self.visit_default(node)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 17,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_DEDENT",
      "sourceCode": "def visit_DEDENT(self, node: Leaf) -> Iterator[Line]:\n        \"\"\"Decrease indentation level, maybe yield a line.\"\"\"\n        # The current line might still wait for trailing comments.  At DEDENT time\n        # there won't be any (they would be prefixes on the preceding NEWLINE).\n        # Emit the line then.\n        yield from self.line()\n\n        # While DEDENT has no value, its prefix may contain standalone comments\n        # that belong to the current indentation level.  Get 'em.\n        yield from self.visit_default(node)\n\n        # Finally, emit the dedent.\n        yield from self.line(-1)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 12,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_stmt",
      "sourceCode": "def visit_stmt(\n        self, node: Node, keywords: set[str], parens: set[str]\n    ) -> Iterator[Line]:\n        \"\"\"Visit a statement.\n\n        This implementation is shared for `if`, `while`, `for`, `try`, `except`,\n        `def`, `with`, `class`, `assert`, and assignments.\n\n        The relevant Python language `keywords` for a given statement will be\n        NAME leaves within it. This methods puts those on a separate line.\n\n        `parens` holds a set of string leaf values immediately after which\n        invisible parens should be put.\n        \"\"\"\n        normalize_invisible_parens(\n            node, parens_after=parens, mode=self.mode, features=self.features\n        )\n        for child in node.children:\n            if is_name_token(child) and child.value in keywords:\n                yield from self.line()\n\n            yield from self.visit(child)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 21,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_dictsetmaker",
      "sourceCode": "def visit_dictsetmaker(self, node: Node) -> Iterator[Line]:\n        if Preview.wrap_long_dict_values_in_parens in self.mode:\n            for i, child in enumerate(node.children):\n                if i == 0:\n                    continue\n                if node.children[i - 1].type == token.COLON:\n                    if (\n                        child.type == syms.atom\n                        and child.children[0].type in OPENING_BRACKETS\n                        and not is_walrus_assignment(child)\n                    ):\n                        maybe_make_parens_invisible_in_atom(\n                            child,\n                            parent=node,\n                            remove_brackets_around_comma=False,\n                        )\n                    else:\n                        wrap_in_parentheses(node, child, visible=False)\n        yield from self.visit_default(node)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 18,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_funcdef",
      "sourceCode": "def visit_funcdef(self, node: Node) -> Iterator[Line]:\n        \"\"\"Visit function definition.\"\"\"\n        yield from self.line()\n\n        # Remove redundant brackets around return type annotation.\n        is_return_annotation = False\n        for child in node.children:\n            if child.type == token.RARROW:\n                is_return_annotation = True\n            elif is_return_annotation:\n                if child.type == syms.atom and child.children[0].type == token.LPAR:\n                    if maybe_make_parens_invisible_in_atom(\n                        child,\n                        parent=node,\n                        remove_brackets_around_comma=False,\n                    ):\n                        wrap_in_parentheses(node, child, visible=False)\n                else:\n                    wrap_in_parentheses(node, child, visible=False)\n                is_return_annotation = False\n\n        for child in node.children:\n            yield from self.visit(child)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 22,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_simple_stmt",
      "sourceCode": "def visit_simple_stmt(self, node: Node) -> Iterator[Line]:\n        \"\"\"Visit a statement without nested statements.\"\"\"\n        prev_type: Optional[int] = None\n        for child in node.children:\n            if (prev_type is None or prev_type == token.SEMI) and is_arith_like(child):\n                wrap_in_parentheses(node, child, visible=False)\n            prev_type = child.type\n\n        if node.parent and node.parent.type in STATEMENT:\n            if is_parent_function_or_class(node) and is_stub_body(node):\n                yield from self.visit_default(node)\n            else:\n                yield from self.line(+1)\n                yield from self.visit_default(node)\n                yield from self.line(-1)\n\n        else:\n            if node.parent and is_stub_suite(node.parent):\n                node.prefix = \"\"\n                yield from self.visit_default(node)\n                return\n            yield from self.line()\n            yield from self.visit_default(node)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 22,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_async_stmt",
      "sourceCode": "def visit_async_stmt(self, node: Node) -> Iterator[Line]:\n        \"\"\"Visit `async def`, `async for`, `async with`.\"\"\"\n        yield from self.line()\n\n        children = iter(node.children)\n        for child in children:\n            yield from self.visit(child)\n\n            if child.type == token.ASYNC or child.type == STANDALONE_COMMENT:\n                # STANDALONE_COMMENT happens when `# fmt: skip` is applied on the async\n                # line.\n                break\n\n        internal_stmt = next(children)\n        yield from self.visit(internal_stmt)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_power",
      "sourceCode": "def visit_power(self, node: Node) -> Iterator[Line]:\n        for idx, leaf in enumerate(node.children[:-1]):\n            next_leaf = node.children[idx + 1]\n\n            if not isinstance(leaf, Leaf):\n                continue\n\n            value = leaf.value.lower()\n            if (\n                leaf.type == token.NUMBER\n                and next_leaf.type == syms.trailer\n                # Ensure that we are in an attribute trailer\n                and next_leaf.children[0].type == token.DOT\n                # It shouldn't wrap hexadecimal, binary and octal literals\n                and not value.startswith((\"0x\", \"0b\", \"0o\"))\n                # It shouldn't wrap complex literals\n                and \"j\" not in value\n            ):\n                wrap_in_parentheses(node, leaf)\n\n        remove_await_parens(node)\n\n        yield from self.visit_default(node)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 22,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_factor",
      "sourceCode": "def visit_factor(self, node: Node) -> Iterator[Line]:\n        \"\"\"Force parentheses between a unary op and a binary power:\n\n        -2 ** 8 -> -(2 ** 8)\n        \"\"\"\n        _operator, operand = node.children\n        if (\n            operand.type == syms.power\n            and len(operand.children) == 3\n            and operand.children[1].type == token.DOUBLESTAR\n        ):\n            lpar = Leaf(token.LPAR, \"(\")\n            rpar = Leaf(token.RPAR, \")\")\n            index = operand.remove() or 0\n            node.insert_child(index, Node(syms.atom, [lpar, operand, rpar]))\n        yield from self.visit_default(node)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_tname",
      "sourceCode": "def visit_tname(self, node: Node) -> Iterator[Line]:\n        \"\"\"\n        Add potential parentheses around types in function parameter lists to be made\n        into real parentheses in case the type hint is too long to fit on a line\n        Examples:\n        def foo(a: int, b: float = 7): ...\n\n        ->\n\n        def foo(a: (int), b: (float) = 7): ...\n        \"\"\"\n        assert len(node.children) == 3\n        if maybe_make_parens_invisible_in_atom(node.children[2], parent=node):\n            wrap_in_parentheses(node, node.children[2], visible=False)\n\n        yield from self.visit_default(node)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_STRING",
      "sourceCode": "def visit_STRING(self, leaf: Leaf) -> Iterator[Line]:\n        if Preview.hex_codes_in_unicode_sequences in self.mode:\n            normalize_unicode_escape_sequences(leaf)\n\n        if is_docstring(leaf, self.mode) and not re.search(r\"\\\\\\s*\\n\", leaf.value):\n            # We're ignoring docstrings with backslash newline escapes because changing\n            # indentation of those changes the AST representation of the code.\n            if self.mode.string_normalization:\n                docstring = normalize_string_prefix(leaf.value)\n                # We handle string normalization at the end of this method, but since\n                # what we do right now acts differently depending on quote style (ex.\n                # see padding logic below), there's a possibility for unstable\n                # formatting. To avoid a situation where this function formats a\n                # docstring differently on the second pass, normalize it early.\n                docstring = normalize_string_quotes(docstring)\n            else:\n                docstring = leaf.value\n            prefix = get_string_prefix(docstring)\n            docstring = docstring[len(prefix) :]  # Remove the prefix\n            quote_char = docstring[0]\n            # A natural way to remove the outer quotes is to do:\n            #   docstring = docstring.strip(quote_char)\n            # but that breaks on \"\"\"\"\"x\"\"\" (which is '\"\"x').\n            # So we actually need to remove the first character and the next two\n            # characters but only if they are the same as the first.\n            quote_len = 1 if docstring[1] != quote_char else 3\n            docstring = docstring[quote_len:-quote_len]\n            docstring_started_empty = not docstring\n            indent = \" \" * 4 * self.current_line.depth\n\n            if is_multiline_string(leaf):\n                docstring = fix_multiline_docstring(docstring, indent)\n            else:\n                docstring = docstring.strip()\n\n            has_trailing_backslash = False\n            if docstring:\n                # Add some padding if the docstring starts / ends with a quote mark.\n                if docstring[0] == quote_char:\n                    docstring = \" \" + docstring\n                if docstring[-1] == quote_char:\n                    docstring += \" \"\n                if docstring[-1] == \"\\\\\":\n                    backslash_count = len(docstring) - len(docstring.rstrip(\"\\\\\"))\n                    if backslash_count % 2:\n                        # Odd number of tailing backslashes, add some padding to\n                        # avoid escaping the closing string quote.\n                        docstring += \" \"\n                        has_trailing_backslash = True\n            elif not docstring_started_empty:\n                docstring = \" \"\n\n            # We could enforce triple quotes at this point.\n            quote = quote_char * quote_len\n\n            # It's invalid to put closing single-character quotes on a new line.\n            if quote_len == 3:\n                # We need to find the length of the last line of the docstring\n                # to find if we can add the closing quotes to the line without\n                # exceeding the maximum line length.\n                # If docstring is one line, we don't put the closing quotes on a\n                # separate line because it looks ugly (#3320).\n                lines = docstring.splitlines()\n                last_line_length = len(lines[-1]) if docstring else 0\n\n                # If adding closing quotes would cause the last line to exceed\n                # the maximum line length, and the closing quote is not\n                # prefixed by a newline then put a line break before\n                # the closing quotes\n                if (\n                    len(lines) > 1\n                    and last_line_length + quote_len > self.mode.line_length\n                    and len(indent) + quote_len <= self.mode.line_length\n                    and not has_trailing_backslash\n                ):\n                    if (\n                        Preview.docstring_check_for_newline in self.mode\n                        and leaf.value[-1 - quote_len] == \"\\n\"\n                    ):\n                        leaf.value = prefix + quote + docstring + quote\n                    else:\n                        leaf.value = prefix + quote + docstring + \"\\n\" + indent + quote\n                else:\n                    leaf.value = prefix + quote + docstring + quote\n            else:\n                leaf.value = prefix + quote + docstring + quote\n\n        if self.mode.string_normalization and leaf.type == token.STRING:\n            leaf.value = normalize_string_prefix(leaf.value)\n            leaf.value = normalize_string_quotes(leaf.value)\n        yield from self.visit_default(leaf)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 90,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_atom",
      "sourceCode": "def visit_atom(self, node: Node) -> Iterator[Line]:\n        \"\"\"Visit any atom\"\"\"\n        if (\n            Preview.remove_lone_list_item_parens in self.mode\n            and len(node.children) == 3\n        ):\n            first = node.children[0]\n            last = node.children[-1]\n            if (first.type == token.LSQB and last.type == token.RSQB) or (\n                first.type == token.LBRACE and last.type == token.RBRACE\n            ):\n                # Lists or sets of one item\n                maybe_make_parens_invisible_in_atom(node.children[1], parent=node)\n\n        yield from self.visit_default(node)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "visit_fstring",
      "sourceCode": "def visit_fstring(self, node: Node) -> Iterator[Line]:\n        # currently we don't want to format and split f-strings at all.\n        string_leaf = fstring_to_string(node)\n        node.replace(string_leaf)\n        if \"\\\\\" in string_leaf.value and any(\n            \"\\\\\" in str(child)\n            for child in node.children\n            if child.type == syms.fstring_replacement_field\n        ):\n            # string normalization doesn't account for nested quotes,\n            # causing breakages. skip normalization when nested quotes exist\n            yield from self.visit_default(string_leaf)\n            return\n        yield from self.visit_STRING(string_leaf)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 13,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "__post_init__",
      "sourceCode": "def __post_init__(self) -> None:\n        \"\"\"You are in a twisty little maze of passages.\"\"\"\n        self.current_line = Line(mode=self.mode)\n\n        v = self.visit_stmt\n        Ã˜: set[str] = set()\n        self.visit_assert_stmt = partial(v, keywords={\"assert\"}, parens={\"assert\", \",\"})\n        self.visit_if_stmt = partial(\n            v, keywords={\"if\", \"else\", \"elif\"}, parens={\"if\", \"elif\"}\n        )\n        self.visit_while_stmt = partial(v, keywords={\"while\", \"else\"}, parens={\"while\"})\n        self.visit_for_stmt = partial(v, keywords={\"for\", \"else\"}, parens={\"for\", \"in\"})\n        self.visit_try_stmt = partial(\n            v, keywords={\"try\", \"except\", \"else\", \"finally\"}, parens=Ã˜\n        )\n        self.visit_except_clause = partial(v, keywords={\"except\"}, parens={\"except\"})\n        self.visit_with_stmt = partial(v, keywords={\"with\"}, parens={\"with\"})\n        self.visit_classdef = partial(v, keywords={\"class\"}, parens=Ã˜)\n\n        self.visit_expr_stmt = partial(v, keywords=Ã˜, parens=ASSIGNMENTS)\n        self.visit_return_stmt = partial(v, keywords={\"return\"}, parens={\"return\"})\n        self.visit_import_from = partial(v, keywords=Ã˜, parens={\"import\"})\n        self.visit_del_stmt = partial(v, keywords=Ã˜, parens={\"del\"})\n        self.visit_async_funcdef = self.visit_async_stmt\n        self.visit_decorated = self.visit_decorators\n\n        # PEP 634\n        self.visit_match_stmt = self.visit_match_case\n        self.visit_case_block = self.visit_match_case\n        if Preview.remove_redundant_guard_parens in self.mode:\n            self.visit_guard = partial(v, keywords=Ã˜, parens={\"if\"})",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 30,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "transform_line",
      "sourceCode": "def transform_line(\n    line: Line, mode: Mode, features: Collection[Feature] = ()\n) -> Iterator[Line]:\n    \"\"\"Transform a `line`, potentially splitting it into many lines.\n\n    They should fit in the allotted `line_length` but might not be able to.\n\n    `features` are syntactical features that may be used in the output.\n    \"\"\"\n    if line.is_comment:\n        yield line\n        return\n\n    line_str = line_to_string(line)\n\n    # We need the line string when power operators are hugging to determine if we should\n    # split the line. Default to line_str, if no power operator are present on the line.\n    line_str_hugging_power_ops = (\n        _hugging_power_ops_line_to_string(line, features, mode) or line_str\n    )\n\n    ll = mode.line_length\n    sn = mode.string_normalization\n    string_merge = StringMerger(ll, sn)\n    string_paren_strip = StringParenStripper(ll, sn)\n    string_split = StringSplitter(ll, sn)\n    string_paren_wrap = StringParenWrapper(ll, sn)\n\n    transformers: list[Transformer]\n    if (\n        not line.contains_uncollapsable_type_comments()\n        and not line.should_split_rhs\n        and not line.magic_trailing_comma\n        and (\n            is_line_short_enough(line, mode=mode, line_str=line_str_hugging_power_ops)\n            or line.contains_unsplittable_type_ignore()\n        )\n        and not (line.inside_brackets and line.contains_standalone_comments())\n        and not line.contains_implicit_multiline_string_with_comments()\n    ):\n        # Only apply basic string preprocessing, since lines shouldn't be split here.\n        if Preview.string_processing in mode:\n            transformers = [string_merge, string_paren_strip]\n        else:\n            transformers = []\n    elif line.is_def and not should_split_funcdef_with_rhs(line, mode):\n        transformers = [left_hand_split]\n    else:\n\n        def _rhs(\n            self: object, line: Line, features: Collection[Feature], mode: Mode\n        ) -> Iterator[Line]:\n            \"\"\"Wraps calls to `right_hand_split`.\n\n            The calls increasingly `omit` right-hand trailers (bracket pairs with\n            content), meaning the trailers get glued together to split on another\n            bracket pair instead.\n            \"\"\"\n            for omit in generate_trailers_to_omit(line, mode.line_length):\n                lines = list(right_hand_split(line, mode, features, omit=omit))\n                # Note: this check is only able to figure out if the first line of the\n                # *current* transformation fits in the line length.  This is true only\n                # for simple cases.  All others require running more transforms via\n                # `transform_line()`.  This check doesn't know if those would succeed.\n                if is_line_short_enough(lines[0], mode=mode):\n                    yield from lines\n                    return\n\n            # All splits failed, best effort split with no omits.\n            # This mostly happens to multiline strings that are by definition\n            # reported as not fitting a single line, as well as lines that contain\n            # trailing commas (those have to be exploded).\n            yield from right_hand_split(line, mode, features=features)\n\n        # HACK: nested functions (like _rhs) compiled by mypyc don't retain their\n        # __name__ attribute which is needed in `run_transformer` further down.\n        # Unfortunately a nested class breaks mypyc too. So a class must be created\n        # via type ... https://github.com/mypyc/mypyc/issues/884\n        rhs = type(\"rhs\", (), {\"__call__\": _rhs})()\n\n        if Preview.string_processing in mode:\n            if line.inside_brackets:\n                transformers = [\n                    string_merge,\n                    string_paren_strip,\n                    string_split,\n                    delimiter_split,\n                    standalone_comment_split,\n                    string_paren_wrap,\n                    rhs,\n                ]\n            else:\n                transformers = [\n                    string_merge,\n                    string_paren_strip,\n                    string_split,\n                    string_paren_wrap,\n                    rhs,\n                ]\n        else:\n            if line.inside_brackets:\n                transformers = [delimiter_split, standalone_comment_split, rhs]\n            else:\n                transformers = [rhs]\n    # It's always safe to attempt hugging of power operations and pretty much every line\n    # could match.\n    transformers.append(hug_power_op)\n\n    for transform in transformers:\n        # We are accumulating lines in `result` because we might want to abort\n        # mission and return the original line in the end, or attempt a different\n        # split altogether.\n        try:\n            result = run_transformer(line, transform, mode, features, line_str=line_str)\n        except CannotTransform:\n            continue\n        else:\n            yield from result\n            break\n\n    else:\n        yield line",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 121,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "_rhs",
      "sourceCode": "def _rhs(\n            self: object, line: Line, features: Collection[Feature], mode: Mode\n        ) -> Iterator[Line]:\n            \"\"\"Wraps calls to `right_hand_split`.\n\n            The calls increasingly `omit` right-hand trailers (bracket pairs with\n            content), meaning the trailers get glued together to split on another\n            bracket pair instead.\n            \"\"\"\n            for omit in generate_trailers_to_omit(line, mode.line_length):\n                lines = list(right_hand_split(line, mode, features, omit=omit))\n                # Note: this check is only able to figure out if the first line of the\n                # *current* transformation fits in the line length.  This is true only\n                # for simple cases.  All others require running more transforms via\n                # `transform_line()`.  This check doesn't know if those would succeed.\n                if is_line_short_enough(lines[0], mode=mode):\n                    yield from lines\n                    return\n\n            # All splits failed, best effort split with no omits.\n            # This mostly happens to multiline strings that are by definition\n            # reported as not fitting a single line, as well as lines that contain\n            # trailing commas (those have to be exploded).\n            yield from right_hand_split(line, mode, features=features)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 23,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "should_split_funcdef_with_rhs",
      "sourceCode": "def should_split_funcdef_with_rhs(line: Line, mode: Mode) -> bool:\n    \"\"\"If a funcdef has a magic trailing comma in the return type, then we should first\n    split the line with rhs to respect the comma.\n    \"\"\"\n    return_type_leaves: list[Leaf] = []\n    in_return_type = False\n\n    for leaf in line.leaves:\n        if leaf.type == token.COLON:\n            in_return_type = False\n        if in_return_type:\n            return_type_leaves.append(leaf)\n        if leaf.type == token.RARROW:\n            in_return_type = True\n\n    # using `bracket_split_build_line` will mess with whitespace, so we duplicate a\n    # couple lines from it.\n    result = Line(mode=line.mode, depth=line.depth)\n    leaves_to_track = get_leaves_inside_matching_brackets(return_type_leaves)\n    for leaf in return_type_leaves:\n        result.append(\n            leaf,\n            preformatted=True,\n            track_bracket=id(leaf) in leaves_to_track,\n        )\n\n    # we could also return true if the line is too long, and the return type is longer\n    # than the param list. Or if `should_split_rhs` returns True.\n    return result.magic_trailing_comma is not None",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 28,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "left_hand_split",
      "sourceCode": "def left_hand_split(\n    line: Line, _features: Collection[Feature], mode: Mode\n) -> Iterator[Line]:\n    \"\"\"Split line into many lines, starting with the first matching bracket pair.\n\n    Note: this usually looks weird, only use this for function definitions.\n    Prefer RHS otherwise.  This is why this function is not symmetrical with\n    :func:`right_hand_split` which also handles optional parentheses.\n    \"\"\"\n    tail_leaves: list[Leaf] = []\n    body_leaves: list[Leaf] = []\n    head_leaves: list[Leaf] = []\n    current_leaves = head_leaves\n    matching_bracket: Optional[Leaf] = None\n    for leaf in line.leaves:\n        if (\n            current_leaves is body_leaves\n            and leaf.type in CLOSING_BRACKETS\n            and leaf.opening_bracket is matching_bracket\n            and isinstance(matching_bracket, Leaf)\n        ):\n            ensure_visible(leaf)\n            ensure_visible(matching_bracket)\n            current_leaves = tail_leaves if body_leaves else head_leaves\n        current_leaves.append(leaf)\n        if current_leaves is head_leaves:\n            if leaf.type in OPENING_BRACKETS:\n                matching_bracket = leaf\n                current_leaves = body_leaves\n    if not matching_bracket or not tail_leaves:\n        raise CannotSplit(\"No brackets found\")\n\n    head = bracket_split_build_line(\n        head_leaves, line, matching_bracket, component=_BracketSplitComponent.head\n    )\n    body = bracket_split_build_line(\n        body_leaves, line, matching_bracket, component=_BracketSplitComponent.body\n    )\n    tail = bracket_split_build_line(\n        tail_leaves, line, matching_bracket, component=_BracketSplitComponent.tail\n    )\n    bracket_split_succeeded_or_raise(head, body, tail)\n    for result in (head, body, tail):\n        if result:\n            yield result",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 44,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "right_hand_split",
      "sourceCode": "def right_hand_split(\n    line: Line,\n    mode: Mode,\n    features: Collection[Feature] = (),\n    omit: Collection[LeafID] = (),\n) -> Iterator[Line]:\n    \"\"\"Split line into many lines, starting with the last matching bracket pair.\n\n    If the split was by optional parentheses, attempt splitting without them, too.\n    `omit` is a collection of closing bracket IDs that shouldn't be considered for\n    this split.\n\n    Note: running this function modifies `bracket_depth` on the leaves of `line`.\n    \"\"\"\n    rhs_result = _first_right_hand_split(line, omit=omit)\n    yield from _maybe_split_omitting_optional_parens(\n        rhs_result, line, mode, features=features, omit=omit\n    )",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 17,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "_first_right_hand_split",
      "sourceCode": "def _first_right_hand_split(\n    line: Line,\n    omit: Collection[LeafID] = (),\n) -> RHSResult:\n    \"\"\"Split the line into head, body, tail starting with the last bracket pair.\n\n    Note: this function should not have side effects. It's relied upon by\n    _maybe_split_omitting_optional_parens to get an opinion whether to prefer\n    splitting on the right side of an assignment statement.\n    \"\"\"\n    tail_leaves: list[Leaf] = []\n    body_leaves: list[Leaf] = []\n    head_leaves: list[Leaf] = []\n    current_leaves = tail_leaves\n    opening_bracket: Optional[Leaf] = None\n    closing_bracket: Optional[Leaf] = None\n    for leaf in reversed(line.leaves):\n        if current_leaves is body_leaves:\n            if leaf is opening_bracket:\n                current_leaves = head_leaves if body_leaves else tail_leaves\n        current_leaves.append(leaf)\n        if current_leaves is tail_leaves:\n            if leaf.type in CLOSING_BRACKETS and id(leaf) not in omit:\n                opening_bracket = leaf.opening_bracket\n                closing_bracket = leaf\n                current_leaves = body_leaves\n    if not (opening_bracket and closing_bracket and head_leaves):\n        # If there is no opening or closing_bracket that means the split failed and\n        # all content is in the tail.  Otherwise, if `head_leaves` are empty, it means\n        # the matching `opening_bracket` wasn't available on `line` anymore.\n        raise CannotSplit(\"No brackets found\")\n\n    tail_leaves.reverse()\n    body_leaves.reverse()\n    head_leaves.reverse()\n\n    body: Optional[Line] = None\n    if (\n        Preview.hug_parens_with_braces_and_square_brackets in line.mode\n        and tail_leaves[0].value\n        and tail_leaves[0].opening_bracket is head_leaves[-1]\n    ):\n        inner_body_leaves = list(body_leaves)\n        hugged_opening_leaves: list[Leaf] = []\n        hugged_closing_leaves: list[Leaf] = []\n        is_unpacking = body_leaves[0].type in [token.STAR, token.DOUBLESTAR]\n        unpacking_offset: int = 1 if is_unpacking else 0\n        while (\n            len(inner_body_leaves) >= 2 + unpacking_offset\n            and inner_body_leaves[-1].type in CLOSING_BRACKETS\n            and inner_body_leaves[-1].opening_bracket\n            is inner_body_leaves[unpacking_offset]\n        ):\n            if unpacking_offset:\n                hugged_opening_leaves.append(inner_body_leaves.pop(0))\n                unpacking_offset = 0\n            hugged_opening_leaves.append(inner_body_leaves.pop(0))\n            hugged_closing_leaves.insert(0, inner_body_leaves.pop())\n\n        if hugged_opening_leaves and inner_body_leaves:\n            inner_body = bracket_split_build_line(\n                inner_body_leaves,\n                line,\n                hugged_opening_leaves[-1],\n                component=_BracketSplitComponent.body,\n            )\n            if (\n                line.mode.magic_trailing_comma\n                and inner_body_leaves[-1].type == token.COMMA\n            ):\n                should_hug = True\n            else:\n                line_length = line.mode.line_length - sum(\n                    len(str(leaf))\n                    for leaf in hugged_opening_leaves + hugged_closing_leaves\n                )\n                if is_line_short_enough(\n                    inner_body, mode=replace(line.mode, line_length=line_length)\n                ):\n                    # Do not hug if it fits on a single line.\n                    should_hug = False\n                else:\n                    should_hug = True\n            if should_hug:\n                body_leaves = inner_body_leaves\n                head_leaves.extend(hugged_opening_leaves)\n                tail_leaves = hugged_closing_leaves + tail_leaves\n                body = inner_body  # No need to re-calculate the body again later.\n\n    head = bracket_split_build_line(\n        head_leaves, line, opening_bracket, component=_BracketSplitComponent.head\n    )\n    if body is None:\n        body = bracket_split_build_line(\n            body_leaves, line, opening_bracket, component=_BracketSplitComponent.body\n        )\n    tail = bracket_split_build_line(\n        tail_leaves, line, opening_bracket, component=_BracketSplitComponent.tail\n    )\n    bracket_split_succeeded_or_raise(head, body, tail)\n    return RHSResult(head, body, tail, opening_bracket, closing_bracket)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 100,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "_maybe_split_omitting_optional_parens",
      "sourceCode": "def _maybe_split_omitting_optional_parens(\n    rhs: RHSResult,\n    line: Line,\n    mode: Mode,\n    features: Collection[Feature] = (),\n    omit: Collection[LeafID] = (),\n) -> Iterator[Line]:\n    if (\n        Feature.FORCE_OPTIONAL_PARENTHESES not in features\n        # the opening bracket is an optional paren\n        and rhs.opening_bracket.type == token.LPAR\n        and not rhs.opening_bracket.value\n        # the closing bracket is an optional paren\n        and rhs.closing_bracket.type == token.RPAR\n        and not rhs.closing_bracket.value\n        # it's not an import (optional parens are the only thing we can split on\n        # in this case; attempting a split without them is a waste of time)\n        and not line.is_import\n        # and we can actually remove the parens\n        and can_omit_invisible_parens(rhs, mode.line_length)\n    ):\n        omit = {id(rhs.closing_bracket), *omit}\n        try:\n            # The RHSResult Omitting Optional Parens.\n            rhs_oop = _first_right_hand_split(line, omit=omit)\n            is_split_right_after_equal = (\n                len(rhs.head.leaves) >= 2 and rhs.head.leaves[-2].type == token.EQUAL\n            )\n            rhs_head_contains_brackets = any(\n                leaf.type in BRACKETS for leaf in rhs.head.leaves[:-1]\n            )\n            # the -1 is for the ending optional paren\n            rhs_head_short_enough = is_line_short_enough(\n                rhs.head, mode=replace(mode, line_length=mode.line_length - 1)\n            )\n            rhs_head_explode_blocked_by_magic_trailing_comma = (\n                rhs.head.magic_trailing_comma is None\n            )\n            if (\n                not (\n                    is_split_right_after_equal\n                    and rhs_head_contains_brackets\n                    and rhs_head_short_enough\n                    and rhs_head_explode_blocked_by_magic_trailing_comma\n                )\n                # the omit optional parens split is preferred by some other reason\n                or _prefer_split_rhs_oop_over_rhs(rhs_oop, rhs, mode)\n            ):\n                yield from _maybe_split_omitting_optional_parens(\n                    rhs_oop, line, mode, features=features, omit=omit\n                )\n                return\n\n        except CannotSplit as e:\n            # For chained assignments we want to use the previous successful split\n            if line.is_chained_assignment:\n                pass\n\n            elif not can_be_split(rhs.body) and not is_line_short_enough(\n                rhs.body, mode=mode\n            ):\n                raise CannotSplit(\n                    \"Splitting failed, body is still too long and can't be split.\"\n                ) from e\n\n            elif (\n                rhs.head.contains_multiline_strings()\n                or rhs.tail.contains_multiline_strings()\n            ):\n                raise CannotSplit(\n                    \"The current optional pair of parentheses is bound to fail to\"\n                    \" satisfy the splitting algorithm because the head or the tail\"\n                    \" contains multiline strings which by definition never fit one\"\n                    \" line.\"\n                ) from e\n\n    ensure_visible(rhs.opening_bracket)\n    ensure_visible(rhs.closing_bracket)\n    for result in (rhs.head, rhs.body, rhs.tail):\n        if result:\n            yield result",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 80,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "_prefer_split_rhs_oop_over_rhs",
      "sourceCode": "def _prefer_split_rhs_oop_over_rhs(\n    rhs_oop: RHSResult, rhs: RHSResult, mode: Mode\n) -> bool:\n    \"\"\"\n    Returns whether we should prefer the result from a split omitting optional parens\n    (rhs_oop) over the original (rhs).\n    \"\"\"\n    # If we have multiple targets, we prefer more `=`s on the head vs pushing them to\n    # the body\n    rhs_head_equal_count = [leaf.type for leaf in rhs.head.leaves].count(token.EQUAL)\n    rhs_oop_head_equal_count = [leaf.type for leaf in rhs_oop.head.leaves].count(\n        token.EQUAL\n    )\n    if rhs_head_equal_count > 1 and rhs_head_equal_count > rhs_oop_head_equal_count:\n        return False\n\n    has_closing_bracket_after_assign = False\n    for leaf in reversed(rhs_oop.head.leaves):\n        if leaf.type == token.EQUAL:\n            break\n        if leaf.type in CLOSING_BRACKETS:\n            has_closing_bracket_after_assign = True\n            break\n    return (\n        # contains matching brackets after the `=` (done by checking there is a\n        # closing bracket)\n        has_closing_bracket_after_assign\n        or (\n            # the split is actually from inside the optional parens (done by checking\n            # the first line still contains the `=`)\n            any(leaf.type == token.EQUAL for leaf in rhs_oop.head.leaves)\n            # the first line is short enough\n            and is_line_short_enough(rhs_oop.head, mode=mode)\n        )\n        # contains unsplittable type ignore\n        or rhs_oop.head.contains_unsplittable_type_ignore()\n        or rhs_oop.body.contains_unsplittable_type_ignore()\n        or rhs_oop.tail.contains_unsplittable_type_ignore()\n    )",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 38,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "bracket_split_succeeded_or_raise",
      "sourceCode": "def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None:\n    \"\"\"Raise :exc:`CannotSplit` if the last left- or right-hand split failed.\n\n    Do nothing otherwise.\n\n    A left- or right-hand split is based on a pair of brackets. Content before\n    (and including) the opening bracket is left on one line, content inside the\n    brackets is put on a separate line, and finally content starting with and\n    following the closing bracket is put on a separate line.\n\n    Those are called `head`, `body`, and `tail`, respectively. If the split\n    produced the same line (all content in `head`) or ended up with an empty `body`\n    and the `tail` is just the closing bracket, then it's considered failed.\n    \"\"\"\n    tail_len = len(str(tail).strip())\n    if not body:\n        if tail_len == 0:\n            raise CannotSplit(\"Splitting brackets produced the same line\")\n\n        elif tail_len < 3:\n            raise CannotSplit(\n                f\"Splitting brackets on an empty body to save {tail_len} characters is\"\n                \" not worth it\"\n            )",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 23,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "_ensure_trailing_comma",
      "sourceCode": "def _ensure_trailing_comma(\n    leaves: list[Leaf], original: Line, opening_bracket: Leaf\n) -> bool:\n    if not leaves:\n        return False\n    # Ensure a trailing comma for imports\n    if original.is_import:\n        return True\n    # ...and standalone function arguments\n    if not original.is_def:\n        return False\n    if opening_bracket.value != \"(\":\n        return False\n    # Don't add commas if we already have any commas\n    if any(\n        leaf.type == token.COMMA\n        and (\n            Preview.typed_params_trailing_comma not in original.mode\n            or not is_part_of_annotation(leaf)\n        )\n        for leaf in leaves\n    ):\n        return False\n\n    # Find a leaf with a parent (comments don't have parents)\n    leaf_with_parent = next((leaf for leaf in leaves if leaf.parent), None)\n    if leaf_with_parent is None:\n        return True\n    # Don't add commas inside parenthesized return annotations\n    if get_annotation_type(leaf_with_parent) == \"return\":\n        return False\n    # Don't add commas inside PEP 604 unions\n    if (\n        leaf_with_parent.parent\n        and leaf_with_parent.parent.next_sibling\n        and leaf_with_parent.parent.next_sibling.type == token.VBAR\n    ):\n        return False\n    return True",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 38,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "bracket_split_build_line",
      "sourceCode": "def bracket_split_build_line(\n    leaves: list[Leaf],\n    original: Line,\n    opening_bracket: Leaf,\n    *,\n    component: _BracketSplitComponent,\n) -> Line:\n    \"\"\"Return a new line with given `leaves` and respective comments from `original`.\n\n    If it's the head component, brackets will be tracked so trailing commas are\n    respected.\n\n    If it's the body component, the result line is one-indented inside brackets and as\n    such has its first leaf's prefix normalized and a trailing comma added when\n    expected.\n    \"\"\"\n    result = Line(mode=original.mode, depth=original.depth)\n    if component is _BracketSplitComponent.body:\n        result.inside_brackets = True\n        result.depth += 1\n        if _ensure_trailing_comma(leaves, original, opening_bracket):\n            for i in range(len(leaves) - 1, -1, -1):\n                if leaves[i].type == STANDALONE_COMMENT:\n                    continue\n\n                if leaves[i].type != token.COMMA:\n                    new_comma = Leaf(token.COMMA, \",\")\n                    leaves.insert(i + 1, new_comma)\n                break\n\n    leaves_to_track: set[LeafID] = set()\n    if component is _BracketSplitComponent.head:\n        leaves_to_track = get_leaves_inside_matching_brackets(leaves)\n    # Populate the line\n    for leaf in leaves:\n        result.append(\n            leaf,\n            preformatted=True,\n            track_bracket=id(leaf) in leaves_to_track,\n        )\n        for comment_after in original.comments_after(leaf):\n            result.append(comment_after, preformatted=True)\n    if component is _BracketSplitComponent.body and should_split_line(\n        result, opening_bracket\n    ):\n        result.should_split_rhs = True\n    return result",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 46,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "dont_increase_indentation",
      "sourceCode": "def dont_increase_indentation(split_func: Transformer) -> Transformer:\n    \"\"\"Normalize prefix of the first leaf in every line returned by `split_func`.\n\n    This is a decorator over relevant split functions.\n    \"\"\"\n\n    @wraps(split_func)\n    def split_wrapper(\n        line: Line, features: Collection[Feature], mode: Mode\n    ) -> Iterator[Line]:\n        for split_line in split_func(line, features, mode):\n            split_line.leaves[0].prefix = \"\"\n            yield split_line\n\n    return split_wrapper",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "delimiter_split",
      "sourceCode": "@dont_increase_indentation\ndef delimiter_split(\n    line: Line, features: Collection[Feature], mode: Mode\n) -> Iterator[Line]:\n    \"\"\"Split according to delimiters of the highest priority.\n\n    If the appropriate Features are given, the split will add trailing commas\n    also in function signatures and calls that contain `*` and `**`.\n    \"\"\"\n    if len(line.leaves) == 0:\n        raise CannotSplit(\"Line empty\") from None\n    last_leaf = line.leaves[-1]\n\n    bt = line.bracket_tracker\n    try:\n        delimiter_priority = bt.max_delimiter_priority(exclude={id(last_leaf)})\n    except ValueError:\n        raise CannotSplit(\"No delimiters found\") from None\n\n    if (\n        delimiter_priority == DOT_PRIORITY\n        and bt.delimiter_count_with_priority(delimiter_priority) == 1\n    ):\n        raise CannotSplit(\"Splitting a single attribute from its owner looks wrong\")\n\n    current_line = Line(\n        mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets\n    )\n    lowest_depth = sys.maxsize\n    trailing_comma_safe = True\n\n    def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        \"\"\"Append `leaf` to current line or to new line if appending impossible.\"\"\"\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError:\n            yield current_line\n\n            current_line = Line(\n                mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets\n            )\n            current_line.append(leaf)\n\n    def append_comments(leaf: Leaf) -> Iterator[Line]:\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n\n    last_non_comment_leaf = _get_last_non_comment_leaf(line)\n    for leaf_idx, leaf in enumerate(line.leaves):\n        yield from append_to_line(leaf)\n\n        previous_priority = leaf_idx > 0 and bt.delimiters.get(\n            id(line.leaves[leaf_idx - 1])\n        )\n        if (\n            previous_priority != delimiter_priority\n            or delimiter_priority in MIGRATE_COMMENT_DELIMITERS\n        ):\n            yield from append_comments(leaf)\n\n        lowest_depth = min(lowest_depth, leaf.bracket_depth)\n        if trailing_comma_safe and leaf.bracket_depth == lowest_depth:\n            trailing_comma_safe = _can_add_trailing_comma(leaf, features)\n\n        if last_leaf.type == STANDALONE_COMMENT and leaf_idx == last_non_comment_leaf:\n            current_line = _safe_add_trailing_comma(\n                trailing_comma_safe, delimiter_priority, current_line\n            )\n\n        leaf_priority = bt.delimiters.get(id(leaf))\n        if leaf_priority == delimiter_priority:\n            if (\n                leaf_idx + 1 < len(line.leaves)\n                and delimiter_priority not in MIGRATE_COMMENT_DELIMITERS\n            ):\n                yield from append_comments(line.leaves[leaf_idx + 1])\n\n            yield current_line\n            current_line = Line(\n                mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets\n            )\n\n    if current_line:\n        current_line = _safe_add_trailing_comma(\n            trailing_comma_safe, delimiter_priority, current_line\n        )\n        yield current_line",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 87,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "append_to_line",
      "sourceCode": "def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        \"\"\"Append `leaf` to current line or to new line if appending impossible.\"\"\"\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError:\n            yield current_line\n\n            current_line = Line(\n                mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets\n            )\n            current_line.append(leaf)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "standalone_comment_split",
      "sourceCode": "@dont_increase_indentation\ndef standalone_comment_split(\n    line: Line, features: Collection[Feature], mode: Mode\n) -> Iterator[Line]:\n    \"\"\"Split standalone comments from the rest of the line.\"\"\"\n    if not line.contains_standalone_comments():\n        raise CannotSplit(\"Line does not have any standalone comments\")\n\n    current_line = Line(\n        mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets\n    )\n\n    def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        \"\"\"Append `leaf` to current line or to new line if appending impossible.\"\"\"\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError:\n            yield current_line\n\n            current_line = Line(\n                line.mode, depth=line.depth, inside_brackets=line.inside_brackets\n            )\n            current_line.append(leaf)\n\n    for leaf in line.leaves:\n        yield from append_to_line(leaf)\n\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n\n    if current_line:\n        yield current_line",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 32,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "append_to_line",
      "sourceCode": "def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        \"\"\"Append `leaf` to current line or to new line if appending impossible.\"\"\"\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError:\n            yield current_line\n\n            current_line = Line(\n                line.mode, depth=line.depth, inside_brackets=line.inside_brackets\n            )\n            current_line.append(leaf)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "normalize_invisible_parens",
      "sourceCode": "def normalize_invisible_parens(  # noqa: C901\n    node: Node, parens_after: set[str], *, mode: Mode, features: Collection[Feature]\n) -> None:\n    \"\"\"Make existing optional parentheses invisible or create new ones.\n\n    `parens_after` is a set of string leaf values immediately after which parens\n    should be put.\n\n    Standardizes on visible parentheses for single-element tuples, and keeps\n    existing visible parentheses for other tuples and generator expressions.\n    \"\"\"\n    for pc in list_comments(node.prefix, is_endmarker=False):\n        if pc.value in FMT_OFF:\n            # This `node` has a prefix with `# fmt: off`, don't mess with parens.\n            return\n\n    # The multiple context managers grammar has a different pattern, thus this is\n    # separate from the for-loop below. This possibly wraps them in invisible parens,\n    # and later will be removed in remove_with_parens when needed.\n    if node.type == syms.with_stmt:\n        _maybe_wrap_cms_in_parens(node, mode, features)\n\n    check_lpar = False\n    for index, child in enumerate(list(node.children)):\n        # Fixes a bug where invisible parens are not properly stripped from\n        # assignment statements that contain type annotations.\n        if isinstance(child, Node) and child.type == syms.annassign:\n            normalize_invisible_parens(\n                child, parens_after=parens_after, mode=mode, features=features\n            )\n\n        # Fixes a bug where invisible parens are not properly wrapped around\n        # case blocks.\n        if isinstance(child, Node) and child.type == syms.case_block:\n            normalize_invisible_parens(\n                child, parens_after={\"case\"}, mode=mode, features=features\n            )\n\n        # Add parentheses around if guards in case blocks\n        if (\n            isinstance(child, Node)\n            and child.type == syms.guard\n            and Preview.parens_for_long_if_clauses_in_case_block in mode\n        ):\n            normalize_invisible_parens(\n                child, parens_after={\"if\"}, mode=mode, features=features\n            )\n\n        # Add parentheses around long tuple unpacking in assignments.\n        if (\n            index == 0\n            and isinstance(child, Node)\n            and child.type == syms.testlist_star_expr\n        ):\n            check_lpar = True\n\n        if check_lpar:\n            if (\n                child.type == syms.atom\n                and node.type == syms.for_stmt\n                and isinstance(child.prev_sibling, Leaf)\n                and child.prev_sibling.type == token.NAME\n                and child.prev_sibling.value == \"for\"\n            ):\n                if maybe_make_parens_invisible_in_atom(\n                    child,\n                    parent=node,\n                    remove_brackets_around_comma=True,\n                ):\n                    wrap_in_parentheses(node, child, visible=False)\n            elif isinstance(child, Node) and node.type == syms.with_stmt:\n                remove_with_parens(child, node)\n            elif child.type == syms.atom:\n                if maybe_make_parens_invisible_in_atom(\n                    child,\n                    parent=node,\n                ):\n                    wrap_in_parentheses(node, child, visible=False)\n            elif is_one_tuple(child):\n                wrap_in_parentheses(node, child, visible=True)\n            elif node.type == syms.import_from:\n                _normalize_import_from(node, child, index)\n                break\n            elif (\n                index == 1\n                and child.type == token.STAR\n                and node.type == syms.except_clause\n            ):\n                # In except* (PEP 654), the star is actually part of\n                # of the keyword. So we need to skip the insertion of\n                # invisible parentheses to work more precisely.\n                continue\n\n            elif (\n                isinstance(child, Leaf)\n                and child.next_sibling is not None\n                and child.next_sibling.type == token.COLON\n                and child.value == \"case\"\n            ):\n                # A special patch for \"case case:\" scenario, the second occurrence\n                # of case will be not parsed as a Python keyword.\n                break\n\n            elif not is_multiline_string(child):\n                wrap_in_parentheses(node, child, visible=False)\n\n        comma_check = child.type == token.COMMA\n\n        check_lpar = isinstance(child, Leaf) and (\n            child.value in parens_after or comma_check\n        )",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 110,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "_normalize_import_from",
      "sourceCode": "def _normalize_import_from(parent: Node, child: LN, index: int) -> None:\n    # \"import from\" nodes store parentheses directly as part of\n    # the statement\n    if is_lpar_token(child):\n        assert is_rpar_token(parent.children[-1])\n        # make parentheses invisible\n        child.value = \"\"\n        parent.children[-1].value = \"\"\n    elif child.type != token.STAR:\n        # insert invisible parentheses\n        parent.insert_child(index, Leaf(token.LPAR, \"\"))\n        parent.append_child(Leaf(token.RPAR, \"\"))",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "remove_await_parens",
      "sourceCode": "def remove_await_parens(node: Node) -> None:\n    if node.children[0].type == token.AWAIT and len(node.children) > 1:\n        if (\n            node.children[1].type == syms.atom\n            and node.children[1].children[0].type == token.LPAR\n        ):\n            if maybe_make_parens_invisible_in_atom(\n                node.children[1],\n                parent=node,\n                remove_brackets_around_comma=True,\n            ):\n                wrap_in_parentheses(node, node.children[1], visible=False)\n\n            # Since await is an expression we shouldn't remove\n            # brackets in cases where this would change\n            # the AST due to operator precedence.\n            # Therefore we only aim to remove brackets around\n            # power nodes that aren't also await expressions themselves.\n            # https://peps.python.org/pep-0492/#updated-operator-precedence-table\n            # N.B. We've still removed any redundant nested brackets though :)\n            opening_bracket = cast(Leaf, node.children[1].children[0])\n            closing_bracket = cast(Leaf, node.children[1].children[-1])\n            bracket_contents = node.children[1].children[1]\n            if isinstance(bracket_contents, Node) and (\n                bracket_contents.type != syms.power\n                or bracket_contents.children[0].type == token.AWAIT\n                or any(\n                    isinstance(child, Leaf) and child.type == token.DOUBLESTAR\n                    for child in bracket_contents.children\n                )\n            ):\n                ensure_visible(opening_bracket)\n                ensure_visible(closing_bracket)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 32,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "_maybe_wrap_cms_in_parens",
      "sourceCode": "def _maybe_wrap_cms_in_parens(\n    node: Node, mode: Mode, features: Collection[Feature]\n) -> None:\n    \"\"\"When enabled and safe, wrap the multiple context managers in invisible parens.\n\n    It is only safe when `features` contain Feature.PARENTHESIZED_CONTEXT_MANAGERS.\n    \"\"\"\n    if (\n        Feature.PARENTHESIZED_CONTEXT_MANAGERS not in features\n        or len(node.children) <= 2\n        # If it's an atom, it's already wrapped in parens.\n        or node.children[1].type == syms.atom\n    ):\n        return\n    colon_index: Optional[int] = None\n    for i in range(2, len(node.children)):\n        if node.children[i].type == token.COLON:\n            colon_index = i\n            break\n    if colon_index is not None:\n        lpar = Leaf(token.LPAR, \"\")\n        rpar = Leaf(token.RPAR, \"\")\n        context_managers = node.children[1:colon_index]\n        for child in context_managers:\n            child.remove()\n        # After wrapping, the with_stmt will look like this:\n        #   with_stmt\n        #     NAME 'with'\n        #     atom\n        #       LPAR ''\n        #       testlist_gexp\n        #         ... <-- context_managers\n        #       /testlist_gexp\n        #       RPAR ''\n        #     /atom\n        #     COLON ':'\n        new_child = Node(\n            syms.atom, [lpar, Node(syms.testlist_gexp, context_managers), rpar]\n        )\n        node.insert_child(1, new_child)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 39,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "remove_with_parens",
      "sourceCode": "def remove_with_parens(node: Node, parent: Node) -> None:\n    \"\"\"Recursively hide optional parens in `with` statements.\"\"\"\n    # Removing all unnecessary parentheses in with statements in one pass is a tad\n    # complex as different variations of bracketed statements result in pretty\n    # different parse trees:\n    #\n    # with (open(\"file\")) as f:                       # this is an asexpr_test\n    #     ...\n    #\n    # with (open(\"file\") as f):                       # this is an atom containing an\n    #     ...                                         # asexpr_test\n    #\n    # with (open(\"file\")) as f, (open(\"file\")) as f:  # this is asexpr_test, COMMA,\n    #     ...                                         # asexpr_test\n    #\n    # with (open(\"file\") as f, open(\"file\") as f):    # an atom containing a\n    #     ...                                         # testlist_gexp which then\n    #                                                 # contains multiple asexpr_test(s)\n    if node.type == syms.atom:\n        if maybe_make_parens_invisible_in_atom(\n            node,\n            parent=parent,\n            remove_brackets_around_comma=True,\n        ):\n            wrap_in_parentheses(parent, node, visible=False)\n        if isinstance(node.children[1], Node):\n            remove_with_parens(node.children[1], node)\n    elif node.type == syms.testlist_gexp:\n        for child in node.children:\n            if isinstance(child, Node):\n                remove_with_parens(child, node)\n    elif node.type == syms.asexpr_test and not any(\n        leaf.type == token.COLONEQUAL for leaf in node.leaves()\n    ):\n        if maybe_make_parens_invisible_in_atom(\n            node.children[0],\n            parent=node,\n            remove_brackets_around_comma=True,\n        ):\n            wrap_in_parentheses(node, node.children[0], visible=False)",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 39,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "maybe_make_parens_invisible_in_atom",
      "sourceCode": "def maybe_make_parens_invisible_in_atom(\n    node: LN,\n    parent: LN,\n    remove_brackets_around_comma: bool = False,\n) -> bool:\n    \"\"\"If it's safe, make the parens in the atom `node` invisible, recursively.\n    Additionally, remove repeated, adjacent invisible parens from the atom `node`\n    as they are redundant.\n\n    Returns whether the node should itself be wrapped in invisible parentheses.\n    \"\"\"\n    if (\n        node.type not in (syms.atom, syms.expr)\n        or is_empty_tuple(node)\n        or is_one_tuple(node)\n        or (is_yield(node) and parent.type != syms.expr_stmt)\n        or (\n            # This condition tries to prevent removing non-optional brackets\n            # around a tuple, however, can be a bit overzealous so we provide\n            # and option to skip this check for `for` and `with` statements.\n            not remove_brackets_around_comma\n            and max_delimiter_priority_in_atom(node) >= COMMA_PRIORITY\n        )\n        or is_tuple_containing_walrus(node)\n        or is_tuple_containing_star(node)\n        or is_generator(node)\n    ):\n        return False\n\n    if is_walrus_assignment(node):\n        if parent.type in [\n            syms.annassign,\n            syms.expr_stmt,\n            syms.assert_stmt,\n            syms.return_stmt,\n            syms.except_clause,\n            syms.funcdef,\n            syms.with_stmt,\n            syms.tname,\n            # these ones aren't useful to end users, but they do please fuzzers\n            syms.for_stmt,\n            syms.del_stmt,\n            syms.for_stmt,\n        ]:\n            return False\n\n    first = node.children[0]\n    last = node.children[-1]\n    if is_lpar_token(first) and is_rpar_token(last):\n        middle = node.children[1]\n        # make parentheses invisible\n        if (\n            # If the prefix of `middle` includes a type comment with\n            # ignore annotation, then we do not remove the parentheses\n            not is_type_ignore_comment_string(middle.prefix.strip())\n        ):\n            first.value = \"\"\n            last.value = \"\"\n        maybe_make_parens_invisible_in_atom(\n            middle,\n            parent=parent,\n            remove_brackets_around_comma=remove_brackets_around_comma,\n        )\n\n        if is_atom_with_invisible_parens(middle):\n            # Strip the invisible parens from `middle` by replacing\n            # it with the child in-between the invisible parens\n            middle.replace(middle.children[1])\n\n            if middle.children[0].prefix.strip():\n                # Preserve comments before first paren\n                middle.children[1].prefix = (\n                    middle.children[0].prefix + middle.children[1].prefix\n                )\n\n            if middle.children[-1].prefix.strip():\n                # Preserve comments before last paren\n                last.prefix = middle.children[-1].prefix + last.prefix\n\n        return False\n\n    return True",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 81,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "should_split_line",
      "sourceCode": "def should_split_line(line: Line, opening_bracket: Leaf) -> bool:\n    \"\"\"Should `line` be immediately split with `delimiter_split()` after RHS?\"\"\"\n\n    if not (opening_bracket.parent and opening_bracket.value in \"[{(\"):\n        return False\n\n    # We're essentially checking if the body is delimited by commas and there's more\n    # than one of them (we're excluding the trailing comma and if the delimiter priority\n    # is still commas, that means there's more).\n    exclude = set()\n    trailing_comma = False\n    try:\n        last_leaf = line.leaves[-1]\n        if last_leaf.type == token.COMMA:\n            trailing_comma = True\n            exclude.add(id(last_leaf))\n        max_priority = line.bracket_tracker.max_delimiter_priority(exclude=exclude)\n    except (IndexError, ValueError):\n        return False\n\n    return max_priority == COMMA_PRIORITY and (\n        (line.mode.magic_trailing_comma and trailing_comma)\n        # always explode imports\n        or opening_bracket.parent.type in {syms.atom, syms.import_from}\n    )",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 24,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "generate_trailers_to_omit",
      "sourceCode": "def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[set[LeafID]]:\n    \"\"\"Generate sets of closing bracket IDs that should be omitted in a RHS.\n\n    Brackets can be omitted if the entire trailer up to and including\n    a preceding closing bracket fits in one line.\n\n    Yielded sets are cumulative (contain results of previous yields, too).  First\n    set is empty, unless the line should explode, in which case bracket pairs until\n    the one that needs to explode are omitted.\n    \"\"\"\n\n    omit: set[LeafID] = set()\n    if not line.magic_trailing_comma:\n        yield omit\n\n    length = 4 * line.depth\n    opening_bracket: Optional[Leaf] = None\n    closing_bracket: Optional[Leaf] = None\n    inner_brackets: set[LeafID] = set()\n    for index, leaf, leaf_length in line.enumerate_with_length(is_reversed=True):\n        length += leaf_length\n        if length > line_length:\n            break\n\n        has_inline_comment = leaf_length > len(leaf.value) + len(leaf.prefix)\n        if leaf.type == STANDALONE_COMMENT or has_inline_comment:\n            break\n\n        if opening_bracket:\n            if leaf is opening_bracket:\n                opening_bracket = None\n            elif leaf.type in CLOSING_BRACKETS:\n                prev = line.leaves[index - 1] if index > 0 else None\n                if (\n                    prev\n                    and prev.type == token.COMMA\n                    and leaf.opening_bracket is not None\n                    and not is_one_sequence_between(\n                        leaf.opening_bracket, leaf, line.leaves\n                    )\n                ):\n                    # Never omit bracket pairs with trailing commas.\n                    # We need to explode on those.\n                    break\n\n                inner_brackets.add(id(leaf))\n        elif leaf.type in CLOSING_BRACKETS:\n            prev = line.leaves[index - 1] if index > 0 else None\n            if prev and prev.type in OPENING_BRACKETS:\n                # Empty brackets would fail a split so treat them as \"inner\"\n                # brackets (e.g. only add them to the `omit` set if another\n                # pair of brackets was good enough.\n                inner_brackets.add(id(leaf))\n                continue\n\n            if closing_bracket:\n                omit.add(id(closing_bracket))\n                omit.update(inner_brackets)\n                inner_brackets.clear()\n                yield omit\n\n            if (\n                prev\n                and prev.type == token.COMMA\n                and leaf.opening_bracket is not None\n                and not is_one_sequence_between(leaf.opening_bracket, leaf, line.leaves)\n            ):\n                # Never omit bracket pairs with trailing commas.\n                # We need to explode on those.\n                break\n\n            if leaf.value:\n                opening_bracket = leaf.opening_bracket\n                closing_bracket = leaf",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 73,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "run_transformer",
      "sourceCode": "def run_transformer(\n    line: Line,\n    transform: Transformer,\n    mode: Mode,\n    features: Collection[Feature],\n    *,\n    line_str: str = \"\",\n) -> list[Line]:\n    if not line_str:\n        line_str = line_to_string(line)\n    result: list[Line] = []\n    for transformed_line in transform(line, features, mode):\n        if str(transformed_line).strip(\"\\n\") == line_str:\n            raise CannotTransform(\"Line transformer returned an unchanged result\")\n\n        result.extend(transform_line(transformed_line, mode=mode, features=features))\n\n    features_set = set(features)\n    if (\n        Feature.FORCE_OPTIONAL_PARENTHESES in features_set\n        or transform.__class__.__name__ != \"rhs\"\n        or not line.bracket_tracker.invisible\n        or any(bracket.value for bracket in line.bracket_tracker.invisible)\n        or line.contains_multiline_strings()\n        or result[0].contains_uncollapsable_type_comments()\n        or result[0].contains_unsplittable_type_ignore()\n        or is_line_short_enough(result[0], mode=mode)\n        # If any leaves have no parents (which _can_ occur since\n        # `transform(line)` potentially destroys the line's underlying node\n        # structure), then we can't proceed. Doing so would cause the below\n        # call to `append_leaves()` to fail.\n        or any(leaf.parent is None for leaf in line.leaves)\n    ):\n        return result\n\n    line_copy = line.clone()\n    append_leaves(line_copy, line, line.leaves)\n    features_fop = features_set | {Feature.FORCE_OPTIONAL_PARENTHESES}\n    second_opinion = run_transformer(\n        line_copy, transform, mode, features_fop, line_str=line_str\n    )\n    if all(is_line_short_enough(ln, mode=mode) for ln in second_opinion):\n        result = second_opinion\n    return result",
      "importString": "import re\nimport sys\nfrom collections.abc import Collection, Iterator\nfrom dataclasses import replace\nfrom enum import Enum, auto\nfrom functools import partial, wraps\nfrom typing import Optional, Union, cast\nfrom black.brackets import (\nCOMMA_PRIORITY\nDOT_PRIORITY\nSTRING_PRIORITY\nget_leaves_inside_matching_brackets\nmax_delimiter_priority_in_atom\n)\nfrom black.comments import FMT_OFF, generate_comments, list_comments\nfrom black.lines import (\nLine\nRHSResult\nappend_leaves\ncan_be_split\ncan_omit_invisible_parens\nis_line_short_enough\nline_to_string\n)\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nASSIGNMENTS\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nSTATEMENT\nWHITESPACE\nVisitor\nensure_visible\nfstring_to_string\nget_annotation_type\nis_arith_like\nis_async_stmt_or_funcdef\nis_atom_with_invisible_parens\nis_docstring\nis_empty_tuple\nis_generator\nis_lpar_token\nis_multiline_string\nis_name_token\nis_one_sequence_between\nis_one_tuple\nis_parent_function_or_class\nis_part_of_annotation\nis_rpar_token\nis_stub_body\nis_stub_suite\nis_tuple_containing_star\nis_tuple_containing_walrus\nis_type_ignore_comment_string\nis_vararg\nis_walrus_assignment\nis_yield\nsyms\nwrap_in_parentheses\n)\nfrom black.numerics import normalize_numeric_literal\nfrom black.strings import (\nfix_multiline_docstring\nget_string_prefix\nnormalize_string_prefix\nnormalize_string_quotes\nnormalize_unicode_escape_sequences\n)\nfrom black.trans import (\nCannotTransform\nStringMerger\nStringParenStripper\nStringParenWrapper\nStringSplitter\nTransformer\nhug_power_op\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 43,
      "relativeDocumentPath": "src/black/linegen.py"
    },
    {
      "symbolName": "append",
      "sourceCode": "def append(\n        self, leaf: Leaf, preformatted: bool = False, track_bracket: bool = False\n    ) -> None:\n        \"\"\"Add a new `leaf` to the end of the line.\n\n        Unless `preformatted` is True, the `leaf` will receive a new consistent\n        whitespace prefix and metadata applied by :class:`BracketTracker`.\n        Trailing commas are maybe removed, unpacked for loop variables are\n        demoted from being delimiters.\n\n        Inline comments are put aside.\n        \"\"\"\n        has_value = (\n            leaf.type in BRACKETS\n            # empty fstring-middles must not be truncated\n            or leaf.type == token.FSTRING_MIDDLE\n            or bool(leaf.value.strip())\n        )\n        if not has_value:\n            return\n\n        if token.COLON == leaf.type and self.is_class_paren_empty:\n            del self.leaves[-2:]\n        if self.leaves and not preformatted:\n            # Note: at this point leaf.prefix should be empty except for\n            # imports, for which we only preserve newlines.\n            leaf.prefix += whitespace(\n                leaf,\n                complex_subscript=self.is_complex_subscript(leaf),\n                mode=self.mode,\n            )\n        if self.inside_brackets or not preformatted or track_bracket:\n            self.bracket_tracker.mark(leaf)\n            if self.mode.magic_trailing_comma:\n                if self.has_magic_trailing_comma(leaf):\n                    self.magic_trailing_comma = leaf\n            elif self.has_magic_trailing_comma(leaf):\n                self.remove_trailing_comma()\n        if not self.append_comment(leaf):\n            self.leaves.append(leaf)",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 39,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "append_safe",
      "sourceCode": "def append_safe(self, leaf: Leaf, preformatted: bool = False) -> None:\n        \"\"\"Like :func:`append()` but disallow invalid standalone comment structure.\n\n        Raises ValueError when any `leaf` is appended after a standalone comment\n        or when a standalone comment is not the first leaf on the line.\n        \"\"\"\n        if (\n            self.bracket_tracker.depth == 0\n            or self.bracket_tracker.any_open_for_or_lambda()\n        ):\n            if self.is_comment:\n                raise ValueError(\"cannot append to standalone comments\")\n\n            if self.leaves and leaf.type == STANDALONE_COMMENT:\n                raise ValueError(\n                    \"cannot append standalone comments to a populated line\"\n                )\n\n        self.append(leaf, preformatted=preformatted)",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 18,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "is_def",
      "sourceCode": "@property\n    def is_def(self) -> bool:\n        \"\"\"Is this a function definition? (Also returns True for async defs.)\"\"\"\n        try:\n            first_leaf = self.leaves[0]\n        except IndexError:\n            return False\n\n        try:\n            second_leaf: Optional[Leaf] = self.leaves[1]\n        except IndexError:\n            second_leaf = None\n        return (first_leaf.type == token.NAME and first_leaf.value == \"def\") or (\n            first_leaf.type == token.ASYNC\n            and second_leaf is not None\n            and second_leaf.type == token.NAME\n            and second_leaf.value == \"def\"\n        )",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 17,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "is_class_paren_empty",
      "sourceCode": "@property\n    def is_class_paren_empty(self) -> bool:\n        \"\"\"Is this a class with no base classes but using parentheses?\n\n        Those are unnecessary and should be removed.\n        \"\"\"\n        return (\n            bool(self)\n            and len(self.leaves) == 4\n            and self.is_class\n            and self.leaves[2].type == token.LPAR\n            and self.leaves[2].value == \"(\"\n            and self.leaves[3].type == token.RPAR\n            and self.leaves[3].value == \")\"\n        )",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "_is_triple_quoted_string",
      "sourceCode": "@property\n    def _is_triple_quoted_string(self) -> bool:\n        \"\"\"Is the line a triple quoted string?\"\"\"\n        if not self or self.leaves[0].type != token.STRING:\n            return False\n        value = self.leaves[0].value\n        if value.startswith(('\"\"\"', \"'''\")):\n            return True\n        if value.startswith((\"r'''\", 'r\"\"\"', \"R'''\", 'R\"\"\"')):\n            return True\n        return False",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 10,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "is_fmt_pass_converted",
      "sourceCode": "def is_fmt_pass_converted(\n        self, *, first_leaf_matches: Optional[Callable[[Leaf], bool]] = None\n    ) -> bool:\n        \"\"\"Is this line converted from fmt off/skip code?\n\n        If first_leaf_matches is not None, it only returns True if the first\n        leaf of converted code matches.\n        \"\"\"\n        if len(self.leaves) != 1:\n            return False\n        leaf = self.leaves[0]\n        if (\n            leaf.type != STANDALONE_COMMENT\n            or leaf.fmt_pass_converted_first_leaf is None\n        ):\n            return False\n        return first_leaf_matches is None or first_leaf_matches(\n            leaf.fmt_pass_converted_first_leaf\n        )",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 18,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "contains_implicit_multiline_string_with_comments",
      "sourceCode": "def contains_implicit_multiline_string_with_comments(self) -> bool:\n        \"\"\"Chck if we have an implicit multiline string with comments on the line\"\"\"\n        for leaf_type, leaf_group_iterator in itertools.groupby(\n            self.leaves, lambda leaf: leaf.type\n        ):\n            if leaf_type != token.STRING:\n                continue\n            leaf_list = list(leaf_group_iterator)\n            if len(leaf_list) == 1:\n                continue\n            for leaf in leaf_list:\n                if self.comments_after(leaf):\n                    return True\n        return False",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 13,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "contains_uncollapsable_type_comments",
      "sourceCode": "def contains_uncollapsable_type_comments(self) -> bool:\n        ignored_ids = set()\n        try:\n            last_leaf = self.leaves[-1]\n            ignored_ids.add(id(last_leaf))\n            if last_leaf.type == token.COMMA or (\n                last_leaf.type == token.RPAR and not last_leaf.value\n            ):\n                # When trailing commas or optional parens are inserted by Black for\n                # consistency, comments after the previous last element are not moved\n                # (they don't have to, rendering will still be correct).  So we ignore\n                # trailing commas and invisible.\n                last_leaf = self.leaves[-2]\n                ignored_ids.add(id(last_leaf))\n        except IndexError:\n            return False\n\n        # A type comment is uncollapsable if it is attached to a leaf\n        # that isn't at the end of the line (since that could cause it\n        # to get associated to a different argument) or if there are\n        # comments before it (since that could cause it to get hidden\n        # behind a comment.\n        comment_seen = False\n        for leaf_id, comments in self.comments.items():\n            for comment in comments:\n                if is_type_comment(comment):\n                    if comment_seen or (\n                        not is_type_ignore_comment(comment)\n                        and leaf_id not in ignored_ids\n                    ):\n                        return True\n\n                comment_seen = True\n\n        return False",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 34,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "contains_unsplittable_type_ignore",
      "sourceCode": "def contains_unsplittable_type_ignore(self) -> bool:\n        if not self.leaves:\n            return False\n\n        # If a 'type: ignore' is attached to the end of a line, we\n        # can't split the line, because we can't know which of the\n        # subexpressions the ignore was meant to apply to.\n        #\n        # We only want this to apply to actual physical lines from the\n        # original source, though: we don't want the presence of a\n        # 'type: ignore' at the end of a multiline expression to\n        # justify pushing it all onto one line. Thus we\n        # (unfortunately) need to check the actual source lines and\n        # only report an unsplittable 'type: ignore' if this line was\n        # one line in the original code.\n\n        # Grab the first and last line numbers, skipping generated leaves\n        first_line = next((leaf.lineno for leaf in self.leaves if leaf.lineno != 0), 0)\n        last_line = next(\n            (leaf.lineno for leaf in reversed(self.leaves) if leaf.lineno != 0), 0\n        )\n\n        if first_line == last_line:\n            # We look at the last two leaves since a comma or an\n            # invisible paren could have been added at the end of the\n            # line.\n            for node in self.leaves[-2:]:\n                for comment in self.comments.get(id(node), []):\n                    if is_type_ignore_comment(comment):\n                        return True\n\n        return False",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 31,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "has_magic_trailing_comma",
      "sourceCode": "def has_magic_trailing_comma(self, closing: Leaf) -> bool:\n        \"\"\"Return True if we have a magic trailing comma, that is when:\n        - there's a trailing comma here\n        - it's not from single-element square bracket indexing\n        - it's not a one-tuple\n        \"\"\"\n        if not (\n            closing.type in CLOSING_BRACKETS\n            and self.leaves\n            and self.leaves[-1].type == token.COMMA\n        ):\n            return False\n\n        if closing.type == token.RBRACE:\n            return True\n\n        if closing.type == token.RSQB:\n            if (\n                closing.parent is not None\n                and closing.parent.type == syms.trailer\n                and closing.opening_bracket is not None\n                and is_one_sequence_between(\n                    closing.opening_bracket,\n                    closing,\n                    self.leaves,\n                    brackets=(token.LSQB, token.RSQB),\n                )\n            ):\n                assert closing.prev_sibling is not None\n                assert closing.prev_sibling.type == syms.subscriptlist\n                return False\n\n            return True\n\n        if self.is_import:\n            return True\n\n        if closing.opening_bracket is not None and not is_one_sequence_between(\n            closing.opening_bracket, closing, self.leaves\n        ):\n            return True\n\n        return False",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 42,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "append_comment",
      "sourceCode": "def append_comment(self, comment: Leaf) -> bool:\n        \"\"\"Add an inline or standalone comment to the line.\"\"\"\n        if (\n            comment.type == STANDALONE_COMMENT\n            and self.bracket_tracker.any_open_brackets()\n        ):\n            comment.prefix = \"\"\n            return False\n\n        if comment.type != token.COMMENT:\n            return False\n\n        if not self.leaves:\n            comment.type = STANDALONE_COMMENT\n            comment.prefix = \"\"\n            return False\n\n        last_leaf = self.leaves[-1]\n        if (\n            last_leaf.type == token.RPAR\n            and not last_leaf.value\n            and last_leaf.parent\n            and len(list(last_leaf.parent.leaves())) <= 3\n            and not is_type_comment(comment)\n        ):\n            # Comments on an optional parens wrapping a single leaf should belong to\n            # the wrapped node except if it's a type comment. Pinning the comment like\n            # this avoids unstable formatting caused by comment migration.\n            if len(self.leaves) < 2:\n                comment.type = STANDALONE_COMMENT\n                comment.prefix = \"\"\n                return False\n\n            last_leaf = self.leaves[-2]\n        self.comments.setdefault(id(last_leaf), []).append(comment)\n        return True",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 35,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "is_complex_subscript",
      "sourceCode": "def is_complex_subscript(self, leaf: Leaf) -> bool:\n        \"\"\"Return True iff `leaf` is part of a slice with non-trivial exprs.\"\"\"\n        open_lsqb = self.bracket_tracker.get_open_lsqb()\n        if open_lsqb is None:\n            return False\n\n        subscript_start = open_lsqb.next_sibling\n\n        if isinstance(subscript_start, Node):\n            if subscript_start.type == syms.listmaker:\n                return False\n\n            if subscript_start.type == syms.subscriptlist:\n                subscript_start = child_towards(subscript_start, leaf)\n\n        return subscript_start is not None and any(\n            n.type in TEST_DESCENDANTS for n in subscript_start.pre_order()\n        )",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 17,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "enumerate_with_length",
      "sourceCode": "def enumerate_with_length(\n        self, is_reversed: bool = False\n    ) -> Iterator[tuple[Index, Leaf, int]]:\n        \"\"\"Return an enumeration of leaves with their length.\n\n        Stops prematurely on multiline strings and standalone comments.\n        \"\"\"\n        op = cast(\n            Callable[[Sequence[Leaf]], Iterator[tuple[Index, Leaf]]],\n            enumerate_reversed if is_reversed else enumerate,\n        )\n        for index, leaf in op(self.leaves):\n            length = len(leaf.prefix) + len(leaf.value)\n            if \"\\n\" in leaf.value:\n                return  # Multiline strings, we can't continue.\n\n            for comment in self.comments_after(leaf):\n                length += len(comment.value)\n\n            yield index, leaf, length",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 19,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "__str__",
      "sourceCode": "def __str__(self) -> str:\n        \"\"\"Render the line.\"\"\"\n        if not self:\n            return \"\\n\"\n\n        indent = \"    \" * self.depth\n        leaves = iter(self.leaves)\n        first = next(leaves)\n        res = f\"{first.prefix}{indent}{first.value}\"\n        for leaf in leaves:\n            res += str(leaf)\n        for comment in itertools.chain.from_iterable(self.comments.values()):\n            res += str(comment)\n\n        return res + \"\\n\"",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "maybe_empty_lines",
      "sourceCode": "def maybe_empty_lines(self, current_line: Line) -> LinesBlock:\n        \"\"\"Return the number of extra empty lines before and after the `current_line`.\n\n        This is for separating `def`, `async def` and `class` with extra empty\n        lines (two on module-level).\n        \"\"\"\n        form_feed = (\n            current_line.depth == 0\n            and bool(current_line.leaves)\n            and \"\\f\\n\" in current_line.leaves[0].prefix\n        )\n        before, after = self._maybe_empty_lines(current_line)\n        previous_after = self.previous_block.after if self.previous_block else 0\n        before = max(0, before - previous_after)\n        if (\n            # Always have one empty line after a module docstring\n            self.previous_block\n            and self.previous_block.previous_block is None\n            and len(self.previous_block.original_line.leaves) == 1\n            and self.previous_block.original_line.is_docstring\n            and not (current_line.is_class or current_line.is_def)\n        ):\n            before = 1\n\n        block = LinesBlock(\n            mode=self.mode,\n            previous_block=self.previous_block,\n            original_line=current_line,\n            before=before,\n            after=after,\n            form_feed=form_feed,\n        )\n\n        # Maintain the semantic_leading_comment state.\n        if current_line.is_comment:\n            if self.previous_line is None or (\n                not self.previous_line.is_decorator\n                # `or before` means this comment already has an empty line before\n                and (not self.previous_line.is_comment or before)\n                and (self.semantic_leading_comment is None or before)\n            ):\n                self.semantic_leading_comment = block\n        # `or before` means this decorator already has an empty line before\n        elif not current_line.is_decorator or before:\n            self.semantic_leading_comment = None\n\n        self.previous_line = current_line\n        self.previous_block = block\n        return block",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 48,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "_maybe_empty_lines",
      "sourceCode": "def _maybe_empty_lines(self, current_line: Line) -> tuple[int, int]:  # noqa: C901\n        max_allowed = 1\n        if current_line.depth == 0:\n            max_allowed = 1 if self.mode.is_pyi else 2\n\n        if current_line.leaves:\n            # Consume the first leaf's extra newlines.\n            first_leaf = current_line.leaves[0]\n            before = first_leaf.prefix.count(\"\\n\")\n            before = min(before, max_allowed)\n            first_leaf.prefix = \"\"\n        else:\n            before = 0\n\n        user_had_newline = bool(before)\n        depth = current_line.depth\n\n        # Mutate self.previous_defs, remainder of this function should be pure\n        previous_def = None\n        while self.previous_defs and self.previous_defs[-1].depth >= depth:\n            previous_def = self.previous_defs.pop()\n        if current_line.is_def or current_line.is_class:\n            self.previous_defs.append(current_line)\n\n        if self.previous_line is None:\n            # Don't insert empty lines before the first line in the file.\n            return 0, 0\n\n        if current_line.is_docstring:\n            if self.previous_line.is_class:\n                return 0, 1\n            if self.previous_line.opens_block and self.previous_line.is_def:\n                return 0, 0\n\n        if previous_def is not None:\n            assert self.previous_line is not None\n            if self.mode.is_pyi:\n                if previous_def.is_class and not previous_def.is_stub_class:\n                    before = 1\n                elif depth and not current_line.is_def and self.previous_line.is_def:\n                    # Empty lines between attributes and methods should be preserved.\n                    before = 1 if user_had_newline else 0\n                elif depth:\n                    before = 0\n                else:\n                    before = 1\n            else:\n                if depth:\n                    before = 1\n                elif (\n                    not depth\n                    and previous_def.depth\n                    and current_line.leaves[-1].type == token.COLON\n                    and (\n                        current_line.leaves[0].value\n                        not in (\"with\", \"try\", \"for\", \"while\", \"if\", \"match\")\n                    )\n                ):\n                    # We shouldn't add two newlines between an indented function and\n                    # a dependent non-indented clause. This is to avoid issues with\n                    # conditional function definitions that are technically top-level\n                    # and therefore get two trailing newlines, but look weird and\n                    # inconsistent when they're followed by elif, else, etc. This is\n                    # worse because these functions only get *one* preceding newline\n                    # already.\n                    before = 1\n                else:\n                    before = 2\n\n        if current_line.is_decorator or current_line.is_def or current_line.is_class:\n            return self._maybe_empty_lines_for_class_or_def(\n                current_line, before, user_had_newline\n            )\n\n        if (\n            self.previous_line.is_import\n            and self.previous_line.depth == 0\n            and current_line.depth == 0\n            and not current_line.is_import\n            and Preview.always_one_newline_after_import in self.mode\n        ):\n            return 1, 0\n\n        if (\n            self.previous_line.is_import\n            and not current_line.is_import\n            and not current_line.is_fmt_pass_converted(first_leaf_matches=is_import)\n            and depth == self.previous_line.depth\n        ):\n            return (before or 1), 0\n\n        return before, 0",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 91,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "_maybe_empty_lines_for_class_or_def",
      "sourceCode": "def _maybe_empty_lines_for_class_or_def(  # noqa: C901\n        self, current_line: Line, before: int, user_had_newline: bool\n    ) -> tuple[int, int]:\n        assert self.previous_line is not None\n\n        if self.previous_line.is_decorator:\n            if self.mode.is_pyi and current_line.is_stub_class:\n                # Insert an empty line after a decorated stub class\n                return 0, 1\n            return 0, 0\n\n        if self.previous_line.depth < current_line.depth and (\n            self.previous_line.is_class or self.previous_line.is_def\n        ):\n            if self.mode.is_pyi:\n                return 0, 0\n            return 1 if user_had_newline else 0, 0\n\n        comment_to_add_newlines: Optional[LinesBlock] = None\n        if (\n            self.previous_line.is_comment\n            and self.previous_line.depth == current_line.depth\n            and before == 0\n        ):\n            slc = self.semantic_leading_comment\n            if (\n                slc is not None\n                and slc.previous_block is not None\n                and not slc.previous_block.original_line.is_class\n                and not slc.previous_block.original_line.opens_block\n                and slc.before <= 1\n            ):\n                comment_to_add_newlines = slc\n            else:\n                return 0, 0\n\n        if self.mode.is_pyi:\n            if current_line.is_class or self.previous_line.is_class:\n                if self.previous_line.depth < current_line.depth:\n                    newlines = 0\n                elif self.previous_line.depth > current_line.depth:\n                    newlines = 1\n                elif current_line.is_stub_class and self.previous_line.is_stub_class:\n                    # No blank line between classes with an empty body\n                    newlines = 0\n                else:\n                    newlines = 1\n            # Don't inspect the previous line if it's part of the body of the previous\n            # statement in the same level, we always want a blank line if there's\n            # something with a body preceding.\n            elif self.previous_line.depth > current_line.depth:\n                newlines = 1\n            elif (\n                current_line.is_def or current_line.is_decorator\n            ) and not self.previous_line.is_def:\n                if current_line.depth:\n                    # In classes empty lines between attributes and methods should\n                    # be preserved.\n                    newlines = min(1, before)\n                else:\n                    # Blank line between a block of functions (maybe with preceding\n                    # decorators) and a block of non-functions\n                    newlines = 1\n            else:\n                newlines = 0\n        else:\n            newlines = 1 if current_line.depth else 2\n            # If a user has left no space after a dummy implementation, don't insert\n            # new lines. This is useful for instance for @overload or Protocols.\n            if self.previous_line.is_stub_def and not user_had_newline:\n                newlines = 0\n        if comment_to_add_newlines is not None:\n            previous_block = comment_to_add_newlines.previous_block\n            if previous_block is not None:\n                comment_to_add_newlines.before = (\n                    max(comment_to_add_newlines.before, newlines) - previous_block.after\n                )\n                newlines = 0\n        return newlines, 0",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 78,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "append_leaves",
      "sourceCode": "def append_leaves(\n    new_line: Line, old_line: Line, leaves: list[Leaf], preformatted: bool = False\n) -> None:\n    \"\"\"\n    Append leaves (taken from @old_line) to @new_line, making sure to fix the\n    underlying Node structure where appropriate.\n\n    All of the leaves in @leaves are duplicated. The duplicates are then\n    appended to @new_line and used to replace their originals in the underlying\n    Node structure. Any comments attached to the old leaves are reattached to\n    the new leaves.\n\n    Pre-conditions:\n        set(@leaves) is a subset of set(@old_line.leaves).\n    \"\"\"\n    for old_leaf in leaves:\n        new_leaf = Leaf(old_leaf.type, old_leaf.value)\n        replace_child(old_leaf, new_leaf)\n        new_line.append(new_leaf, preformatted=preformatted)\n\n        for comment_leaf in old_line.comments_after(old_leaf):\n            new_line.append(comment_leaf, preformatted=True)",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 21,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "is_line_short_enough",
      "sourceCode": "def is_line_short_enough(  # noqa: C901\n    line: Line, *, mode: Mode, line_str: str = \"\"\n) -> bool:\n    \"\"\"For non-multiline strings, return True if `line` is no longer than `line_length`.\n    For multiline strings, looks at the context around `line` to determine\n    if it should be inlined or split up.\n    Uses the provided `line_str` rendering, if any, otherwise computes a new one.\n    \"\"\"\n    if not line_str:\n        line_str = line_to_string(line)\n\n    if Preview.multiline_string_handling not in mode:\n        return (\n            str_width(line_str) <= mode.line_length\n            and \"\\n\" not in line_str  # multiline strings\n            and not line.contains_standalone_comments()\n        )\n\n    if line.contains_standalone_comments():\n        return False\n    if \"\\n\" not in line_str:\n        # No multiline strings (MLS) present\n        return str_width(line_str) <= mode.line_length\n\n    first, *_, last = line_str.split(\"\\n\")\n    if str_width(first) > mode.line_length or str_width(last) > mode.line_length:\n        return False\n\n    # Traverse the AST to examine the context of the multiline string (MLS),\n    # tracking aspects such as depth and comma existence,\n    # to determine whether to split the MLS or keep it together.\n    # Depth (which is based on the existing bracket_depth concept)\n    # is needed to determine nesting level of the MLS.\n    # Includes special case for trailing commas.\n    commas: list[int] = []  # tracks number of commas per depth level\n    multiline_string: Optional[Leaf] = None\n    # store the leaves that contain parts of the MLS\n    multiline_string_contexts: list[LN] = []\n\n    max_level_to_update: Union[int, float] = math.inf  # track the depth of the MLS\n    for i, leaf in enumerate(line.leaves):\n        if max_level_to_update == math.inf:\n            had_comma: Optional[int] = None\n            if leaf.bracket_depth + 1 > len(commas):\n                commas.append(0)\n            elif leaf.bracket_depth + 1 < len(commas):\n                had_comma = commas.pop()\n            if (\n                had_comma is not None\n                and multiline_string is not None\n                and multiline_string.bracket_depth == leaf.bracket_depth + 1\n            ):\n                # Have left the level with the MLS, stop tracking commas\n                max_level_to_update = leaf.bracket_depth\n                if had_comma > 0:\n                    # MLS was in parens with at least one comma - force split\n                    return False\n\n        if leaf.bracket_depth <= max_level_to_update and leaf.type == token.COMMA:\n            # Inside brackets, ignore trailing comma\n            # directly after MLS/MLS-containing expression\n            ignore_ctxs: list[Optional[LN]] = [None]\n            ignore_ctxs += multiline_string_contexts\n            if (line.inside_brackets or leaf.bracket_depth > 0) and (\n                i != len(line.leaves) - 1 or leaf.prev_sibling not in ignore_ctxs\n            ):\n                commas[leaf.bracket_depth] += 1\n        if max_level_to_update != math.inf:\n            max_level_to_update = min(max_level_to_update, leaf.bracket_depth)\n\n        if is_multiline_string(leaf):\n            if len(multiline_string_contexts) > 0:\n                # >1 multiline string cannot fit on a single line - force split\n                return False\n            multiline_string = leaf\n            ctx: LN = leaf\n            # fetch the leaf components of the MLS in the AST\n            while str(ctx) in line_str:\n                multiline_string_contexts.append(ctx)\n                if ctx.parent is None:\n                    break\n                ctx = ctx.parent\n\n    # May not have a triple-quoted multiline string at all,\n    # in case of a regular string with embedded newlines and line continuations\n    if len(multiline_string_contexts) == 0:\n        return True\n\n    return all(val == 0 for val in commas)",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 88,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "can_be_split",
      "sourceCode": "def can_be_split(line: Line) -> bool:\n    \"\"\"Return False if the line cannot be split *for sure*.\n\n    This is not an exhaustive search but a cheap heuristic that we can use to\n    avoid some unfortunate formattings (mostly around wrapping unsplittable code\n    in unnecessary parentheses).\n    \"\"\"\n    leaves = line.leaves\n    if len(leaves) < 2:\n        return False\n\n    if leaves[0].type == token.STRING and leaves[1].type == token.DOT:\n        call_count = 0\n        dot_count = 0\n        next = leaves[-1]\n        for leaf in leaves[-2::-1]:\n            if leaf.type in OPENING_BRACKETS:\n                if next.type not in CLOSING_BRACKETS:\n                    return False\n\n                call_count += 1\n            elif leaf.type == token.DOT:\n                dot_count += 1\n            elif leaf.type == token.NAME:\n                if not (next.type == token.DOT or next.type in OPENING_BRACKETS):\n                    return False\n\n            elif leaf.type not in CLOSING_BRACKETS:\n                return False\n\n            if dot_count > 1 and call_count > 1:\n                return False\n\n    return True",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 33,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "can_omit_invisible_parens",
      "sourceCode": "def can_omit_invisible_parens(\n    rhs: RHSResult,\n    line_length: int,\n) -> bool:\n    \"\"\"Does `rhs.body` have a shape safe to reformat without optional parens around it?\n\n    Returns True for only a subset of potentially nice looking formattings but\n    the point is to not return false positives that end up producing lines that\n    are too long.\n    \"\"\"\n    line = rhs.body\n\n    # We need optional parens in order to split standalone comments to their own lines\n    # if there are no nested parens around the standalone comments\n    closing_bracket: Optional[Leaf] = None\n    for leaf in reversed(line.leaves):\n        if closing_bracket and leaf is closing_bracket.opening_bracket:\n            closing_bracket = None\n        if leaf.type == STANDALONE_COMMENT and not closing_bracket:\n            return False\n        if (\n            not closing_bracket\n            and leaf.type in CLOSING_BRACKETS\n            and leaf.opening_bracket in line.leaves\n            and leaf.value\n        ):\n            closing_bracket = leaf\n\n    bt = line.bracket_tracker\n    if not bt.delimiters:\n        # Without delimiters the optional parentheses are useless.\n        return True\n\n    max_priority = bt.max_delimiter_priority()\n    delimiter_count = bt.delimiter_count_with_priority(max_priority)\n    if delimiter_count > 1:\n        # With more than one delimiter of a kind the optional parentheses read better.\n        return False\n\n    if delimiter_count == 1:\n        if max_priority == COMMA_PRIORITY and rhs.head.is_with_or_async_with_stmt:\n            # For two context manager with statements, the optional parentheses read\n            # better. In this case, `rhs.body` is the context managers part of\n            # the with statement. `rhs.head` is the `with (` part on the previous\n            # line.\n            return False\n        # Otherwise it may also read better, but we don't do it today and requires\n        # careful considerations for all possible cases. See\n        # https://github.com/psf/black/issues/2156.\n\n    if max_priority == DOT_PRIORITY:\n        # A single stranded method call doesn't require optional parentheses.\n        return True\n\n    assert len(line.leaves) >= 2, \"Stranded delimiter\"\n\n    # With a single delimiter, omit if the expression starts or ends with\n    # a bracket.\n    first = line.leaves[0]\n    second = line.leaves[1]\n    if first.type in OPENING_BRACKETS and second.type not in CLOSING_BRACKETS:\n        if _can_omit_opening_paren(line, first=first, line_length=line_length):\n            return True\n\n        # Note: we are not returning False here because a line might have *both*\n        # a leading opening bracket and a trailing closing bracket.  If the\n        # opening bracket doesn't match our rule, maybe the closing will.\n\n    penultimate = line.leaves[-2]\n    last = line.leaves[-1]\n\n    if (\n        last.type == token.RPAR\n        or last.type == token.RBRACE\n        or (\n            # don't use indexing for omitting optional parentheses;\n            # it looks weird\n            last.type == token.RSQB\n            and last.parent\n            and last.parent.type != syms.trailer\n        )\n    ):\n        if penultimate.type in OPENING_BRACKETS:\n            # Empty brackets don't help.\n            return False\n\n        if is_multiline_string(first):\n            # Additional wrapping of a multiline string in this situation is\n            # unnecessary.\n            return True\n\n        if _can_omit_closing_paren(line, last=last, line_length=line_length):\n            return True\n\n    return False",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 94,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "_can_omit_opening_paren",
      "sourceCode": "def _can_omit_opening_paren(line: Line, *, first: Leaf, line_length: int) -> bool:\n    \"\"\"See `can_omit_invisible_parens`.\"\"\"\n    remainder = False\n    length = 4 * line.depth\n    _index = -1\n    for _index, leaf, leaf_length in line.enumerate_with_length():\n        if leaf.type in CLOSING_BRACKETS and leaf.opening_bracket is first:\n            remainder = True\n        if remainder:\n            length += leaf_length\n            if length > line_length:\n                break\n\n            if leaf.type in OPENING_BRACKETS:\n                # There are brackets we can further split on.\n                remainder = False\n\n    else:\n        # checked the entire string and line length wasn't exceeded\n        if len(line.leaves) == _index + 1:\n            return True\n\n    return False",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 22,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "_can_omit_closing_paren",
      "sourceCode": "def _can_omit_closing_paren(line: Line, *, last: Leaf, line_length: int) -> bool:\n    \"\"\"See `can_omit_invisible_parens`.\"\"\"\n    length = 4 * line.depth\n    seen_other_brackets = False\n    for _index, leaf, leaf_length in line.enumerate_with_length():\n        length += leaf_length\n        if leaf is last.opening_bracket:\n            if seen_other_brackets or length <= line_length:\n                return True\n\n        elif leaf.type in OPENING_BRACKETS:\n            # There are brackets we can further split on.\n            seen_other_brackets = True\n\n    return False",
      "importString": "import itertools\nimport math\nfrom collections.abc import Callable, Iterator, Sequence\nfrom dataclasses import dataclass, field\nfrom typing import Optional, TypeVar, Union, cast\n\nfrom black.brackets import COMMA_PRIORITY, DOT_PRIORITY, BracketTracker\nfrom black.mode import Mode, Preview\nfrom black.nodes import (\nBRACKETS\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nTEST_DESCENDANTS\nchild_towards\nis_docstring\nis_import\nis_multiline_string\nis_one_sequence_between\nis_type_comment\nis_type_ignore_comment\nis_with_or_async_with_stmt\nmake_simple_prefix\nreplace_child\nsyms\nwhitespace\n)\nfrom black.strings import str_width\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/lines.py"
    },
    {
      "symbolName": "__contains__",
      "sourceCode": "def __contains__(self, feature: Preview) -> bool:\n        \"\"\"\n        Provide `Preview.FEATURE in Mode` syntax that mirrors the ``preview`` flag.\n\n        In unstable mode, all features are enabled. In preview mode, all features\n        except those in UNSTABLE_FEATURES are enabled. Any features in\n        `self.enabled_features` are also enabled.\n        \"\"\"\n        if self.unstable:\n            return True\n        if feature in self.enabled_features:\n            return True\n        return self.preview and feature not in UNSTABLE_FEATURES",
      "importString": "from dataclasses import dataclass, field\nfrom enum import Enum, auto\nfrom hashlib import sha256\nfrom operator import attrgetter\nfrom typing import Final\n\nfrom black.const import DEFAULT_LINE_LENGTH",
      "lineNum": 12,
      "relativeDocumentPath": "src/black/mode.py"
    },
    {
      "symbolName": "get_cache_key",
      "sourceCode": "def get_cache_key(self) -> str:\n        if self.target_versions:\n            version_str = \",\".join(\n                str(version.value)\n                for version in sorted(self.target_versions, key=attrgetter(\"value\"))\n            )\n        else:\n            version_str = \"-\"\n        if len(version_str) > _MAX_CACHE_KEY_PART_LENGTH:\n            version_str = sha256(version_str.encode()).hexdigest()[\n                :_MAX_CACHE_KEY_PART_LENGTH\n            ]\n        features_and_magics = (\n            \",\".join(sorted(f.name for f in self.enabled_features))\n            + \"@\"\n            + \",\".join(sorted(self.python_cell_magics))\n        )\n        if len(features_and_magics) > _MAX_CACHE_KEY_PART_LENGTH:\n            features_and_magics = sha256(features_and_magics.encode()).hexdigest()[\n                :_MAX_CACHE_KEY_PART_LENGTH\n            ]\n        parts = [\n            version_str,\n            str(self.line_length),\n            str(int(self.string_normalization)),\n            str(int(self.is_pyi)),\n            str(int(self.is_ipynb)),\n            str(int(self.skip_source_first_line)),\n            str(int(self.magic_trailing_comma)),\n            str(int(self.preview)),\n            str(int(self.unstable)),\n            features_and_magics,\n        ]\n        return \".\".join(parts)",
      "importString": "from dataclasses import dataclass, field\nfrom enum import Enum, auto\nfrom hashlib import sha256\nfrom operator import attrgetter\nfrom typing import Final\n\nfrom black.const import DEFAULT_LINE_LENGTH",
      "lineNum": 33,
      "relativeDocumentPath": "src/black/mode.py"
    },
    {
      "symbolName": "visit",
      "sourceCode": "def visit(self, node: LN) -> Iterator[T]:\n        \"\"\"Main method to visit `node` and its children.\n\n        It tries to find a `visit_*()` method for the given `node.type`, like\n        `visit_simple_stmt` for Node objects or `visit_INDENT` for Leaf objects.\n        If no dedicated `visit_*()` method is found, chooses `visit_default()`\n        instead.\n\n        Then yields objects of type `T` from the selected visitor.\n        \"\"\"\n        if node.type < 256:\n            name = token.tok_name[node.type]\n        else:\n            name = str(type_repr(node.type))\n        # We explicitly branch on whether a visitor exists (instead of\n        # using self.visit_default as the default arg to getattr) in order\n        # to save needing to create a bound method object and so mypyc can\n        # generate a native call to visit_default.\n        visitf = getattr(self, f\"visit_{name}\", None)\n        if visitf:\n            yield from visitf(node)\n        else:\n            yield from self.visit_default(node)",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 22,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "whitespace",
      "sourceCode": "def whitespace(leaf: Leaf, *, complex_subscript: bool, mode: Mode) -> str:  # noqa: C901\n    \"\"\"Return whitespace prefix if needed for the given `leaf`.\n\n    `complex_subscript` signals whether the given leaf is part of a subscription\n    which has non-trivial arguments, like arithmetic expressions or function calls.\n    \"\"\"\n    NO: Final[str] = \"\"\n    SPACE: Final[str] = \" \"\n    DOUBLESPACE: Final[str] = \"  \"\n    t = leaf.type\n    p = leaf.parent\n    v = leaf.value\n    if t in ALWAYS_NO_SPACE:\n        return NO\n\n    if t == token.COMMENT:\n        return DOUBLESPACE\n\n    assert p is not None, f\"INTERNAL ERROR: hand-made leaf without parent: {leaf!r}\"\n    if t == token.COLON and p.type not in {\n        syms.subscript,\n        syms.subscriptlist,\n        syms.sliceop,\n    }:\n        return NO\n\n    if t == token.LBRACE and p.type == syms.fstring_replacement_field:\n        return NO\n\n    prev = leaf.prev_sibling\n    if not prev:\n        prevp = preceding_leaf(p)\n        if not prevp or prevp.type in OPENING_BRACKETS:\n            return NO\n\n        if t == token.COLON:\n            if prevp.type == token.COLON:\n                return NO\n\n            elif prevp.type != token.COMMA and not complex_subscript:\n                return NO\n\n            return SPACE\n\n        if prevp.type == token.EQUAL:\n            if prevp.parent:\n                if prevp.parent.type in {\n                    syms.arglist,\n                    syms.argument,\n                    syms.parameters,\n                    syms.varargslist,\n                }:\n                    return NO\n\n                elif prevp.parent.type == syms.typedargslist:\n                    # A bit hacky: if the equal sign has whitespace, it means we\n                    # previously found it's a typed argument.  So, we're using\n                    # that, too.\n                    return prevp.prefix\n\n        elif (\n            prevp.type == token.STAR\n            and parent_type(prevp) == syms.star_expr\n            and (\n                parent_type(prevp.parent) == syms.subscriptlist\n                or (\n                    Preview.pep646_typed_star_arg_type_var_tuple in mode\n                    and parent_type(prevp.parent) == syms.tname_star\n                )\n            )\n        ):\n            # No space between typevar tuples or unpacking them.\n            return NO\n\n        elif prevp.type in VARARGS_SPECIALS:\n            if is_vararg(prevp, within=VARARGS_PARENTS | UNPACKING_PARENTS):\n                return NO\n\n        elif prevp.type == token.COLON:\n            if prevp.parent and prevp.parent.type in {syms.subscript, syms.sliceop}:\n                return SPACE if complex_subscript else NO\n\n        elif (\n            prevp.parent\n            and prevp.parent.type == syms.factor\n            and prevp.type in MATH_OPERATORS\n        ):\n            return NO\n\n        elif prevp.type == token.AT and p.parent and p.parent.type == syms.decorator:\n            # no space in decorators\n            return NO\n\n    elif prev.type in OPENING_BRACKETS:\n        return NO\n\n    elif prev.type == token.BANG:\n        return NO\n\n    if p.type in {syms.parameters, syms.arglist}:\n        # untyped function signatures or calls\n        if not prev or prev.type != token.COMMA:\n            return NO\n\n    elif p.type == syms.varargslist:\n        # lambdas\n        if prev and prev.type != token.COMMA:\n            return NO\n\n    elif p.type == syms.typedargslist:\n        # typed function signatures\n        if not prev:\n            return NO\n\n        if t == token.EQUAL:\n            if prev.type not in TYPED_NAMES:\n                return NO\n\n        elif prev.type == token.EQUAL:\n            # A bit hacky: if the equal sign has whitespace, it means we\n            # previously found it's a typed argument.  So, we're using that, too.\n            return prev.prefix\n\n        elif prev.type != token.COMMA:\n            return NO\n\n    elif p.type in TYPED_NAMES:\n        # type names\n        if not prev:\n            prevp = preceding_leaf(p)\n            if not prevp or prevp.type != token.COMMA:\n                return NO\n\n    elif p.type == syms.trailer:\n        # attributes and calls\n        if t == token.LPAR or t == token.RPAR:\n            return NO\n\n        if not prev:\n            if t == token.DOT or t == token.LSQB:\n                return NO\n\n        elif prev.type != token.COMMA:\n            return NO\n\n    elif p.type == syms.argument:\n        # single argument\n        if t == token.EQUAL:\n            return NO\n\n        if not prev:\n            prevp = preceding_leaf(p)\n            if not prevp or prevp.type == token.LPAR:\n                return NO\n\n        elif prev.type in {token.EQUAL} | VARARGS_SPECIALS:\n            return NO\n\n    elif p.type == syms.decorator:\n        # decorators\n        return NO\n\n    elif p.type == syms.dotted_name:\n        if prev:\n            return NO\n\n        prevp = preceding_leaf(p)\n        if not prevp or prevp.type == token.AT or prevp.type == token.DOT:\n            return NO\n\n    elif p.type == syms.classdef:\n        if t == token.LPAR:\n            return NO\n\n        if prev and prev.type == token.LPAR:\n            return NO\n\n    elif p.type in {syms.subscript, syms.sliceop}:\n        # indexing\n        if not prev:\n            assert p.parent is not None, \"subscripts are always parented\"\n            if p.parent.type == syms.subscriptlist:\n                return SPACE\n\n            return NO\n\n        elif t == token.COLONEQUAL or prev.type == token.COLONEQUAL:\n            return SPACE\n\n        elif not complex_subscript:\n            return NO\n\n    elif p.type == syms.atom:\n        if prev and t == token.DOT:\n            # dots, but not the first one.\n            return NO\n\n    elif p.type == syms.dictsetmaker:\n        # dict unpacking\n        if prev and prev.type == token.DOUBLESTAR:\n            return NO\n\n    elif p.type in {syms.factor, syms.star_expr}:\n        # unary ops\n        if not prev:\n            prevp = preceding_leaf(p)\n            if not prevp or prevp.type in OPENING_BRACKETS:\n                return NO\n\n            prevp_parent = prevp.parent\n            assert prevp_parent is not None\n            if prevp.type == token.COLON and prevp_parent.type in {\n                syms.subscript,\n                syms.sliceop,\n            }:\n                return NO\n\n            elif prevp.type == token.EQUAL and prevp_parent.type == syms.argument:\n                return NO\n\n        # TODO: add fstring here?\n        elif t in {token.NAME, token.NUMBER, token.STRING}:\n            return NO\n\n    elif p.type == syms.import_from:\n        if t == token.DOT:\n            if prev and prev.type == token.DOT:\n                return NO\n\n        elif t == token.NAME:\n            if v == \"import\":\n                return SPACE\n\n            if prev and prev.type == token.DOT:\n                return NO\n\n    elif p.type == syms.sliceop:\n        return NO\n\n    elif p.type == syms.except_clause:\n        if t == token.STAR:\n            return NO\n\n    return SPACE",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 243,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "preceding_leaf",
      "sourceCode": "def preceding_leaf(node: Optional[LN]) -> Optional[Leaf]:\n    \"\"\"Return the first leaf that precedes `node`, if any.\"\"\"\n    while node:\n        res = node.prev_sibling\n        if res:\n            if isinstance(res, Leaf):\n                return res\n\n            try:\n                return list(res.leaves())[-1]\n\n            except IndexError:\n                return None\n\n        node = node.parent\n    return None",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "prev_siblings_are",
      "sourceCode": "def prev_siblings_are(node: Optional[LN], tokens: list[Optional[NodeType]]) -> bool:\n    \"\"\"Return if the `node` and its previous siblings match types against the provided\n    list of tokens; the provided `node`has its type matched against the last element in\n    the list.  `None` can be used as the first element to declare that the start of the\n    list is anchored at the start of its parent's children.\"\"\"\n    if not tokens:\n        return True\n    if tokens[-1] is None:\n        return node is None\n    if not node:\n        return False\n    if node.type != tokens[-1]:\n        return False\n    return prev_siblings_are(node.prev_sibling, tokens[:-1])",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 13,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "parent_type",
      "sourceCode": "def parent_type(node: Optional[LN]) -> Optional[NodeType]:\n    \"\"\"\n    Returns:\n        @node.parent.type, if @node is not None and has a parent.\n            OR\n        None, otherwise.\n    \"\"\"\n    if node is None or node.parent is None:\n        return None\n\n    return node.parent.type",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 10,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "replace_child",
      "sourceCode": "def replace_child(old_child: LN, new_child: LN) -> None:\n    \"\"\"\n    Side Effects:\n        * If @old_child.parent is set, replace @old_child with @new_child in\n        @old_child's underlying Node structure.\n            OR\n        * Otherwise, this function does nothing.\n    \"\"\"\n    parent = old_child.parent\n    if not parent:\n        return\n\n    child_idx = old_child.remove()\n    if child_idx is not None:\n        parent.insert_child(child_idx, new_child)",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "container_of",
      "sourceCode": "def container_of(leaf: Leaf) -> LN:\n    \"\"\"Return `leaf` or one of its ancestors that is the topmost container of it.\n\n    By \"container\" we mean a node where `leaf` is the very first child.\n    \"\"\"\n    same_prefix = leaf.prefix\n    container: LN = leaf\n    while container:\n        parent = container.parent\n        if parent is None:\n            break\n\n        if parent.children[0].prefix != same_prefix:\n            break\n\n        if parent.type == syms.file_input:\n            break\n\n        if parent.prev_sibling is not None and parent.prev_sibling.type in BRACKETS:\n            break\n\n        container = parent\n    return container",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 22,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_docstring",
      "sourceCode": "def is_docstring(node: NL, mode: Mode) -> bool:\n    if isinstance(node, Leaf):\n        if node.type != token.STRING:\n            return False\n\n        prefix = get_string_prefix(node.value)\n        if set(prefix).intersection(\"bBfF\"):\n            return False\n\n    if (\n        Preview.unify_docstring_detection in mode\n        and node.parent\n        and node.parent.type == syms.simple_stmt\n        and not node.parent.prev_sibling\n        and node.parent.parent\n        and node.parent.parent.type == syms.file_input\n    ):\n        return True\n\n    if prev_siblings_are(\n        node.parent, [None, token.NEWLINE, token.INDENT, syms.simple_stmt]\n    ):\n        return True\n\n    # Multiline docstring on the same line as the `def`.\n    if prev_siblings_are(node.parent, [syms.parameters, token.COLON, syms.simple_stmt]):\n        # `syms.parameters` is only used in funcdefs and async_funcdefs in the Python\n        # grammar. We're safe to return True without further checks.\n        return True\n\n    return False",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 30,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_one_tuple",
      "sourceCode": "def is_one_tuple(node: LN) -> bool:\n    \"\"\"Return True if `node` holds a tuple with one element, with or without parens.\"\"\"\n    if node.type == syms.atom:\n        gexp = unwrap_singleton_parenthesis(node)\n        if gexp is None or gexp.type != syms.testlist_gexp:\n            return False\n\n        return len(gexp.children) == 2 and gexp.children[1].type == token.COMMA\n\n    return (\n        node.type in IMPLICIT_TUPLE\n        and len(node.children) == 2\n        and node.children[1].type == token.COMMA\n    )",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 13,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_one_sequence_between",
      "sourceCode": "def is_one_sequence_between(\n    opening: Leaf,\n    closing: Leaf,\n    leaves: list[Leaf],\n    brackets: tuple[int, int] = (token.LPAR, token.RPAR),\n) -> bool:\n    \"\"\"Return True if content between `opening` and `closing` is a one-sequence.\"\"\"\n    if (opening.type, closing.type) != brackets:\n        return False\n\n    depth = closing.bracket_depth + 1\n    for _opening_index, leaf in enumerate(leaves):\n        if leaf is opening:\n            break\n\n    else:\n        raise LookupError(\"Opening paren not found in `leaves`\")\n\n    commas = 0\n    _opening_index += 1\n    for leaf in leaves[_opening_index:]:\n        if leaf is closing:\n            break\n\n        bracket_depth = leaf.bracket_depth\n        if bracket_depth == depth and leaf.type == token.COMMA:\n            commas += 1\n            if leaf.parent and leaf.parent.type in {\n                syms.arglist,\n                syms.typedargslist,\n            }:\n                commas += 1\n                break\n\n    return commas < 2",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 34,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_simple_decorator_trailer",
      "sourceCode": "def is_simple_decorator_trailer(node: LN, last: bool = False) -> bool:\n    \"\"\"Return True iff `node` is a trailer valid in a simple decorator\"\"\"\n    return node.type == syms.trailer and (\n        (\n            len(node.children) == 2\n            and node.children[0].type == token.DOT\n            and node.children[1].type == token.NAME\n        )\n        # last trailer can be an argument-less parentheses pair\n        or (\n            last\n            and len(node.children) == 2\n            and node.children[0].type == token.LPAR\n            and node.children[1].type == token.RPAR\n        )\n        # last trailer can be arguments\n        or (\n            last\n            and len(node.children) == 3\n            and node.children[0].type == token.LPAR\n            # and node.children[1].type == syms.argument\n            and node.children[2].type == token.RPAR\n        )\n    )",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 23,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_simple_decorator_expression",
      "sourceCode": "def is_simple_decorator_expression(node: LN) -> bool:\n    \"\"\"Return True iff `node` could be a 'dotted name' decorator\n\n    This function takes the node of the 'namedexpr_test' of the new decorator\n    grammar and test if it would be valid under the old decorator grammar.\n\n    The old grammar was: decorator: @ dotted_name [arguments] NEWLINE\n    The new grammar is : decorator: @ namedexpr_test NEWLINE\n    \"\"\"\n    if node.type == token.NAME:\n        return True\n    if node.type == syms.power:\n        if node.children:\n            return (\n                node.children[0].type == token.NAME\n                and all(map(is_simple_decorator_trailer, node.children[1:-1]))\n                and (\n                    len(node.children) < 2\n                    or is_simple_decorator_trailer(node.children[-1], last=True)\n                )\n            )\n    return False",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 21,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_yield",
      "sourceCode": "def is_yield(node: LN) -> bool:\n    \"\"\"Return True if `node` holds a `yield` or `yield from` expression.\"\"\"\n    if node.type == syms.yield_expr:\n        return True\n\n    if is_name_token(node) and node.value == \"yield\":\n        return True\n\n    if node.type != syms.atom:\n        return False\n\n    if len(node.children) != 3:\n        return False\n\n    lpar, expr, rpar = node.children\n    if lpar.type == token.LPAR and rpar.type == token.RPAR:\n        return is_yield(expr)\n\n    return False",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 18,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_vararg",
      "sourceCode": "def is_vararg(leaf: Leaf, within: set[NodeType]) -> bool:\n    \"\"\"Return True if `leaf` is a star or double star in a vararg or kwarg.\n\n    If `within` includes VARARGS_PARENTS, this applies to function signatures.\n    If `within` includes UNPACKING_PARENTS, it applies to right hand-side\n    extended iterable unpacking (PEP 3132) and additional unpacking\n    generalizations (PEP 448).\n    \"\"\"\n    if leaf.type not in VARARGS_SPECIALS or not leaf.parent:\n        return False\n\n    p = leaf.parent\n    if p.type == syms.star_expr:\n        # Star expressions are also used as assignment targets in extended\n        # iterable unpacking (PEP 3132).  See what its parent is instead.\n        if not p.parent:\n            return False\n\n        p = p.parent\n\n    return p.type in within",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 20,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_stub_suite",
      "sourceCode": "def is_stub_suite(node: Node) -> bool:\n    \"\"\"Return True if `node` is a suite with a stub body.\"\"\"\n    if node.parent is not None and not is_parent_function_or_class(node):\n        return False\n\n    # If there is a comment, we want to keep it.\n    if node.prefix.strip():\n        return False\n\n    if (\n        len(node.children) != 4\n        or node.children[0].type != token.NEWLINE\n        or node.children[1].type != token.INDENT\n        or node.children[3].type != token.DEDENT\n    ):\n        return False\n\n    if node.children[3].prefix.strip():\n        return False\n\n    return is_stub_body(node.children[2])",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 20,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_stub_body",
      "sourceCode": "def is_stub_body(node: LN) -> bool:\n    \"\"\"Return True if `node` is a simple statement containing an ellipsis.\"\"\"\n    if not isinstance(node, Node) or node.type != syms.simple_stmt:\n        return False\n\n    if len(node.children) != 2:\n        return False\n\n    child = node.children[0]\n    return (\n        not child.prefix.strip()\n        and child.type == syms.atom\n        and len(child.children) == 3\n        and all(leaf == Leaf(token.DOT, \".\") for leaf in child.children)\n    )",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_atom_with_invisible_parens",
      "sourceCode": "def is_atom_with_invisible_parens(node: LN) -> bool:\n    \"\"\"Given a `LN`, determines whether it's an atom `node` with invisible\n    parens. Useful in dedupe-ing and normalizing parens.\n    \"\"\"\n    if isinstance(node, Leaf) or node.type != syms.atom:\n        return False\n\n    first, last = node.children[0], node.children[-1]\n    return (\n        isinstance(first, Leaf)\n        and first.type == token.LPAR\n        and first.value == \"\"\n        and isinstance(last, Leaf)\n        and last.type == token.RPAR\n        and last.value == \"\"\n    )",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_import",
      "sourceCode": "def is_import(leaf: Leaf) -> bool:\n    \"\"\"Return True if the given leaf starts an import statement.\"\"\"\n    p = leaf.parent\n    t = leaf.type\n    v = leaf.value\n    return bool(\n        t == token.NAME\n        and (\n            (v == \"import\" and p and p.type == syms.import_name)\n            or (v == \"from\" and p and p.type == syms.import_from)\n        )\n    )",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_with_or_async_with_stmt",
      "sourceCode": "def is_with_or_async_with_stmt(leaf: Leaf) -> bool:\n    \"\"\"Return True if the given leaf starts a with or async with statement.\"\"\"\n    return bool(\n        leaf.type == token.NAME\n        and leaf.value == \"with\"\n        and leaf.parent\n        and leaf.parent.type == syms.with_stmt\n    ) or bool(\n        leaf.type == token.ASYNC\n        and leaf.next_sibling\n        and leaf.next_sibling.type == syms.with_stmt\n    )",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "is_async_stmt_or_funcdef",
      "sourceCode": "def is_async_stmt_or_funcdef(leaf: Leaf) -> bool:\n    \"\"\"Return True if the given leaf starts an async def/for/with statement.\n\n    Note that `async def` can be either an `async_stmt` or `async_funcdef`,\n    the latter is used when it has decorators.\n    \"\"\"\n    return bool(\n        leaf.type == token.ASYNC\n        and leaf.parent\n        and leaf.parent.type in {syms.async_stmt, syms.async_funcdef}\n    )",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 10,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "wrap_in_parentheses",
      "sourceCode": "def wrap_in_parentheses(parent: Node, child: LN, *, visible: bool = True) -> None:\n    \"\"\"Wrap `child` in parentheses.\n\n    This replaces `child` with an atom holding the parentheses and the old\n    child.  That requires moving the prefix.\n\n    If `visible` is False, the leaves will be valueless (and thus invisible).\n    \"\"\"\n    lpar = Leaf(token.LPAR, \"(\" if visible else \"\")\n    rpar = Leaf(token.RPAR, \")\" if visible else \"\")\n    prefix = child.prefix\n    child.prefix = \"\"\n    index = child.remove() or 0\n    new_child = Node(syms.atom, [lpar, child, rpar])\n    new_child.prefix = prefix\n    parent.insert_child(index, new_child)",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "unwrap_singleton_parenthesis",
      "sourceCode": "def unwrap_singleton_parenthesis(node: LN) -> Optional[LN]:\n    \"\"\"Returns `wrapped` if `node` is of the shape ( wrapped ).\n\n    Parenthesis can be optional. Returns None otherwise\"\"\"\n    if len(node.children) != 3:\n        return None\n\n    lpar, wrapped, rpar = node.children\n    if not (lpar.type == token.LPAR and rpar.type == token.RPAR):\n        return None\n\n    return wrapped",
      "importString": "import sys\nfrom collections.abc import Iterator\nfrom typing import Final, Generic, Literal, Optional, TypeVar, Union\nfrom mypy_extensions import mypyc_attr\n\nfrom black.cache import CACHE_DIR\nfrom black.mode import Mode, Preview\nfrom black.strings import get_string_prefix, has_triple_quotes\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import NL, Leaf, Node, type_repr",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/nodes.py"
    },
    {
      "symbolName": "format_scientific_notation",
      "sourceCode": "def format_scientific_notation(text: str) -> str:\n    \"\"\"Formats a numeric string utilizing scientific notation\"\"\"\n    before, after = text.split(\"e\")\n    sign = \"\"\n    if after.startswith(\"-\"):\n        after = after[1:]\n        sign = \"-\"\n    elif after.startswith(\"+\"):\n        after = after[1:]\n    before = format_float_or_int_string(before)\n    return f\"{before}e{sign}{after}\"",
      "importString": "from blib2to3.pytree import Leaf",
      "lineNum": 10,
      "relativeDocumentPath": "src/black/numerics.py"
    },
    {
      "symbolName": "normalize_numeric_literal",
      "sourceCode": "def normalize_numeric_literal(leaf: Leaf) -> None:\n    \"\"\"Normalizes numeric (float, int, and complex) literals.\n\n    All letters used in the representation are normalized to lowercase.\"\"\"\n    text = leaf.value.lower()\n    if text.startswith((\"0o\", \"0b\")):\n        # Leave octal and binary literals alone.\n        pass\n    elif text.startswith(\"0x\"):\n        text = format_hex(text)\n    elif \"e\" in text:\n        text = format_scientific_notation(text)\n    elif text.endswith(\"j\"):\n        text = format_complex_number(text)\n    else:\n        text = format_float_or_int_string(text)\n    leaf.value = text",
      "importString": "from blib2to3.pytree import Leaf",
      "lineNum": 16,
      "relativeDocumentPath": "src/black/numerics.py"
    },
    {
      "symbolName": "ipynb_diff",
      "sourceCode": "def ipynb_diff(a: str, b: str, a_name: str, b_name: str) -> str:\n    \"\"\"Return a unified diff string between each cell in notebooks `a` and `b`.\"\"\"\n    a_nb = json.loads(a)\n    b_nb = json.loads(b)\n    diff_lines = [\n        diff(\n            \"\".join(a_nb[\"cells\"][cell_number][\"source\"]) + \"\\n\",\n            \"\".join(b_nb[\"cells\"][cell_number][\"source\"]) + \"\\n\",\n            f\"{a_name}:cell_{cell_number}\",\n            f\"{b_name}:cell_{cell_number}\",\n        )\n        for cell_number, cell in enumerate(a_nb[\"cells\"])\n        if cell[\"cell_type\"] == \"code\"\n    ]\n    return \"\".join(diff_lines)",
      "importString": "import json\nimport re\nimport tempfile\nfrom typing import Any, Optional\n\nfrom click import echo, style\nfrom mypy_extensions import mypyc_attr",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/output.py"
    },
    {
      "symbolName": "_splitlines_no_ff",
      "sourceCode": "def _splitlines_no_ff(source: str) -> list[str]:\n    \"\"\"Split a string into lines ignoring form feed and other chars.\n\n    This mimics how the Python parser splits source code.\n\n    A simplified version of the function with the same name in Lib/ast.py\n    \"\"\"\n    result = [match[0] for match in _line_pattern.finditer(source)]\n    if result[-1] == \"\":\n        result.pop(-1)\n    return result",
      "importString": "import json\nimport re\nimport tempfile\nfrom typing import Any, Optional\n\nfrom click import echo, style\nfrom mypy_extensions import mypyc_attr",
      "lineNum": 10,
      "relativeDocumentPath": "src/black/output.py"
    },
    {
      "symbolName": "diff",
      "sourceCode": "def diff(a: str, b: str, a_name: str, b_name: str) -> str:\n    \"\"\"Return a unified diff string between strings `a` and `b`.\"\"\"\n    import difflib\n\n    a_lines = _splitlines_no_ff(a)\n    b_lines = _splitlines_no_ff(b)\n    diff_lines = []\n    for line in difflib.unified_diff(\n        a_lines, b_lines, fromfile=a_name, tofile=b_name, n=5\n    ):\n        # Work around https://bugs.python.org/issue2142\n        # See:\n        # https://www.gnu.org/software/diffutils/manual/html_node/Incomplete-Lines.html\n        if line[-1] == \"\\n\":\n            diff_lines.append(line)\n        else:\n            diff_lines.append(line + \"\\n\")\n            diff_lines.append(\"\\\\ No newline at end of file\\n\")\n    return \"\".join(diff_lines)",
      "importString": "import json\nimport re\nimport tempfile\nfrom typing import Any, Optional\n\nfrom click import echo, style\nfrom mypy_extensions import mypyc_attr",
      "lineNum": 18,
      "relativeDocumentPath": "src/black/output.py"
    },
    {
      "symbolName": "color_diff",
      "sourceCode": "def color_diff(contents: str) -> str:\n    \"\"\"Inject the ANSI color codes to the diff.\"\"\"\n    lines = contents.split(\"\\n\")\n    for i, line in enumerate(lines):\n        if line.startswith(\"+++\") or line.startswith(\"---\"):\n            line = \"\\033[1m\" + line + \"\\033[0m\"  # bold, reset\n        elif line.startswith(\"@@\"):\n            line = \"\\033[36m\" + line + \"\\033[0m\"  # cyan, reset\n        elif line.startswith(\"+\"):\n            line = \"\\033[32m\" + line + \"\\033[0m\"  # green, reset\n        elif line.startswith(\"-\"):\n            line = \"\\033[31m\" + line + \"\\033[0m\"  # red, reset\n        lines[i] = line\n    return \"\\n\".join(lines)",
      "importString": "import json\nimport re\nimport tempfile\nfrom typing import Any, Optional\n\nfrom click import echo, style\nfrom mypy_extensions import mypyc_attr",
      "lineNum": 13,
      "relativeDocumentPath": "src/black/output.py"
    },
    {
      "symbolName": "dump_to_file",
      "sourceCode": "@mypyc_attr(patchable=True)\ndef dump_to_file(*output: str, ensure_final_newline: bool = True) -> str:\n    \"\"\"Dump `output` to a temporary file. Return path to the file.\"\"\"\n    with tempfile.NamedTemporaryFile(\n        mode=\"w\", prefix=\"blk_\", suffix=\".log\", delete=False, encoding=\"utf8\"\n    ) as f:\n        for lines in output:\n            f.write(lines)\n            if ensure_final_newline and lines and lines[-1] != \"\\n\":\n                f.write(\"\\n\")\n    return f.name",
      "importString": "import json\nimport re\nimport tempfile\nfrom typing import Any, Optional\n\nfrom click import echo, style\nfrom mypy_extensions import mypyc_attr",
      "lineNum": 10,
      "relativeDocumentPath": "src/black/output.py"
    },
    {
      "symbolName": "get_grammars",
      "sourceCode": "def get_grammars(target_versions: set[TargetVersion]) -> list[Grammar]:\n    if not target_versions:\n        # No target_version specified, so try all grammars.\n        return [\n            # Python 3.7-3.9\n            pygram.python_grammar_async_keywords,\n            # Python 3.0-3.6\n            pygram.python_grammar,\n            # Python 3.10+\n            pygram.python_grammar_soft_keywords,\n        ]\n\n    grammars = []\n    # If we have to parse both, try to parse async as a keyword first\n    if not supports_feature(\n        target_versions, Feature.ASYNC_IDENTIFIERS\n    ) and not supports_feature(target_versions, Feature.PATTERN_MATCHING):\n        # Python 3.7-3.9\n        grammars.append(pygram.python_grammar_async_keywords)\n    if not supports_feature(target_versions, Feature.ASYNC_KEYWORDS):\n        # Python 3.0-3.6\n        grammars.append(pygram.python_grammar)\n    if any(Feature.PATTERN_MATCHING in VERSION_TO_FEATURES[v] for v in target_versions):\n        # Python 3.10+\n        grammars.append(pygram.python_grammar_soft_keywords)\n\n    # At least one of the above branches must have been taken, because every Python\n    # version has exactly one of the two 'ASYNC_*' flags\n    return grammars",
      "importString": "import sys\nimport warnings\nfrom collections.abc import Collection, Iterator\n\nfrom black.mode import VERSION_TO_FEATURES, Feature, TargetVersion, supports_feature\nfrom black.nodes import syms\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import driver\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.parse import ParseError\nfrom blib2to3.pgen2.tokenize import TokenError\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 28,
      "relativeDocumentPath": "src/black/parsing.py"
    },
    {
      "symbolName": "lib2to3_parse",
      "sourceCode": "def lib2to3_parse(\n    src_txt: str, target_versions: Collection[TargetVersion] = ()\n) -> Node:\n    \"\"\"Given a string with source, return the lib2to3 Node.\"\"\"\n    if not src_txt.endswith(\"\\n\"):\n        src_txt += \"\\n\"\n\n    grammars = get_grammars(set(target_versions))\n    if target_versions:\n        max_tv = max(target_versions, key=lambda tv: tv.value)\n        tv_str = f\" for target version {max_tv.pretty()}\"\n    else:\n        tv_str = \"\"\n\n    errors = {}\n    for grammar in grammars:\n        drv = driver.Driver(grammar)\n        try:\n            result = drv.parse_string(src_txt, True)\n            break\n\n        except ParseError as pe:\n            lineno, column = pe.context[1]\n            lines = src_txt.splitlines()\n            try:\n                faulty_line = lines[lineno - 1]\n            except IndexError:\n                faulty_line = \"<line number missing in source>\"\n            errors[grammar.version] = InvalidInput(\n                f\"Cannot parse{tv_str}: {lineno}:{column}: {faulty_line}\"\n            )\n\n        except TokenError as te:\n            # In edge cases these are raised; and typically don't have a \"faulty_line\".\n            lineno, column = te.args[1]\n            errors[grammar.version] = InvalidInput(\n                f\"Cannot parse{tv_str}: {lineno}:{column}: {te.args[0]}\"\n            )\n\n    else:\n        # Choose the latest version when raising the actual parsing error.\n        assert len(errors) >= 1\n        exc = errors[max(errors)]\n        raise exc from None\n\n    if isinstance(result, Leaf):\n        result = Node(syms.file_input, [result])\n    return result",
      "importString": "import sys\nimport warnings\nfrom collections.abc import Collection, Iterator\n\nfrom black.mode import VERSION_TO_FEATURES, Feature, TargetVersion, supports_feature\nfrom black.nodes import syms\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import driver\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.parse import ParseError\nfrom blib2to3.pgen2.tokenize import TokenError\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 47,
      "relativeDocumentPath": "src/black/parsing.py"
    },
    {
      "symbolName": "parse_ast",
      "sourceCode": "def parse_ast(src: str) -> ast.AST:\n    # TODO: support Python 4+ ;)\n    versions = [(3, minor) for minor in range(3, sys.version_info[1] + 1)]\n\n    first_error = \"\"\n    for version in sorted(versions, reverse=True):\n        try:\n            return _parse_single_version(src, version, type_comments=True)\n        except SyntaxError as e:\n            if not first_error:\n                first_error = str(e)\n\n    # Try to parse without type comments\n    for version in sorted(versions, reverse=True):\n        try:\n            return _parse_single_version(src, version, type_comments=False)\n        except SyntaxError:\n            pass\n\n    raise SyntaxError(first_error)",
      "importString": "import sys\nimport warnings\nfrom collections.abc import Collection, Iterator\n\nfrom black.mode import VERSION_TO_FEATURES, Feature, TargetVersion, supports_feature\nfrom black.nodes import syms\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import driver\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.parse import ParseError\nfrom blib2to3.pgen2.tokenize import TokenError\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 19,
      "relativeDocumentPath": "src/black/parsing.py"
    },
    {
      "symbolName": "_stringify_ast",
      "sourceCode": "def _stringify_ast(node: ast.AST, parent_stack: list[ast.AST]) -> Iterator[str]:\n    if (\n        isinstance(node, ast.Constant)\n        and isinstance(node.value, str)\n        and node.kind == \"u\"\n    ):\n        # It's a quirk of history that we strip the u prefix over here. We used to\n        # rewrite the AST nodes for Python version compatibility and we never copied\n        # over the kind\n        node.kind = None\n\n    yield f\"{'    ' * len(parent_stack)}{node.__class__.__name__}(\"\n\n    for field in sorted(node._fields):  # noqa: F402\n        # TypeIgnore has only one field 'lineno' which breaks this comparison\n        if isinstance(node, ast.TypeIgnore):\n            break\n\n        try:\n            value: object = getattr(node, field)\n        except AttributeError:\n            continue\n\n        yield f\"{'    ' * (len(parent_stack) + 1)}{field}=\"\n\n        if isinstance(value, list):\n            for item in value:\n                # Ignore nested tuples within del statements, because we may insert\n                # parentheses and they change the AST.\n                if (\n                    field == \"targets\"\n                    and isinstance(node, ast.Delete)\n                    and isinstance(item, ast.Tuple)\n                ):\n                    for elt in item.elts:\n                        yield from _stringify_ast_with_new_parent(\n                            elt, parent_stack, node\n                        )\n\n                elif isinstance(item, ast.AST):\n                    yield from _stringify_ast_with_new_parent(item, parent_stack, node)\n\n        elif isinstance(value, ast.AST):\n            yield from _stringify_ast_with_new_parent(value, parent_stack, node)\n\n        else:\n            normalized: object\n            if (\n                isinstance(node, ast.Constant)\n                and field == \"value\"\n                and isinstance(value, str)\n                and len(parent_stack) >= 2\n                # Any standalone string, ideally this would\n                # exactly match black.nodes.is_docstring\n                and isinstance(parent_stack[-1], ast.Expr)\n            ):\n                # Constant strings may be indented across newlines, if they are\n                # docstrings; fold spaces after newlines when comparing. Similarly,\n                # trailing and leading space may be removed.\n                normalized = _normalize(\"\\n\", value)\n            elif field == \"type_comment\" and isinstance(value, str):\n                # Trailing whitespace in type comments is removed.\n                normalized = value.rstrip()\n            else:\n                normalized = value\n            yield (\n                f\"{'    ' * (len(parent_stack) + 1)}{normalized!r},  #\"\n                f\" {value.__class__.__name__}\"\n            )\n\n    yield f\"{'    ' * len(parent_stack)})  # /{node.__class__.__name__}\"",
      "importString": "import sys\nimport warnings\nfrom collections.abc import Collection, Iterator\n\nfrom black.mode import VERSION_TO_FEATURES, Feature, TargetVersion, supports_feature\nfrom black.nodes import syms\nfrom blib2to3 import pygram\nfrom blib2to3.pgen2 import driver\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.parse import ParseError\nfrom blib2to3.pgen2.tokenize import TokenError\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 70,
      "relativeDocumentPath": "src/black/parsing.py"
    },
    {
      "symbolName": "parse_line_ranges",
      "sourceCode": "def parse_line_ranges(line_ranges: Sequence[str]) -> list[tuple[int, int]]:\n    lines: list[tuple[int, int]] = []\n    for lines_str in line_ranges:\n        parts = lines_str.split(\"-\")\n        if len(parts) != 2:\n            raise ValueError(\n                \"Incorrect --line-ranges format, expect 'START-END', found\"\n                f\" {lines_str!r}\"\n            )\n        try:\n            start = int(parts[0])\n            end = int(parts[1])\n        except ValueError:\n            raise ValueError(\n                \"Incorrect --line-ranges value, expect integer ranges, found\"\n                f\" {lines_str!r}\"\n            ) from None\n        else:\n            lines.append((start, end))\n    return lines",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 19,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "sanitized_lines",
      "sourceCode": "def sanitized_lines(\n    lines: Collection[tuple[int, int]], src_contents: str\n) -> Collection[tuple[int, int]]:\n    \"\"\"Returns the valid line ranges for the given source.\n\n    This removes ranges that are entirely outside the valid lines.\n\n    Other ranges are normalized so that the start values are at least 1 and the\n    end values are at most the (1-based) index of the last source line.\n    \"\"\"\n    if not src_contents:\n        return []\n    good_lines = []\n    src_line_count = src_contents.count(\"\\n\")\n    if not src_contents.endswith(\"\\n\"):\n        src_line_count += 1\n    for start, end in lines:\n        if start > src_line_count:\n            continue\n        # line-ranges are 1-based\n        start = max(start, 1)\n        if end < start:\n            continue\n        end = min(end, src_line_count)\n        good_lines.append((start, end))\n    return good_lines",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 25,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "adjusted_lines",
      "sourceCode": "def adjusted_lines(\n    lines: Collection[tuple[int, int]],\n    original_source: str,\n    modified_source: str,\n) -> list[tuple[int, int]]:\n    \"\"\"Returns the adjusted line ranges based on edits from the original code.\n\n    This computes the new line ranges by diffing original_source and\n    modified_source, and adjust each range based on how the range overlaps with\n    the diffs.\n\n    Note the diff can contain lines outside of the original line ranges. This can\n    happen when the formatting has to be done in adjacent to maintain consistent\n    local results. For example:\n\n    1. def my_func(arg1, arg2,\n    2.             arg3,):\n    3.   pass\n\n    If it restricts to line 2-2, it can't simply reformat line 2, it also has\n    to reformat line 1:\n\n    1. def my_func(\n    2.     arg1,\n    3.     arg2,\n    4.     arg3,\n    5. ):\n    6.   pass\n\n    In this case, we will expand the line ranges to also include the whole diff\n    block.\n\n    Args:\n      lines: a collection of line ranges.\n      original_source: the original source.\n      modified_source: the modified source.\n    \"\"\"\n    lines_mappings = _calculate_lines_mappings(original_source, modified_source)\n\n    new_lines = []\n    # Keep an index of the current search. Since the lines and lines_mappings are\n    # sorted, this makes the search complexity linear.\n    current_mapping_index = 0\n    for start, end in sorted(lines):\n        start_mapping_index = _find_lines_mapping_index(\n            start,\n            lines_mappings,\n            current_mapping_index,\n        )\n        end_mapping_index = _find_lines_mapping_index(\n            end,\n            lines_mappings,\n            start_mapping_index,\n        )\n        current_mapping_index = start_mapping_index\n        if start_mapping_index >= len(lines_mappings) or end_mapping_index >= len(\n            lines_mappings\n        ):\n            # Protect against invalid inputs.\n            continue\n        start_mapping = lines_mappings[start_mapping_index]\n        end_mapping = lines_mappings[end_mapping_index]\n        if start_mapping.is_changed_block:\n            # When the line falls into a changed block, expands to the whole block.\n            new_start = start_mapping.modified_start\n        else:\n            new_start = (\n                start - start_mapping.original_start + start_mapping.modified_start\n            )\n        if end_mapping.is_changed_block:\n            # When the line falls into a changed block, expands to the whole block.\n            new_end = end_mapping.modified_end\n        else:\n            new_end = end - end_mapping.original_start + end_mapping.modified_start\n        new_range = (new_start, new_end)\n        if is_valid_line_range(new_range):\n            new_lines.append(new_range)\n    return new_lines",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 77,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "convert_unchanged_lines",
      "sourceCode": "def convert_unchanged_lines(src_node: Node, lines: Collection[tuple[int, int]]) -> None:\n    \"\"\"Converts unchanged lines to STANDALONE_COMMENT.\n\n    The idea is similar to how `# fmt: on/off` is implemented. It also converts the\n    nodes between those markers as a single `STANDALONE_COMMENT` leaf node with\n    the unformatted code as its value. `STANDALONE_COMMENT` is a \"fake\" token\n    that will be formatted as-is with its prefix normalized.\n\n    Here we perform two passes:\n\n    1. Visit the top-level statements, and convert them to a single\n       `STANDALONE_COMMENT` when unchanged. This speeds up formatting when some\n       of the top-level statements aren't changed.\n    2. Convert unchanged \"unwrapped lines\" to `STANDALONE_COMMENT` nodes line by\n       line. \"unwrapped lines\" are divided by the `NEWLINE` token. e.g. a\n       multi-line statement is *one* \"unwrapped line\" that ends with `NEWLINE`,\n       even though this statement itself can span multiple lines, and the\n       tokenizer only sees the last '\\n' as the `NEWLINE` token.\n\n    NOTE: During pass (2), comment prefixes and indentations are ALWAYS\n    normalized even when the lines aren't changed. This is fixable by moving\n    more formatting to pass (1). However, it's hard to get it correct when\n    incorrect indentations are used. So we defer this to future optimizations.\n    \"\"\"\n    lines_set: set[int] = set()\n    for start, end in lines:\n        lines_set.update(range(start, end + 1))\n    visitor = _TopLevelStatementsVisitor(lines_set)\n    _ = list(visitor.visit(src_node))  # Consume all results.\n    _convert_unchanged_line_by_line(src_node, lines_set)",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 29,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "visit_simple_stmt",
      "sourceCode": "def visit_simple_stmt(self, node: Node) -> Iterator[None]:\n        # This is only called for top-level statements, since `visit_suite`\n        # won't visit its children nodes.\n        yield from []\n        newline_leaf = last_leaf(node)\n        if not newline_leaf:\n            return\n        assert (\n            newline_leaf.type == NEWLINE\n        ), f\"Unexpectedly found leaf.type={newline_leaf.type}\"\n        # We need to find the furthest ancestor with the NEWLINE as the last\n        # leaf, since a `suite` can simply be a `simple_stmt` when it puts\n        # its body on the same line. Example: `if cond: pass`.\n        ancestor = furthest_ancestor_with_last_leaf(newline_leaf)\n        if not _get_line_range(ancestor).intersection(self._lines_set):\n            _convert_node_to_standalone_comment(ancestor)",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "visit_suite",
      "sourceCode": "def visit_suite(self, node: Node) -> Iterator[None]:\n        yield from []\n        # If there is a STANDALONE_COMMENT node, it means parts of the node tree\n        # have fmt on/off/skip markers. Those STANDALONE_COMMENT nodes can't\n        # be simply converted by calling str(node). So we just don't convert\n        # here.\n        if _contains_standalone_comment(node):\n            return\n        # Find the semantic parent of this suite. For `async_stmt` and\n        # `async_funcdef`, the ASYNC token is defined on a separate level by the\n        # grammar.\n        semantic_parent = node.parent\n        if semantic_parent is not None:\n            if (\n                semantic_parent.prev_sibling is not None\n                and semantic_parent.prev_sibling.type == ASYNC\n            ):\n                semantic_parent = semantic_parent.parent\n        if semantic_parent is not None and not _get_line_range(\n            semantic_parent\n        ).intersection(self._lines_set):\n            _convert_node_to_standalone_comment(semantic_parent)",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 21,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "_convert_unchanged_line_by_line",
      "sourceCode": "def _convert_unchanged_line_by_line(node: Node, lines_set: set[int]) -> None:\n    \"\"\"Converts unchanged to STANDALONE_COMMENT line by line.\"\"\"\n    for leaf in node.leaves():\n        if leaf.type != NEWLINE:\n            # We only consider \"unwrapped lines\", which are divided by the NEWLINE\n            # token.\n            continue\n        if leaf.parent and leaf.parent.type == syms.match_stmt:\n            # The `suite` node is defined as:\n            #   match_stmt: \"match\" subject_expr ':' NEWLINE INDENT case_block+ DEDENT\n            # Here we need to check `subject_expr`. The `case_block+` will be\n            # checked by their own NEWLINEs.\n            nodes_to_ignore: list[LN] = []\n            prev_sibling = leaf.prev_sibling\n            while prev_sibling:\n                nodes_to_ignore.insert(0, prev_sibling)\n                prev_sibling = prev_sibling.prev_sibling\n            if not _get_line_range(nodes_to_ignore).intersection(lines_set):\n                _convert_nodes_to_standalone_comment(nodes_to_ignore, newline=leaf)\n        elif leaf.parent and leaf.parent.type == syms.suite:\n            # The `suite` node is defined as:\n            #   suite: simple_stmt | NEWLINE INDENT stmt+ DEDENT\n            # We will check `simple_stmt` and `stmt+` separately against the lines set\n            parent_sibling = leaf.parent.prev_sibling\n            nodes_to_ignore = []\n            while parent_sibling and not parent_sibling.type == syms.suite:\n                # NOTE: Multiple suite nodes can exist as siblings in e.g. `if_stmt`.\n                nodes_to_ignore.insert(0, parent_sibling)\n                parent_sibling = parent_sibling.prev_sibling\n            # Special case for `async_stmt` and `async_funcdef` where the ASYNC\n            # token is on the grandparent node.\n            grandparent = leaf.parent.parent\n            if (\n                grandparent is not None\n                and grandparent.prev_sibling is not None\n                and grandparent.prev_sibling.type == ASYNC\n            ):\n                nodes_to_ignore.insert(0, grandparent.prev_sibling)\n            if not _get_line_range(nodes_to_ignore).intersection(lines_set):\n                _convert_nodes_to_standalone_comment(nodes_to_ignore, newline=leaf)\n        else:\n            ancestor = furthest_ancestor_with_last_leaf(leaf)\n            # Consider multiple decorators as a whole block, as their\n            # newlines have different behaviors than the rest of the grammar.\n            if (\n                ancestor.type == syms.decorator\n                and ancestor.parent\n                and ancestor.parent.type == syms.decorators\n            ):\n                ancestor = ancestor.parent\n            if not _get_line_range(ancestor).intersection(lines_set):\n                _convert_node_to_standalone_comment(ancestor)",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 51,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "_convert_node_to_standalone_comment",
      "sourceCode": "def _convert_node_to_standalone_comment(node: LN) -> None:\n    \"\"\"Convert node to STANDALONE_COMMENT by modifying the tree inline.\"\"\"\n    parent = node.parent\n    if not parent:\n        return\n    first = first_leaf(node)\n    last = last_leaf(node)\n    if not first or not last:\n        return\n    if first is last:\n        # This can happen on the following edge cases:\n        # 1. A block of `# fmt: off/on` code except the `# fmt: on` is placed\n        #    on the end of the last line instead of on a new line.\n        # 2. A single backslash on its own line followed by a comment line.\n        # Ideally we don't want to format them when not requested, but fixing\n        # isn't easy. These cases are also badly formatted code, so it isn't\n        # too bad we reformat them.\n        return\n    # The prefix contains comments and indentation whitespaces. They are\n    # reformatted accordingly to the correct indentation level.\n    # This also means the indentation will be changed on the unchanged lines, and\n    # this is actually required to not break incremental reformatting.\n    prefix = first.prefix\n    first.prefix = \"\"\n    index = node.remove()\n    if index is not None:\n        # Remove the '\\n', as STANDALONE_COMMENT will have '\\n' appended when\n        # generating the formatted code.\n        value = str(node)[:-1]\n        parent.insert_child(\n            index,\n            Leaf(\n                STANDALONE_COMMENT,\n                value,\n                prefix=prefix,\n                fmt_pass_converted_first_leaf=first,\n            ),\n        )",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 37,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "_convert_nodes_to_standalone_comment",
      "sourceCode": "def _convert_nodes_to_standalone_comment(nodes: Sequence[LN], *, newline: Leaf) -> None:\n    \"\"\"Convert nodes to STANDALONE_COMMENT by modifying the tree inline.\"\"\"\n    if not nodes:\n        return\n    parent = nodes[0].parent\n    first = first_leaf(nodes[0])\n    if not parent or not first:\n        return\n    prefix = first.prefix\n    first.prefix = \"\"\n    value = \"\".join(str(node) for node in nodes)\n    # The prefix comment on the NEWLINE leaf is the trailing comment of the statement.\n    if newline.prefix:\n        value += newline.prefix\n        newline.prefix = \"\"\n    index = nodes[0].remove()\n    for node in nodes[1:]:\n        node.remove()\n    if index is not None:\n        parent.insert_child(\n            index,\n            Leaf(\n                STANDALONE_COMMENT,\n                value,\n                prefix=prefix,\n                fmt_pass_converted_first_leaf=first,\n            ),\n        )",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 27,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "_get_line_range",
      "sourceCode": "def _get_line_range(node_or_nodes: Union[LN, list[LN]]) -> set[int]:\n    \"\"\"Returns the line range of this node or list of nodes.\"\"\"\n    if isinstance(node_or_nodes, list):\n        nodes = node_or_nodes\n        if not nodes:\n            return set()\n        first = first_leaf(nodes[0])\n        last = last_leaf(nodes[-1])\n        if first and last:\n            line_start = first.lineno\n            line_end = _leaf_line_end(last)\n            return set(range(line_start, line_end + 1))\n        else:\n            return set()\n    else:\n        node = node_or_nodes\n        if isinstance(node, Leaf):\n            return set(range(node.lineno, _leaf_line_end(node) + 1))\n        else:\n            first = first_leaf(node)\n            last = last_leaf(node)\n            if first and last:\n                return set(range(first.lineno, _leaf_line_end(last) + 1))\n            else:\n                return set()",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 24,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "_calculate_lines_mappings",
      "sourceCode": "def _calculate_lines_mappings(\n    original_source: str,\n    modified_source: str,\n) -> Sequence[_LinesMapping]:\n    \"\"\"Returns a sequence of _LinesMapping by diffing the sources.\n\n    For example, given the following diff:\n        import re\n      - def func(arg1,\n      -   arg2, arg3):\n      + def func(arg1, arg2, arg3):\n          pass\n    It returns the following mappings:\n      original -> modified\n       (1, 1)  ->  (1, 1), is_changed_block=False (the \"import re\" line)\n       (2, 3)  ->  (2, 2), is_changed_block=True (the diff)\n       (4, 4)  ->  (3, 3), is_changed_block=False (the \"pass\" line)\n\n    You can think of this visually as if it brings up a side-by-side diff, and tries\n    to map the line ranges from the left side to the right side:\n\n      (1, 1)->(1, 1)    1. import re          1. import re\n      (2, 3)->(2, 2)    2. def func(arg1,     2. def func(arg1, arg2, arg3):\n                        3.   arg2, arg3):\n      (4, 4)->(3, 3)    4.   pass             3.   pass\n\n    Args:\n      original_source: the original source.\n      modified_source: the modified source.\n    \"\"\"\n    matcher = difflib.SequenceMatcher(\n        None,\n        original_source.splitlines(keepends=True),\n        modified_source.splitlines(keepends=True),\n    )\n    matching_blocks = matcher.get_matching_blocks()\n    lines_mappings: list[_LinesMapping] = []\n    # matching_blocks is a sequence of \"same block of code ranges\", see\n    # https://docs.python.org/3/library/difflib.html#difflib.SequenceMatcher.get_matching_blocks\n    # Each block corresponds to a _LinesMapping with is_changed_block=False,\n    # and the ranges between two blocks corresponds to a _LinesMapping with\n    # is_changed_block=True,\n    # NOTE: matching_blocks is 0-based, but _LinesMapping is 1-based.\n    for i, block in enumerate(matching_blocks):\n        if i == 0:\n            if block.a != 0 or block.b != 0:\n                lines_mappings.append(\n                    _LinesMapping(\n                        original_start=1,\n                        original_end=block.a,\n                        modified_start=1,\n                        modified_end=block.b,\n                        is_changed_block=False,\n                    )\n                )\n        else:\n            previous_block = matching_blocks[i - 1]\n            lines_mappings.append(\n                _LinesMapping(\n                    original_start=previous_block.a + previous_block.size + 1,\n                    original_end=block.a,\n                    modified_start=previous_block.b + previous_block.size + 1,\n                    modified_end=block.b,\n                    is_changed_block=True,\n                )\n            )\n        if i < len(matching_blocks) - 1:\n            lines_mappings.append(\n                _LinesMapping(\n                    original_start=block.a + 1,\n                    original_end=block.a + block.size,\n                    modified_start=block.b + 1,\n                    modified_end=block.b + block.size,\n                    is_changed_block=False,\n                )\n            )\n    return lines_mappings",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 76,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "_find_lines_mapping_index",
      "sourceCode": "def _find_lines_mapping_index(\n    original_line: int,\n    lines_mappings: Sequence[_LinesMapping],\n    start_index: int,\n) -> int:\n    \"\"\"Returns the original index of the lines mappings for the original line.\"\"\"\n    index = start_index\n    while index < len(lines_mappings):\n        mapping = lines_mappings[index]\n        if mapping.original_start <= original_line <= mapping.original_end:\n            return index\n        index += 1\n    return index",
      "importString": "import difflib\nfrom collections.abc import Collection, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Union\nfrom black.nodes import (\nLN\nSTANDALONE_COMMENT\nLeaf\nNode\nVisitor\nfirst_leaf\nfurthest_ancestor_with_last_leaf\nlast_leaf\nsyms\n)\nfrom blib2to3.pgen2.token import ASYNC, NEWLINE",
      "lineNum": 12,
      "relativeDocumentPath": "src/black/ranges.py"
    },
    {
      "symbolName": "done",
      "sourceCode": "def done(self, src: Path, changed: Changed) -> None:\n        \"\"\"Increment the counter for successful reformatting. Write out a message.\"\"\"\n        if changed is Changed.YES:\n            reformatted = \"would reformat\" if self.check or self.diff else \"reformatted\"\n            if self.verbose or not self.quiet:\n                out(f\"{reformatted} {src}\")\n            self.change_count += 1\n        else:\n            if self.verbose:\n                if changed is Changed.NO:\n                    msg = f\"{src} already well formatted, good job.\"\n                else:\n                    msg = f\"{src} wasn't modified on disk since last run.\"\n                out(msg, bold=False)\n            self.same_count += 1",
      "importString": "from dataclasses import dataclass\nfrom enum import Enum\nfrom pathlib import Path\n\nfrom click import style\n\nfrom black.output import err, out",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/report.py"
    },
    {
      "symbolName": "return_code",
      "sourceCode": "@property\n    def return_code(self) -> int:\n        \"\"\"Return the exit code that the app should use.\n\n        This considers the current state of changed files and failures:\n        - if there were any failures, return 123;\n        - if any files were changed and --check is being used, return 1;\n        - otherwise return 0.\n        \"\"\"\n        # According to http://tldp.org/LDP/abs/html/exitcodes.html starting with\n        # 126 we have special return codes reserved by the shell.\n        if self.failure_count:\n            return 123\n\n        elif self.change_count and self.check:\n            return 1\n\n        return 0",
      "importString": "from dataclasses import dataclass\nfrom enum import Enum\nfrom pathlib import Path\n\nfrom click import style\n\nfrom black.output import err, out",
      "lineNum": 17,
      "relativeDocumentPath": "src/black/report.py"
    },
    {
      "symbolName": "__str__",
      "sourceCode": "def __str__(self) -> str:\n        \"\"\"Render a color report of the current state.\n\n        Use `click.unstyle` to remove colors.\n        \"\"\"\n        if self.check or self.diff:\n            reformatted = \"would be reformatted\"\n            unchanged = \"would be left unchanged\"\n            failed = \"would fail to reformat\"\n        else:\n            reformatted = \"reformatted\"\n            unchanged = \"left unchanged\"\n            failed = \"failed to reformat\"\n        report = []\n        if self.change_count:\n            s = \"s\" if self.change_count > 1 else \"\"\n            report.append(\n                style(f\"{self.change_count} file{s} \", bold=True, fg=\"blue\")\n                + style(f\"{reformatted}\", bold=True)\n            )\n\n        if self.same_count:\n            s = \"s\" if self.same_count > 1 else \"\"\n            report.append(style(f\"{self.same_count} file{s} \", fg=\"blue\") + unchanged)\n        if self.failure_count:\n            s = \"s\" if self.failure_count > 1 else \"\"\n            report.append(style(f\"{self.failure_count} file{s} {failed}\", fg=\"red\"))\n        return \", \".join(report) + \".\"",
      "importString": "from dataclasses import dataclass\nfrom enum import Enum\nfrom pathlib import Path\n\nfrom click import style\n\nfrom black.output import err, out",
      "lineNum": 27,
      "relativeDocumentPath": "src/black/report.py"
    },
    {
      "symbolName": "lines_with_leading_tabs_expanded",
      "sourceCode": "def lines_with_leading_tabs_expanded(s: str) -> list[str]:\n    \"\"\"\n    Splits string into lines and expands only leading tabs (following the normal\n    Python rules)\n    \"\"\"\n    lines = []\n    for line in s.splitlines():\n        stripped_line = line.lstrip()\n        if not stripped_line or stripped_line == line:\n            lines.append(line)\n        else:\n            prefix_length = len(line) - len(stripped_line)\n            prefix = line[:prefix_length].expandtabs()\n            lines.append(prefix + stripped_line)\n    if s.endswith(\"\\n\"):\n        lines.append(\"\")\n    return lines",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 16,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "fix_multiline_docstring",
      "sourceCode": "def fix_multiline_docstring(docstring: str, prefix: str) -> str:\n    # https://www.python.org/dev/peps/pep-0257/#handling-docstring-indentation\n    assert docstring, \"INTERNAL ERROR: Multiline docstrings cannot be empty\"\n    lines = lines_with_leading_tabs_expanded(docstring)\n    # Determine minimum indentation (first line doesn't count):\n    indent = sys.maxsize\n    for line in lines[1:]:\n        stripped = line.lstrip()\n        if stripped:\n            indent = min(indent, len(line) - len(stripped))\n    # Remove indentation (first line is special):\n    trimmed = [lines[0].strip()]\n    if indent < sys.maxsize:\n        last_line_idx = len(lines) - 2\n        for i, line in enumerate(lines[1:]):\n            stripped_line = line[indent:].rstrip()\n            if stripped_line or i == last_line_idx:\n                trimmed.append(prefix + stripped_line)\n            else:\n                trimmed.append(\"\")\n    return \"\\n\".join(trimmed)",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 20,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "get_string_prefix",
      "sourceCode": "def get_string_prefix(string: str) -> str:\n    \"\"\"\n    Pre-conditions:\n        * assert_is_leaf_string(@string)\n\n    Returns:\n        @string's prefix (e.g. '', 'r', 'f', or 'rf').\n    \"\"\"\n    assert_is_leaf_string(string)\n\n    prefix = \"\"\n    prefix_idx = 0\n    while string[prefix_idx] in STRING_PREFIX_CHARS:\n        prefix += string[prefix_idx]\n        prefix_idx += 1\n\n    return prefix",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 16,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "assert_is_leaf_string",
      "sourceCode": "def assert_is_leaf_string(string: str) -> None:\n    \"\"\"\n    Checks the pre-condition that @string has the format that you would expect\n    of `leaf.value` where `leaf` is some Leaf such that `leaf.type ==\n    token.STRING`. A more precise description of the pre-conditions that are\n    checked are listed below.\n\n    Pre-conditions:\n        * @string starts with either ', \", <prefix>', or <prefix>\" where\n        `set(<prefix>)` is some subset of `set(STRING_PREFIX_CHARS)`.\n        * @string ends with a quote character (' or \").\n\n    Raises:\n        AssertionError(...) if the pre-conditions listed above are not\n        satisfied.\n    \"\"\"\n    dquote_idx = string.find('\"')\n    squote_idx = string.find(\"'\")\n    if -1 in [dquote_idx, squote_idx]:\n        quote_idx = max(dquote_idx, squote_idx)\n    else:\n        quote_idx = min(squote_idx, dquote_idx)\n\n    assert (\n        0 <= quote_idx < len(string) - 1\n    ), f\"{string!r} is missing a starting quote character (' or \\\").\"\n    assert string[-1] in (\n        \"'\",\n        '\"',\n    ), f\"{string!r} is missing an ending quote character (' or \\\").\"\n    assert set(string[:quote_idx]).issubset(\n        set(STRING_PREFIX_CHARS)\n    ), f\"{set(string[:quote_idx])} is NOT a subset of {set(STRING_PREFIX_CHARS)}.\"",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 32,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "normalize_string_prefix",
      "sourceCode": "def normalize_string_prefix(s: str) -> str:\n    \"\"\"Make all string prefixes lowercase.\"\"\"\n    match = STRING_PREFIX_RE.match(s)\n    assert match is not None, f\"failed to match string {s!r}\"\n    orig_prefix = match.group(1)\n    new_prefix = (\n        orig_prefix.replace(\"F\", \"f\")\n        .replace(\"B\", \"b\")\n        .replace(\"U\", \"\")\n        .replace(\"u\", \"\")\n    )\n\n    # Python syntax guarantees max 2 prefixes and that one of them is \"r\"\n    if len(new_prefix) == 2 and \"r\" != new_prefix[0].lower():\n        new_prefix = new_prefix[::-1]\n    return f\"{new_prefix}{match.group(2)}\"",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "normalize_string_quotes",
      "sourceCode": "def normalize_string_quotes(s: str) -> str:\n    \"\"\"Prefer double quotes but only if it doesn't cause more escaping.\n\n    Adds or removes backslashes as appropriate.\n    \"\"\"\n    value = s.lstrip(STRING_PREFIX_CHARS)\n    if value[:3] == '\"\"\"':\n        return s\n\n    elif value[:3] == \"'''\":\n        orig_quote = \"'''\"\n        new_quote = '\"\"\"'\n    elif value[0] == '\"':\n        orig_quote = '\"'\n        new_quote = \"'\"\n    else:\n        orig_quote = \"'\"\n        new_quote = '\"'\n    first_quote_pos = s.find(orig_quote)\n    assert first_quote_pos != -1, f\"INTERNAL ERROR: Malformed string {s!r}\"\n\n    prefix = s[:first_quote_pos]\n    unescaped_new_quote = _cached_compile(rf\"(([^\\\\]|^)(\\\\\\\\)*){new_quote}\")\n    escaped_new_quote = _cached_compile(rf\"([^\\\\]|^)\\\\((?:\\\\\\\\)*){new_quote}\")\n    escaped_orig_quote = _cached_compile(rf\"([^\\\\]|^)\\\\((?:\\\\\\\\)*){orig_quote}\")\n    body = s[first_quote_pos + len(orig_quote) : -len(orig_quote)]\n    if \"r\" in prefix.casefold():\n        if unescaped_new_quote.search(body):\n            # There's at least one unescaped new_quote in this raw string\n            # so converting is impossible\n            return s\n\n        # Do not introduce or remove backslashes in raw strings\n        new_body = body\n    else:\n        # remove unnecessary escapes\n        new_body = sub_twice(escaped_new_quote, rf\"\\1\\2{new_quote}\", body)\n        if body != new_body:\n            # Consider the string without unnecessary escapes as the original\n            body = new_body\n            s = f\"{prefix}{orig_quote}{body}{orig_quote}\"\n        new_body = sub_twice(escaped_orig_quote, rf\"\\1\\2{orig_quote}\", new_body)\n        new_body = sub_twice(unescaped_new_quote, rf\"\\1\\\\{new_quote}\", new_body)\n\n    if \"f\" in prefix.casefold():\n        matches = re.findall(\n            r\"\"\"\n            (?:(?<!\\{)|^)\\{  # start of the string or a non-{ followed by a single {\n                ([^{].*?)  # contents of the brackets except if begins with {{\n            \\}(?:(?!\\})|$)  # A } followed by end of the string or a non-}\n            \"\"\",\n            new_body,\n            re.VERBOSE,\n        )\n        for m in matches:\n            if \"\\\\\" in str(m):\n                # Do not introduce backslashes in interpolated expressions\n                return s\n\n    if new_quote == '\"\"\"' and new_body[-1:] == '\"':\n        # edge case:\n        new_body = new_body[:-1] + '\\\\\"'\n    orig_escape_count = body.count(\"\\\\\")\n    new_escape_count = new_body.count(\"\\\\\")\n    if new_escape_count > orig_escape_count:\n        return s  # Do not introduce more escaping\n\n    if new_escape_count == orig_escape_count and orig_quote == '\"':\n        return s  # Prefer double quotes\n\n    return f\"{prefix}{new_quote}{new_body}{new_quote}\"",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 70,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "normalize_fstring_quotes",
      "sourceCode": "def normalize_fstring_quotes(\n    quote: str,\n    middles: list[Leaf],\n    is_raw_fstring: bool,\n) -> tuple[list[Leaf], str]:\n    \"\"\"Prefer double quotes but only if it doesn't cause more escaping.\n\n    Adds or removes backslashes as appropriate.\n    \"\"\"\n    if quote == '\"\"\"':\n        return middles, quote\n\n    elif quote == \"'''\":\n        new_quote = '\"\"\"'\n    elif quote == '\"':\n        new_quote = \"'\"\n    else:\n        new_quote = '\"'\n\n    unescaped_new_quote = _cached_compile(rf\"(([^\\\\]|^)(\\\\\\\\)*){new_quote}\")\n    escaped_new_quote = _cached_compile(rf\"([^\\\\]|^)\\\\((?:\\\\\\\\)*){new_quote}\")\n    escaped_orig_quote = _cached_compile(rf\"([^\\\\]|^)\\\\((?:\\\\\\\\)*){quote}\")\n    if is_raw_fstring:\n        for middle in middles:\n            if unescaped_new_quote.search(middle.value):\n                # There's at least one unescaped new_quote in this raw string\n                # so converting is impossible\n                return middles, quote\n\n        # Do not introduce or remove backslashes in raw strings, just use double quote\n        return middles, '\"'\n\n    new_segments = []\n    for middle in middles:\n        segment = middle.value\n        # remove unnecessary escapes\n        new_segment = sub_twice(escaped_new_quote, rf\"\\1\\2{new_quote}\", segment)\n        if segment != new_segment:\n            # Consider the string without unnecessary escapes as the original\n            middle.value = new_segment\n\n        new_segment = sub_twice(escaped_orig_quote, rf\"\\1\\2{quote}\", new_segment)\n        new_segment = sub_twice(unescaped_new_quote, rf\"\\1\\\\{new_quote}\", new_segment)\n        new_segments.append(new_segment)\n\n    if new_quote == '\"\"\"' and new_segments[-1].endswith('\"'):\n        # edge case:\n        new_segments[-1] = new_segments[-1][:-1] + '\\\\\"'\n\n    for middle, new_segment in zip(middles, new_segments):\n        orig_escape_count = middle.value.count(\"\\\\\")\n        new_escape_count = new_segment.count(\"\\\\\")\n\n    if new_escape_count > orig_escape_count:\n        return middles, quote  # Do not introduce more escaping\n\n    if new_escape_count == orig_escape_count and quote == '\"':\n        return middles, quote  # Prefer double quotes\n\n    for middle, new_segment in zip(middles, new_segments):\n        middle.value = new_segment\n\n    return middles, new_quote",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 62,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "normalize_unicode_escape_sequences",
      "sourceCode": "def normalize_unicode_escape_sequences(leaf: Leaf) -> None:\n    \"\"\"Replace hex codes in Unicode escape sequences with lowercase representation.\"\"\"\n    text = leaf.value\n    prefix = get_string_prefix(text)\n    if \"r\" in prefix.lower():\n        return\n\n    def replace(m: Match[str]) -> str:\n        groups = m.groupdict()\n        back_slashes = groups[\"backslashes\"]\n\n        if len(back_slashes) % 2 == 0:\n            return back_slashes + groups[\"body\"]\n\n        if groups[\"u\"]:\n            # \\u\n            return back_slashes + \"u\" + groups[\"u\"].lower()\n        elif groups[\"U\"]:\n            # \\U\n            return back_slashes + \"U\" + groups[\"U\"].lower()\n        elif groups[\"x\"]:\n            # \\x\n            return back_slashes + \"x\" + groups[\"x\"].lower()\n        else:\n            assert groups[\"N\"], f\"Unexpected match: {m}\"\n            # \\N{}\n            return back_slashes + \"N{\" + groups[\"N\"].upper() + \"}\"\n\n    leaf.value = re.sub(UNICODE_ESCAPE_RE, replace, text)",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 28,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "replace",
      "sourceCode": "def replace(m: Match[str]) -> str:\n        groups = m.groupdict()\n        back_slashes = groups[\"backslashes\"]\n\n        if len(back_slashes) % 2 == 0:\n            return back_slashes + groups[\"body\"]\n\n        if groups[\"u\"]:\n            # \\u\n            return back_slashes + \"u\" + groups[\"u\"].lower()\n        elif groups[\"U\"]:\n            # \\U\n            return back_slashes + \"U\" + groups[\"U\"].lower()\n        elif groups[\"x\"]:\n            # \\x\n            return back_slashes + \"x\" + groups[\"x\"].lower()\n        else:\n            assert groups[\"N\"], f\"Unexpected match: {m}\"\n            # \\N{}\n            return back_slashes + \"N{\" + groups[\"N\"].upper() + \"}\"",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 19,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "char_width",
      "sourceCode": "@lru_cache(maxsize=4096)\ndef char_width(char: str) -> int:\n    \"\"\"Return the width of a single character as it would be displayed in a\n    terminal or editor (which respects Unicode East Asian Width).\n\n    Full width characters are counted as 2, while half width characters are\n    counted as 1.  Also control characters are counted as 0.\n    \"\"\"\n    table = WIDTH_TABLE\n    codepoint = ord(char)\n    highest = len(table) - 1\n    lowest = 0\n    idx = highest // 2\n    while True:\n        start_codepoint, end_codepoint, width = table[idx]\n        if codepoint < start_codepoint:\n            highest = idx - 1\n        elif codepoint > end_codepoint:\n            lowest = idx + 1\n        else:\n            return 0 if width < 0 else width\n        if highest < lowest:\n            break\n        idx = (highest + lowest) // 2\n    return 1",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 24,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "str_width",
      "sourceCode": "def str_width(line_str: str) -> int:\n    \"\"\"Return the width of `line_str` as it would be displayed in a terminal\n    or editor (which respects Unicode East Asian Width).\n\n    You could utilize this function to determine, for example, if a string\n    is too wide to display in a terminal or editor.\n    \"\"\"\n    if line_str.isascii():\n        # Fast path for a line consisting of only ASCII characters\n        return len(line_str)\n    return sum(map(char_width, line_str))",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 10,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "count_chars_in_width",
      "sourceCode": "def count_chars_in_width(line_str: str, max_width: int) -> int:\n    \"\"\"Count the number of characters in `line_str` that would fit in a\n    terminal or editor of `max_width` (which respects Unicode East Asian\n    Width).\n    \"\"\"\n    total_width = 0\n    for i, char in enumerate(line_str):\n        width = char_width(char)\n        if width + total_width > max_width:\n            return i\n        total_width += width\n    return len(line_str)",
      "importString": "import re\nimport sys\nfrom functools import lru_cache\nfrom re import Match, Pattern\nfrom typing import Final\n\nfrom black._width_table import WIDTH_TABLE\nfrom blib2to3.pytree import Leaf",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/strings.py"
    },
    {
      "symbolName": "hug_power_op",
      "sourceCode": "def hug_power_op(\n    line: Line, features: Collection[Feature], mode: Mode\n) -> Iterator[Line]:\n    \"\"\"A transformer which normalizes spacing around power operators.\"\"\"\n\n    # Performance optimization to avoid unnecessary Leaf clones and other ops.\n    for leaf in line.leaves:\n        if leaf.type == token.DOUBLESTAR:\n            break\n    else:\n        raise CannotTransform(\"No doublestar token was found in the line.\")\n\n    def is_simple_lookup(index: int, kind: Literal[1, -1]) -> bool:\n        # Brackets and parentheses indicate calls, subscripts, etc. ...\n        # basically stuff that doesn't count as \"simple\". Only a NAME lookup\n        # or dotted lookup (eg. NAME.NAME) is OK.\n        if Preview.is_simple_lookup_for_doublestar_expression not in mode:\n            return original_is_simple_lookup_func(line, index, kind)\n\n        else:\n            if kind == -1:\n                return handle_is_simple_look_up_prev(\n                    line, index, {token.RPAR, token.RSQB}\n                )\n            else:\n                return handle_is_simple_lookup_forward(\n                    line, index, {token.LPAR, token.LSQB}\n                )\n\n    def is_simple_operand(index: int, kind: Literal[1, -1]) -> bool:\n        # An operand is considered \"simple\" if's a NAME, a numeric CONSTANT, a simple\n        # lookup (see above), with or without a preceding unary operator.\n        start = line.leaves[index]\n        if start.type in {token.NAME, token.NUMBER}:\n            return is_simple_lookup(index, kind)\n\n        if start.type in {token.PLUS, token.MINUS, token.TILDE}:\n            if line.leaves[index + 1].type in {token.NAME, token.NUMBER}:\n                # kind is always one as bases with a preceding unary op will be checked\n                # for simplicity starting from the next token (so it'll hit the check\n                # above).\n                return is_simple_lookup(index + 1, kind=1)\n\n        return False\n\n    new_line = line.clone()\n    should_hug = False\n    for idx, leaf in enumerate(line.leaves):\n        new_leaf = leaf.clone()\n        if should_hug:\n            new_leaf.prefix = \"\"\n            should_hug = False\n\n        should_hug = (\n            (0 < idx < len(line.leaves) - 1)\n            and leaf.type == token.DOUBLESTAR\n            and is_simple_operand(idx - 1, kind=-1)\n            and line.leaves[idx - 1].value != \"lambda\"\n            and is_simple_operand(idx + 1, kind=1)\n        )\n        if should_hug:\n            new_leaf.prefix = \"\"\n\n        # We have to be careful to make a new line properly:\n        # - bracket related metadata must be maintained (handled by Line.append)\n        # - comments need to copied over, updating the leaf IDs they're attached to\n        new_line.append(new_leaf, preformatted=True)\n        for comment_leaf in line.comments_after(leaf):\n            new_line.append(comment_leaf, preformatted=True)\n\n    yield new_line",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 70,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "is_simple_lookup",
      "sourceCode": "def is_simple_lookup(index: int, kind: Literal[1, -1]) -> bool:\n        # Brackets and parentheses indicate calls, subscripts, etc. ...\n        # basically stuff that doesn't count as \"simple\". Only a NAME lookup\n        # or dotted lookup (eg. NAME.NAME) is OK.\n        if Preview.is_simple_lookup_for_doublestar_expression not in mode:\n            return original_is_simple_lookup_func(line, index, kind)\n\n        else:\n            if kind == -1:\n                return handle_is_simple_look_up_prev(\n                    line, index, {token.RPAR, token.RSQB}\n                )\n            else:\n                return handle_is_simple_lookup_forward(\n                    line, index, {token.LPAR, token.LSQB}\n                )",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "is_simple_operand",
      "sourceCode": "def is_simple_operand(index: int, kind: Literal[1, -1]) -> bool:\n        # An operand is considered \"simple\" if's a NAME, a numeric CONSTANT, a simple\n        # lookup (see above), with or without a preceding unary operator.\n        start = line.leaves[index]\n        if start.type in {token.NAME, token.NUMBER}:\n            return is_simple_lookup(index, kind)\n\n        if start.type in {token.PLUS, token.MINUS, token.TILDE}:\n            if line.leaves[index + 1].type in {token.NAME, token.NUMBER}:\n                # kind is always one as bases with a preceding unary op will be checked\n                # for simplicity starting from the next token (so it'll hit the check\n                # above).\n                return is_simple_lookup(index + 1, kind=1)\n\n        return False",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 14,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "original_is_simple_lookup_func",
      "sourceCode": "def original_is_simple_lookup_func(\n    line: Line, index: int, step: Literal[1, -1]\n) -> bool:\n    if step == -1:\n        disallowed = {token.RPAR, token.RSQB}\n    else:\n        disallowed = {token.LPAR, token.LSQB}\n\n    while 0 <= index < len(line.leaves):\n        current = line.leaves[index]\n        if current.type in disallowed:\n            return False\n        if current.type not in {token.NAME, token.DOT} or current.value == \"for\":\n            # If the current token isn't disallowed, we'll assume this is\n            # simple as only the disallowed tokens are semantically\n            # attached to this lookup expression we're checking. Also,\n            # stop early if we hit the 'for' bit of a comprehension.\n            return True\n\n        index += step\n\n    return True",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 21,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "handle_is_simple_look_up_prev",
      "sourceCode": "def handle_is_simple_look_up_prev(line: Line, index: int, disallowed: set[int]) -> bool:\n    \"\"\"\n    Handling the determination of is_simple_lookup for the lines prior to the doublestar\n    token. This is required because of the need to isolate the chained expression\n    to determine the bracket or parenthesis belong to the single expression.\n    \"\"\"\n    contains_disallowed = False\n    chain = []\n\n    while 0 <= index < len(line.leaves):\n        current = line.leaves[index]\n        chain.append(current)\n        if not contains_disallowed and current.type in disallowed:\n            contains_disallowed = True\n        if not is_expression_chained(chain):\n            return not contains_disallowed\n\n        index -= 1\n\n    return True",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 19,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "handle_is_simple_lookup_forward",
      "sourceCode": "def handle_is_simple_lookup_forward(\n    line: Line, index: int, disallowed: set[int]\n) -> bool:\n    \"\"\"\n    Handling decision is_simple_lookup for the lines behind the doublestar token.\n    This function is simplified to keep consistent with the prior logic and the forward\n    case are more straightforward and do not need to care about chained expressions.\n    \"\"\"\n    while 0 <= index < len(line.leaves):\n        current = line.leaves[index]\n        if current.type in disallowed:\n            return False\n        if current.type not in {token.NAME, token.DOT} or (\n            current.type == token.NAME and current.value == \"for\"\n        ):\n            # If the current token isn't disallowed, we'll assume this is simple as\n            # only the disallowed tokens are semantically attached to this lookup\n            # expression we're checking. Also, stop early if we hit the 'for' bit\n            # of a comprehension.\n            return True\n\n        index += 1\n\n    return True",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 23,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "is_expression_chained",
      "sourceCode": "def is_expression_chained(chained_leaves: list[Leaf]) -> bool:\n    \"\"\"\n    Function to determine if the variable is a chained call.\n    (e.g., foo.lookup, foo().lookup, (foo.lookup())) will be recognized as chained call)\n    \"\"\"\n    if len(chained_leaves) < 2:\n        return True\n\n    current_leaf = chained_leaves[-1]\n    past_leaf = chained_leaves[-2]\n\n    if past_leaf.type == token.NAME:\n        return current_leaf.type in {token.DOT}\n    elif past_leaf.type in {token.RPAR, token.RSQB}:\n        return current_leaf.type in {token.RSQB, token.RPAR}\n    elif past_leaf.type in {token.LPAR, token.LSQB}:\n        return current_leaf.type in {token.NAME, token.LPAR, token.LSQB}\n    else:\n        return False",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 18,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_match",
      "sourceCode": "@abstractmethod\n    def do_match(self, line: Line) -> TMatchResult:\n        \"\"\"\n        Returns:\n            * Ok(string_indices) such that for each index, `line.leaves[index]`\n              is our target string if a match was able to be made. For\n              transformers that don't result in more lines (e.g. StringMerger,\n              StringParenStripper), multiple matches and transforms are done at\n              once to reduce the complexity.\n              OR\n            * Err(CannotTransform), if no match could be made.\n        \"\"\"",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_transform",
      "sourceCode": "@abstractmethod\n    def do_transform(\n        self, line: Line, string_indices: list[int]\n    ) -> Iterator[TResult[Line]]:\n        \"\"\"\n        Yields:\n            * Ok(new_line) where new_line is the new transformed line.\n              OR\n            * Err(CannotTransform) if the transformation failed for some reason. The\n              `do_match(...)` template method should usually be used to reject\n              the form of the given Line, but in some cases it is difficult to\n              know whether or not a Line meets the StringTransformer's\n              requirements until the transformation is already midway.\n\n        Side Effects:\n            This method should NOT mutate @line directly, but it MAY mutate the\n            Line's underlying Node structure. (WARNING: If the underlying Node\n            structure IS altered, then this method should NOT be allowed to\n            yield an CannotTransform after that point.)\n        \"\"\"",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 19,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "__call__",
      "sourceCode": "def __call__(\n        self, line: Line, _features: Collection[Feature], _mode: Mode\n    ) -> Iterator[Line]:\n        \"\"\"\n        StringTransformer instances have a call signature that mirrors that of\n        the Transformer type.\n\n        Raises:\n            CannotTransform(...) if the concrete StringTransformer class is unable\n            to transform @line.\n        \"\"\"\n        # Optimization to avoid calling `self.do_match(...)` when the line does\n        # not contain any string.\n        if not any(leaf.type == token.STRING for leaf in line.leaves):\n            raise CannotTransform(\"There are no strings in this line.\")\n\n        match_result = self.do_match(line)\n\n        if isinstance(match_result, Err):\n            cant_transform = match_result.err()\n            raise CannotTransform(\n                f\"The string transformer {self.__class__.__name__} does not recognize\"\n                \" this line as one that it can transform.\"\n            ) from cant_transform\n\n        string_indices = match_result.ok()\n\n        for line_result in self.do_transform(line, string_indices):\n            if isinstance(line_result, Err):\n                cant_transform = line_result.err()\n                raise CannotTransform(\n                    \"StringTransformer failed while attempting to transform string.\"\n                ) from cant_transform\n            line = line_result.ok()\n            yield line",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 34,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "pop_custom_splits",
      "sourceCode": "def pop_custom_splits(self, string: str) -> list[CustomSplit]:\n        \"\"\"Custom Split Map Getter Method\n\n        Returns:\n            * A list of the custom splits that are mapped to @string, if any\n              exist.\n              OR\n            * [], otherwise.\n\n        Side Effects:\n            Deletes the mapping between @string and its associated custom\n            splits (which are returned to the caller).\n        \"\"\"\n        key = self._get_key(string)\n\n        custom_splits = self._CUSTOM_SPLIT_MAP[key]\n        del self._CUSTOM_SPLIT_MAP[key]\n\n        return list(custom_splits)",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 18,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_match",
      "sourceCode": "def do_match(self, line: Line) -> TMatchResult:\n        LL = line.leaves\n\n        is_valid_index = is_valid_index_factory(LL)\n\n        string_indices = []\n        idx = 0\n        while is_valid_index(idx):\n            leaf = LL[idx]\n            if (\n                leaf.type == token.STRING\n                and is_valid_index(idx + 1)\n                and LL[idx + 1].type == token.STRING\n            ):\n                # Let's check if the string group contains an inline comment\n                # If we have a comment inline, we don't merge the strings\n                contains_comment = False\n                i = idx\n                while is_valid_index(i):\n                    if LL[i].type != token.STRING:\n                        break\n                    if line.comments_after(LL[i]):\n                        contains_comment = True\n                        break\n                    i += 1\n\n                if not contains_comment and not is_part_of_annotation(leaf):\n                    string_indices.append(idx)\n\n                # Advance to the next non-STRING leaf.\n                idx += 2\n                while is_valid_index(idx) and LL[idx].type == token.STRING:\n                    idx += 1\n\n            elif leaf.type == token.STRING and \"\\\\\\n\" in leaf.value:\n                string_indices.append(idx)\n                # Advance to the next non-STRING leaf.\n                idx += 1\n                while is_valid_index(idx) and LL[idx].type == token.STRING:\n                    idx += 1\n\n            else:\n                idx += 1\n\n        if string_indices:\n            return Ok(string_indices)\n        else:\n            return TErr(\"This line has no strings that need merging.\")",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 47,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_transform",
      "sourceCode": "def do_transform(\n        self, line: Line, string_indices: list[int]\n    ) -> Iterator[TResult[Line]]:\n        new_line = line\n\n        rblc_result = self._remove_backslash_line_continuation_chars(\n            new_line, string_indices\n        )\n        if isinstance(rblc_result, Ok):\n            new_line = rblc_result.ok()\n\n        msg_result = self._merge_string_group(new_line, string_indices)\n        if isinstance(msg_result, Ok):\n            new_line = msg_result.ok()\n\n        if isinstance(rblc_result, Err) and isinstance(msg_result, Err):\n            msg_cant_transform = msg_result.err()\n            rblc_cant_transform = rblc_result.err()\n            cant_transform = CannotTransform(\n                \"StringMerger failed to merge any strings in this line.\"\n            )\n\n            # Chain the errors together using `__cause__`.\n            msg_cant_transform.__cause__ = rblc_cant_transform\n            cant_transform.__cause__ = msg_cant_transform\n\n            yield Err(cant_transform)\n        else:\n            yield Ok(new_line)",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 28,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_remove_backslash_line_continuation_chars",
      "sourceCode": "@staticmethod\n    def _remove_backslash_line_continuation_chars(\n        line: Line, string_indices: list[int]\n    ) -> TResult[Line]:\n        \"\"\"\n        Merge strings that were split across multiple lines using\n        line-continuation backslashes.\n\n        Returns:\n            Ok(new_line), if @line contains backslash line-continuation\n            characters.\n                OR\n            Err(CannotTransform), otherwise.\n        \"\"\"\n        LL = line.leaves\n\n        indices_to_transform = []\n        for string_idx in string_indices:\n            string_leaf = LL[string_idx]\n            if (\n                string_leaf.type == token.STRING\n                and \"\\\\\\n\" in string_leaf.value\n                and not has_triple_quotes(string_leaf.value)\n            ):\n                indices_to_transform.append(string_idx)\n\n        if not indices_to_transform:\n            return TErr(\n                \"Found no string leaves that contain backslash line continuation\"\n                \" characters.\"\n            )\n\n        new_line = line.clone()\n        new_line.comments = line.comments.copy()\n        append_leaves(new_line, line, LL)\n\n        for string_idx in indices_to_transform:\n            new_string_leaf = new_line.leaves[string_idx]\n            new_string_leaf.value = new_string_leaf.value.replace(\"\\\\\\n\", \"\")\n\n        return Ok(new_line)",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 40,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_merge_string_group",
      "sourceCode": "def _merge_string_group(\n        self, line: Line, string_indices: list[int]\n    ) -> TResult[Line]:\n        \"\"\"\n        Merges string groups (i.e. set of adjacent strings).\n\n        Each index from `string_indices` designates one string group's first\n        leaf in `line.leaves`.\n\n        Returns:\n            Ok(new_line), if ALL of the validation checks found in\n            _validate_msg(...) pass.\n                OR\n            Err(CannotTransform), otherwise.\n        \"\"\"\n        LL = line.leaves\n\n        is_valid_index = is_valid_index_factory(LL)\n\n        # A dict of {string_idx: tuple[num_of_strings, string_leaf]}.\n        merged_string_idx_dict: dict[int, tuple[int, Leaf]] = {}\n        for string_idx in string_indices:\n            vresult = self._validate_msg(line, string_idx)\n            if isinstance(vresult, Err):\n                continue\n            merged_string_idx_dict[string_idx] = self._merge_one_string_group(\n                LL, string_idx, is_valid_index\n            )\n\n        if not merged_string_idx_dict:\n            return TErr(\"No string group is merged\")\n\n        # Build the final line ('new_line') that this method will later return.\n        new_line = line.clone()\n        previous_merged_string_idx = -1\n        previous_merged_num_of_strings = -1\n        for i, leaf in enumerate(LL):\n            if i in merged_string_idx_dict:\n                previous_merged_string_idx = i\n                previous_merged_num_of_strings, string_leaf = merged_string_idx_dict[i]\n                new_line.append(string_leaf)\n\n            if (\n                previous_merged_string_idx\n                <= i\n                < previous_merged_string_idx + previous_merged_num_of_strings\n            ):\n                for comment_leaf in line.comments_after(LL[i]):\n                    new_line.append(comment_leaf, preformatted=True)\n                continue\n\n            append_leaves(new_line, line, [leaf])\n\n        return Ok(new_line)",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 53,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_merge_one_string_group",
      "sourceCode": "def _merge_one_string_group(\n        self, LL: list[Leaf], string_idx: int, is_valid_index: Callable[[int], bool]\n    ) -> tuple[int, Leaf]:\n        \"\"\"\n        Merges one string group where the first string in the group is\n        `LL[string_idx]`.\n\n        Returns:\n            A tuple of `(num_of_strings, leaf)` where `num_of_strings` is the\n            number of strings merged and `leaf` is the newly merged string\n            to be replaced in the new line.\n        \"\"\"\n        # If the string group is wrapped inside an Atom node, we must make sure\n        # to later replace that Atom with our new (merged) string leaf.\n        atom_node = LL[string_idx].parent\n\n        # We will place BREAK_MARK in between every two substrings that we\n        # merge. We will then later go through our final result and use the\n        # various instances of BREAK_MARK we find to add the right values to\n        # the custom split map.\n        BREAK_MARK = \"@@@@@ BLACK BREAKPOINT MARKER @@@@@\"\n\n        QUOTE = LL[string_idx].value[-1]\n\n        def make_naked(string: str, string_prefix: str) -> str:\n            \"\"\"Strip @string (i.e. make it a \"naked\" string)\n\n            Pre-conditions:\n                * assert_is_leaf_string(@string)\n\n            Returns:\n                A string that is identical to @string except that\n                @string_prefix has been stripped, the surrounding QUOTE\n                characters have been removed, and any remaining QUOTE\n                characters have been escaped.\n            \"\"\"\n            assert_is_leaf_string(string)\n            if \"f\" in string_prefix:\n                f_expressions = (\n                    string[span[0] + 1 : span[1] - 1]  # +-1 to get rid of curly braces\n                    for span in iter_fexpr_spans(string)\n                )\n                debug_expressions_contain_visible_quotes = any(\n                    re.search(r\".*[\\'\\\"].*(?<![!:=])={1}(?!=)(?![^\\s:])\", expression)\n                    for expression in f_expressions\n                )\n                if not debug_expressions_contain_visible_quotes:\n                    # We don't want to toggle visible quotes in debug f-strings, as\n                    # that would modify the AST\n                    string = _toggle_fexpr_quotes(string, QUOTE)\n                    # After quotes toggling, quotes in expressions won't be escaped\n                    # because quotes can't be reused in f-strings. So we can simply\n                    # let the escaping logic below run without knowing f-string\n                    # expressions.\n\n            RE_EVEN_BACKSLASHES = r\"(?:(?<!\\\\)(?:\\\\\\\\)*)\"\n            naked_string = string[len(string_prefix) + 1 : -1]\n            naked_string = re.sub(\n                \"(\" + RE_EVEN_BACKSLASHES + \")\" + QUOTE, r\"\\1\\\\\" + QUOTE, naked_string\n            )\n            return naked_string\n\n        # Holds the CustomSplit objects that will later be added to the custom\n        # split map.\n        custom_splits = []\n\n        # Temporary storage for the 'has_prefix' part of the CustomSplit objects.\n        prefix_tracker = []\n\n        # Sets the 'prefix' variable. This is the prefix that the final merged\n        # string will have.\n        next_str_idx = string_idx\n        prefix = \"\"\n        while (\n            not prefix\n            and is_valid_index(next_str_idx)\n            and LL[next_str_idx].type == token.STRING\n        ):\n            prefix = get_string_prefix(LL[next_str_idx].value).lower()\n            next_str_idx += 1\n\n        # The next loop merges the string group. The final string will be\n        # contained in 'S'.\n        #\n        # The following convenience variables are used:\n        #\n        #   S: string\n        #   NS: naked string\n        #   SS: next string\n        #   NSS: naked next string\n        S = \"\"\n        NS = \"\"\n        num_of_strings = 0\n        next_str_idx = string_idx\n        while is_valid_index(next_str_idx) and LL[next_str_idx].type == token.STRING:\n            num_of_strings += 1\n\n            SS = LL[next_str_idx].value\n            next_prefix = get_string_prefix(SS).lower()\n\n            # If this is an f-string group but this substring is not prefixed\n            # with 'f'...\n            if \"f\" in prefix and \"f\" not in next_prefix:\n                # Then we must escape any braces contained in this substring.\n                SS = re.sub(r\"(\\{|\\})\", r\"\\1\\1\", SS)\n\n            NSS = make_naked(SS, next_prefix)\n\n            has_prefix = bool(next_prefix)\n            prefix_tracker.append(has_prefix)\n\n            S = prefix + QUOTE + NS + NSS + BREAK_MARK + QUOTE\n            NS = make_naked(S, prefix)\n\n            next_str_idx += 1\n\n        # Take a note on the index of the non-STRING leaf.\n        non_string_idx = next_str_idx\n\n        S_leaf = Leaf(token.STRING, S)\n        if self.normalize_strings:\n            S_leaf.value = normalize_string_quotes(S_leaf.value)\n\n        # Fill the 'custom_splits' list with the appropriate CustomSplit objects.\n        temp_string = S_leaf.value[len(prefix) + 1 : -1]\n        for has_prefix in prefix_tracker:\n            mark_idx = temp_string.find(BREAK_MARK)\n            assert (\n                mark_idx >= 0\n            ), \"Logic error while filling the custom string breakpoint cache.\"\n\n            temp_string = temp_string[mark_idx + len(BREAK_MARK) :]\n            breakpoint_idx = mark_idx + (len(prefix) if has_prefix else 0) + 1\n            custom_splits.append(CustomSplit(has_prefix, breakpoint_idx))\n\n        string_leaf = Leaf(token.STRING, S_leaf.value.replace(BREAK_MARK, \"\"))\n\n        if atom_node is not None:\n            # If not all children of the atom node are merged (this can happen\n            # when there is a standalone comment in the middle) ...\n            if non_string_idx - string_idx < len(atom_node.children):\n                # We need to replace the old STRING leaves with the new string leaf.\n                first_child_idx = LL[string_idx].remove()\n                for idx in range(string_idx + 1, non_string_idx):\n                    LL[idx].remove()\n                if first_child_idx is not None:\n                    atom_node.insert_child(first_child_idx, string_leaf)\n            else:\n                # Else replace the atom node with the new string leaf.\n                replace_child(atom_node, string_leaf)\n\n        self.add_custom_splits(string_leaf.value, custom_splits)\n        return num_of_strings, string_leaf",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 152,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "make_naked",
      "sourceCode": "def make_naked(string: str, string_prefix: str) -> str:\n            \"\"\"Strip @string (i.e. make it a \"naked\" string)\n\n            Pre-conditions:\n                * assert_is_leaf_string(@string)\n\n            Returns:\n                A string that is identical to @string except that\n                @string_prefix has been stripped, the surrounding QUOTE\n                characters have been removed, and any remaining QUOTE\n                characters have been escaped.\n            \"\"\"\n            assert_is_leaf_string(string)\n            if \"f\" in string_prefix:\n                f_expressions = (\n                    string[span[0] + 1 : span[1] - 1]  # +-1 to get rid of curly braces\n                    for span in iter_fexpr_spans(string)\n                )\n                debug_expressions_contain_visible_quotes = any(\n                    re.search(r\".*[\\'\\\"].*(?<![!:=])={1}(?!=)(?![^\\s:])\", expression)\n                    for expression in f_expressions\n                )\n                if not debug_expressions_contain_visible_quotes:\n                    # We don't want to toggle visible quotes in debug f-strings, as\n                    # that would modify the AST\n                    string = _toggle_fexpr_quotes(string, QUOTE)\n                    # After quotes toggling, quotes in expressions won't be escaped\n                    # because quotes can't be reused in f-strings. So we can simply\n                    # let the escaping logic below run without knowing f-string\n                    # expressions.\n\n            RE_EVEN_BACKSLASHES = r\"(?:(?<!\\\\)(?:\\\\\\\\)*)\"\n            naked_string = string[len(string_prefix) + 1 : -1]\n            naked_string = re.sub(\n                \"(\" + RE_EVEN_BACKSLASHES + \")\" + QUOTE, r\"\\1\\\\\" + QUOTE, naked_string\n            )\n            return naked_string",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 36,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_validate_msg",
      "sourceCode": "@staticmethod\n    def _validate_msg(line: Line, string_idx: int) -> TResult[None]:\n        \"\"\"Validate (M)erge (S)tring (G)roup\n\n        Transform-time string validation logic for _merge_string_group(...).\n\n        Returns:\n            * Ok(None), if ALL validation checks (listed below) pass.\n                OR\n            * Err(CannotTransform), if any of the following are true:\n                - The target string group does not contain ANY stand-alone comments.\n                - The target string is not in a string group (i.e. it has no\n                  adjacent strings).\n                - The string group has more than one inline comment.\n                - The string group has an inline comment that appears to be a pragma.\n                - The set of all string prefixes in the string group is of\n                  length greater than one and is not equal to {\"\", \"f\"}.\n                - The string group consists of raw strings.\n                - The string group would merge f-strings with different quote types\n                  and internal quotes.\n                - The string group is stringified type annotations. We don't want to\n                  process stringified type annotations since pyright doesn't support\n                  them spanning multiple string values. (NOTE: mypy, pytype, pyre do\n                  support them, so we can change if pyright also gains support in the\n                  future. See https://github.com/microsoft/pyright/issues/4359.)\n        \"\"\"\n        # We first check for \"inner\" stand-alone comments (i.e. stand-alone\n        # comments that have a string leaf before them AND after them).\n        for inc in [1, -1]:\n            i = string_idx\n            found_sa_comment = False\n            is_valid_index = is_valid_index_factory(line.leaves)\n            while is_valid_index(i) and line.leaves[i].type in [\n                token.STRING,\n                STANDALONE_COMMENT,\n            ]:\n                if line.leaves[i].type == STANDALONE_COMMENT:\n                    found_sa_comment = True\n                elif found_sa_comment:\n                    return TErr(\n                        \"StringMerger does NOT merge string groups which contain \"\n                        \"stand-alone comments.\"\n                    )\n\n                i += inc\n\n        QUOTE = line.leaves[string_idx].value[-1]\n\n        num_of_inline_string_comments = 0\n        set_of_prefixes = set()\n        num_of_strings = 0\n        for leaf in line.leaves[string_idx:]:\n            if leaf.type != token.STRING:\n                # If the string group is trailed by a comma, we count the\n                # comments trailing the comma to be one of the string group's\n                # comments.\n                if leaf.type == token.COMMA and id(leaf) in line.comments:\n                    num_of_inline_string_comments += 1\n                break\n\n            if has_triple_quotes(leaf.value):\n                return TErr(\"StringMerger does NOT merge multiline strings.\")\n\n            num_of_strings += 1\n            prefix = get_string_prefix(leaf.value).lower()\n            if \"r\" in prefix:\n                return TErr(\"StringMerger does NOT merge raw strings.\")\n\n            set_of_prefixes.add(prefix)\n\n            if (\n                \"f\" in prefix\n                and leaf.value[-1] != QUOTE\n                and (\n                    \"'\" in leaf.value[len(prefix) + 1 : -1]\n                    or '\"' in leaf.value[len(prefix) + 1 : -1]\n                )\n            ):\n                return TErr(\n                    \"StringMerger does NOT merge f-strings with different quote types\"\n                    \"and internal quotes.\"\n                )\n\n            if id(leaf) in line.comments:\n                num_of_inline_string_comments += 1\n                if contains_pragma_comment(line.comments[id(leaf)]):\n                    return TErr(\"Cannot merge strings which have pragma comments.\")\n\n        if num_of_strings < 2:\n            return TErr(\n                f\"Not enough strings to merge (num_of_strings={num_of_strings}).\"\n            )\n\n        if num_of_inline_string_comments > 1:\n            return TErr(\n                f\"Too many inline string comments ({num_of_inline_string_comments}).\"\n            )\n\n        if len(set_of_prefixes) > 1 and set_of_prefixes != {\"\", \"f\"}:\n            return TErr(f\"Too many different prefixes ({set_of_prefixes}).\")\n\n        return Ok(None)",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 101,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_match",
      "sourceCode": "def do_match(self, line: Line) -> TMatchResult:\n        LL = line.leaves\n\n        is_valid_index = is_valid_index_factory(LL)\n\n        string_indices = []\n\n        idx = -1\n        while True:\n            idx += 1\n            if idx >= len(LL):\n                break\n            leaf = LL[idx]\n\n            # Should be a string...\n            if leaf.type != token.STRING:\n                continue\n\n            # If this is a \"pointless\" string...\n            if (\n                leaf.parent\n                and leaf.parent.parent\n                and leaf.parent.parent.type == syms.simple_stmt\n            ):\n                continue\n\n            # Should be preceded by a non-empty LPAR...\n            if (\n                not is_valid_index(idx - 1)\n                or LL[idx - 1].type != token.LPAR\n                or is_empty_lpar(LL[idx - 1])\n            ):\n                continue\n\n            # That LPAR should NOT be preceded by a function name or a closing\n            # bracket (which could be a function which returns a function or a\n            # list/dictionary that contains a function)...\n            if is_valid_index(idx - 2) and (\n                LL[idx - 2].type == token.NAME or LL[idx - 2].type in CLOSING_BRACKETS\n            ):\n                continue\n\n            string_idx = idx\n\n            # Skip the string trailer, if one exists.\n            string_parser = StringParser()\n            next_idx = string_parser.parse(LL, string_idx)\n\n            # if the leaves in the parsed string include a PERCENT, we need to\n            # make sure the initial LPAR is NOT preceded by an operator with\n            # higher or equal precedence to PERCENT\n            if is_valid_index(idx - 2):\n                # mypy can't quite follow unless we name this\n                before_lpar = LL[idx - 2]\n                if token.PERCENT in {leaf.type for leaf in LL[idx - 1 : next_idx]} and (\n                    (\n                        before_lpar.type\n                        in {\n                            token.STAR,\n                            token.AT,\n                            token.SLASH,\n                            token.DOUBLESLASH,\n                            token.PERCENT,\n                            token.TILDE,\n                            token.DOUBLESTAR,\n                            token.AWAIT,\n                            token.LSQB,\n                            token.LPAR,\n                        }\n                    )\n                    or (\n                        # only unary PLUS/MINUS\n                        before_lpar.parent\n                        and before_lpar.parent.type == syms.factor\n                        and (before_lpar.type in {token.PLUS, token.MINUS})\n                    )\n                ):\n                    continue\n\n            # Should be followed by a non-empty RPAR...\n            if (\n                is_valid_index(next_idx)\n                and LL[next_idx].type == token.RPAR\n                and not is_empty_rpar(LL[next_idx])\n            ):\n                # That RPAR should NOT be followed by anything with higher\n                # precedence than PERCENT\n                if is_valid_index(next_idx + 1) and LL[next_idx + 1].type in {\n                    token.DOUBLESTAR,\n                    token.LSQB,\n                    token.LPAR,\n                    token.DOT,\n                }:\n                    continue\n\n                string_indices.append(string_idx)\n                idx = string_idx\n                while idx < len(LL) - 1 and LL[idx + 1].type == token.STRING:\n                    idx += 1\n\n        if string_indices:\n            return Ok(string_indices)\n        return TErr(\"This line has no strings wrapped in parens.\")",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 102,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_transform",
      "sourceCode": "def do_transform(\n        self, line: Line, string_indices: list[int]\n    ) -> Iterator[TResult[Line]]:\n        LL = line.leaves\n\n        string_and_rpar_indices: list[int] = []\n        for string_idx in string_indices:\n            string_parser = StringParser()\n            rpar_idx = string_parser.parse(LL, string_idx)\n\n            should_transform = True\n            for leaf in (LL[string_idx - 1], LL[rpar_idx]):\n                if line.comments_after(leaf):\n                    # Should not strip parentheses which have comments attached\n                    # to them.\n                    should_transform = False\n                    break\n            if should_transform:\n                string_and_rpar_indices.extend((string_idx, rpar_idx))\n\n        if string_and_rpar_indices:\n            yield Ok(self._transform_to_new_line(line, string_and_rpar_indices))\n        else:\n            yield Err(\n                CannotTransform(\"All string groups have comments attached to them.\")\n            )",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 25,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_transform_to_new_line",
      "sourceCode": "def _transform_to_new_line(\n        self, line: Line, string_and_rpar_indices: list[int]\n    ) -> Line:\n        LL = line.leaves\n\n        new_line = line.clone()\n        new_line.comments = line.comments.copy()\n\n        previous_idx = -1\n        # We need to sort the indices, since string_idx and its matching\n        # rpar_idx may not come in order, e.g. in\n        # `(\"outer\" % (\"inner\".join(items)))`, the \"inner\" string's\n        # string_idx is smaller than \"outer\" string's rpar_idx.\n        for idx in sorted(string_and_rpar_indices):\n            leaf = LL[idx]\n            lpar_or_rpar_idx = idx - 1 if leaf.type == token.STRING else idx\n            append_leaves(new_line, line, LL[previous_idx + 1 : lpar_or_rpar_idx])\n            if leaf.type == token.STRING:\n                string_leaf = Leaf(token.STRING, LL[idx].value)\n                LL[lpar_or_rpar_idx].remove()  # Remove lpar.\n                replace_child(LL[idx], string_leaf)\n                new_line.append(string_leaf)\n                # replace comments\n                old_comments = new_line.comments.pop(id(LL[idx]), [])\n                new_line.comments.setdefault(id(string_leaf), []).extend(old_comments)\n            else:\n                LL[lpar_or_rpar_idx].remove()  # This is a rpar.\n\n            previous_idx = idx\n\n        # Append the leaves after the last idx:\n        append_leaves(new_line, line, LL[idx + 1 :])\n\n        return new_line",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 33,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_match",
      "sourceCode": "def do_match(self, line: Line) -> TMatchResult:\n        match_result = self.do_splitter_match(line)\n        if isinstance(match_result, Err):\n            return match_result\n\n        string_indices = match_result.ok()\n        assert len(string_indices) == 1, (\n            f\"{self.__class__.__name__} should only find one match at a time, found\"\n            f\" {len(string_indices)}\"\n        )\n        string_idx = string_indices[0]\n        vresult = self._validate(line, string_idx)\n        if isinstance(vresult, Err):\n            return vresult\n\n        return match_result",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 15,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_validate",
      "sourceCode": "def _validate(self, line: Line, string_idx: int) -> TResult[None]:\n        \"\"\"\n        Checks that @line meets all of the requirements listed in this classes'\n        docstring. Refer to `help(BaseStringSplitter)` for a detailed\n        description of those requirements.\n\n        Returns:\n            * Ok(None), if ALL of the requirements are met.\n              OR\n            * Err(CannotTransform), if ANY of the requirements are NOT met.\n        \"\"\"\n        LL = line.leaves\n\n        string_leaf = LL[string_idx]\n\n        max_string_length = self._get_max_string_length(line, string_idx)\n        if len(string_leaf.value) <= max_string_length:\n            return TErr(\n                \"The string itself is not what is causing this line to be too long.\"\n            )\n\n        if not string_leaf.parent or [L.type for L in string_leaf.parent.children] == [\n            token.STRING,\n            token.NEWLINE,\n        ]:\n            return TErr(\n                f\"This string ({string_leaf.value}) appears to be pointless (i.e. has\"\n                \" no parent).\"\n            )\n\n        if id(line.leaves[string_idx]) in line.comments and contains_pragma_comment(\n            line.comments[id(line.leaves[string_idx])]\n        ):\n            return TErr(\n                \"Line appears to end with an inline pragma comment. Splitting the line\"\n                \" could modify the pragma's behavior.\"\n            )\n\n        if has_triple_quotes(string_leaf.value):\n            return TErr(\"We cannot split multiline strings.\")\n\n        return Ok(None)",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 41,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_get_max_string_length",
      "sourceCode": "def _get_max_string_length(self, line: Line, string_idx: int) -> int:\n        \"\"\"\n        Calculates the max string length used when attempting to determine\n        whether or not the target string is responsible for causing the line to\n        go over the line length limit.\n\n        WARNING: This method is tightly coupled to both StringSplitter and\n        (especially) StringParenWrapper. There is probably a better way to\n        accomplish what is being done here.\n\n        Returns:\n            max_string_length: such that `line.leaves[string_idx].value >\n            max_string_length` implies that the target string IS responsible\n            for causing this line to exceed the line length limit.\n        \"\"\"\n        LL = line.leaves\n\n        is_valid_index = is_valid_index_factory(LL)\n\n        # We use the shorthand \"WMA4\" in comments to abbreviate \"We must\n        # account for\". When giving examples, we use STRING to mean some/any\n        # valid string.\n        #\n        # Finally, we use the following convenience variables:\n        #\n        #   P:  The leaf that is before the target string leaf.\n        #   N:  The leaf that is after the target string leaf.\n        #   NN: The leaf that is after N.\n\n        # WMA4 the whitespace at the beginning of the line.\n        offset = line.depth * 4\n\n        if is_valid_index(string_idx - 1):\n            p_idx = string_idx - 1\n            if (\n                LL[string_idx - 1].type == token.LPAR\n                and LL[string_idx - 1].value == \"\"\n                and string_idx >= 2\n            ):\n                # If the previous leaf is an empty LPAR placeholder, we should skip it.\n                p_idx -= 1\n\n            P = LL[p_idx]\n            if P.type in self.STRING_OPERATORS:\n                # WMA4 a space and a string operator (e.g. `+ STRING` or `== STRING`).\n                offset += len(str(P)) + 1\n\n            if P.type == token.COMMA:\n                # WMA4 a space, a comma, and a closing bracket [e.g. `), STRING`].\n                offset += 3\n\n            if P.type in [token.COLON, token.EQUAL, token.PLUSEQUAL, token.NAME]:\n                # This conditional branch is meant to handle dictionary keys,\n                # variable assignments, 'return STRING' statement lines, and\n                # 'else STRING' ternary expression lines.\n\n                # WMA4 a single space.\n                offset += 1\n\n                # WMA4 the lengths of any leaves that came before that space,\n                # but after any closing bracket before that space.\n                for leaf in reversed(LL[: p_idx + 1]):\n                    offset += len(str(leaf))\n                    if leaf.type in CLOSING_BRACKETS:\n                        break\n\n        if is_valid_index(string_idx + 1):\n            N = LL[string_idx + 1]\n            if N.type == token.RPAR and N.value == \"\" and len(LL) > string_idx + 2:\n                # If the next leaf is an empty RPAR placeholder, we should skip it.\n                N = LL[string_idx + 2]\n\n            if N.type == token.COMMA:\n                # WMA4 a single comma at the end of the string (e.g `STRING,`).\n                offset += 1\n\n            if is_valid_index(string_idx + 2):\n                NN = LL[string_idx + 2]\n\n                if N.type == token.DOT and NN.type == token.NAME:\n                    # This conditional branch is meant to handle method calls invoked\n                    # off of a string literal up to and including the LPAR character.\n\n                    # WMA4 the '.' character.\n                    offset += 1\n\n                    if (\n                        is_valid_index(string_idx + 3)\n                        and LL[string_idx + 3].type == token.LPAR\n                    ):\n                        # WMA4 the left parenthesis character.\n                        offset += 1\n\n                    # WMA4 the length of the method's name.\n                    offset += len(NN.value)\n\n        has_comments = False\n        for comment_leaf in line.comments_after(LL[string_idx]):\n            if not has_comments:\n                has_comments = True\n                # WMA4 two spaces before the '#' character.\n                offset += 2\n\n            # WMA4 the length of the inline comment.\n            offset += len(comment_leaf.value)\n\n        max_string_length = count_chars_in_width(str(line), self.line_length - offset)\n        return max_string_length",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 107,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_prefer_paren_wrap_match",
      "sourceCode": "@staticmethod\n    def _prefer_paren_wrap_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the \"prefer paren wrap\" statement\n            requirements listed in the 'Requirements' section of the StringParenWrapper\n            class's docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # The line must start with a string.\n        if LL[0].type != token.STRING:\n            return None\n\n        matching_nodes = [\n            syms.listmaker,\n            syms.dictsetmaker,\n            syms.testlist_gexp,\n        ]\n        # If the string is an immediate child of a list/set/tuple literal...\n        if (\n            parent_type(LL[0]) in matching_nodes\n            or parent_type(LL[0].parent) in matching_nodes\n        ):\n            # And the string is surrounded by commas (or is the first/last child)...\n            prev_sibling = LL[0].prev_sibling\n            next_sibling = LL[0].next_sibling\n            if (\n                not prev_sibling\n                and not next_sibling\n                and parent_type(LL[0]) == syms.atom\n            ):\n                # If it's an atom string, we need to check the parent atom's siblings.\n                parent = LL[0].parent\n                assert parent is not None  # For type checkers.\n                prev_sibling = parent.prev_sibling\n                next_sibling = parent.next_sibling\n            if (not prev_sibling or prev_sibling.type == token.COMMA) and (\n                not next_sibling or next_sibling.type == token.COMMA\n            ):\n                return 0\n\n        return None",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 43,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "iter_fexpr_spans",
      "sourceCode": "def iter_fexpr_spans(s: str) -> Iterator[tuple[int, int]]:\n    \"\"\"\n    Yields spans corresponding to expressions in a given f-string.\n    Spans are half-open ranges (left inclusive, right exclusive).\n    Assumes the input string is a valid f-string, but will not crash if the input\n    string is invalid.\n    \"\"\"\n    stack: list[int] = []  # our curly paren stack\n    i = 0\n    while i < len(s):\n        if s[i] == \"{\":\n            # if we're in a string part of the f-string, ignore escaped curly braces\n            if not stack and i + 1 < len(s) and s[i + 1] == \"{\":\n                i += 2\n                continue\n            stack.append(i)\n            i += 1\n            continue\n\n        if s[i] == \"}\":\n            if not stack:\n                i += 1\n                continue\n            j = stack.pop()\n            # we've made it back out of the expression! yield the span\n            if not stack:\n                yield (j, i + 1)\n            i += 1\n            continue\n\n        # if we're in an expression part of the f-string, fast-forward through strings\n        # note that backslashes are not legal in the expression portion of f-strings\n        if stack:\n            delim = None\n            if s[i : i + 3] in (\"'''\", '\"\"\"'):\n                delim = s[i : i + 3]\n            elif s[i] in (\"'\", '\"'):\n                delim = s[i]\n            if delim:\n                i += len(delim)\n                while i < len(s) and s[i : i + len(delim)] != delim:\n                    i += 1\n                i += len(delim)\n                continue\n        i += 1",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 44,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_toggle_fexpr_quotes",
      "sourceCode": "def _toggle_fexpr_quotes(fstring: str, old_quote: str) -> str:\n    \"\"\"\n    Toggles quotes used in f-string expressions that are `old_quote`.\n\n    f-string expressions can't contain backslashes, so we need to toggle the\n    quotes if the f-string itself will end up using the same quote. We can\n    simply toggle without escaping because, quotes can't be reused in f-string\n    expressions. They will fail to parse.\n\n    NOTE: If PEP 701 is accepted, above statement will no longer be true.\n    Though if quotes can be reused, we can simply reuse them without updates or\n    escaping, once Black figures out how to parse the new grammar.\n    \"\"\"\n    new_quote = \"'\" if old_quote == '\"' else '\"'\n    parts = []\n    previous_index = 0\n    for start, end in iter_fexpr_spans(fstring):\n        parts.append(fstring[previous_index:start])\n        parts.append(fstring[start:end].replace(old_quote, new_quote))\n        previous_index = end\n    parts.append(fstring[previous_index:])\n    return \"\".join(parts)",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 21,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_splitter_match",
      "sourceCode": "def do_splitter_match(self, line: Line) -> TMatchResult:\n        LL = line.leaves\n\n        if self._prefer_paren_wrap_match(LL) is not None:\n            return TErr(\"Line needs to be wrapped in parens first.\")\n\n        is_valid_index = is_valid_index_factory(LL)\n\n        idx = 0\n\n        # The first two leaves MAY be the 'not in' keywords...\n        if (\n            is_valid_index(idx)\n            and is_valid_index(idx + 1)\n            and [LL[idx].type, LL[idx + 1].type] == [token.NAME, token.NAME]\n            and str(LL[idx]) + str(LL[idx + 1]) == \"not in\"\n        ):\n            idx += 2\n        # Else the first leaf MAY be a string operator symbol or the 'in' keyword...\n        elif is_valid_index(idx) and (\n            LL[idx].type in self.STRING_OPERATORS\n            or LL[idx].type == token.NAME\n            and str(LL[idx]) == \"in\"\n        ):\n            idx += 1\n\n        # The next/first leaf MAY be an empty LPAR...\n        if is_valid_index(idx) and is_empty_lpar(LL[idx]):\n            idx += 1\n\n        # The next/first leaf MUST be a string...\n        if not is_valid_index(idx) or LL[idx].type != token.STRING:\n            return TErr(\"Line does not start with a string.\")\n\n        string_idx = idx\n\n        # Skip the string trailer, if one exists.\n        string_parser = StringParser()\n        idx = string_parser.parse(LL, string_idx)\n\n        # That string MAY be followed by an empty RPAR...\n        if is_valid_index(idx) and is_empty_rpar(LL[idx]):\n            idx += 1\n\n        # That string / empty RPAR leaf MAY be followed by a comma...\n        if is_valid_index(idx) and LL[idx].type == token.COMMA:\n            idx += 1\n\n        # But no more leaves are allowed...\n        if is_valid_index(idx):\n            return TErr(\"This line does not end with a string.\")\n\n        return Ok([string_idx])",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 52,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_transform",
      "sourceCode": "def do_transform(\n        self, line: Line, string_indices: list[int]\n    ) -> Iterator[TResult[Line]]:\n        LL = line.leaves\n        assert len(string_indices) == 1, (\n            f\"{self.__class__.__name__} should only find one match at a time, found\"\n            f\" {len(string_indices)}\"\n        )\n        string_idx = string_indices[0]\n\n        QUOTE = LL[string_idx].value[-1]\n\n        is_valid_index = is_valid_index_factory(LL)\n        insert_str_child = insert_str_child_factory(LL[string_idx])\n\n        prefix = get_string_prefix(LL[string_idx].value).lower()\n\n        # We MAY choose to drop the 'f' prefix from substrings that don't\n        # contain any f-expressions, but ONLY if the original f-string\n        # contains at least one f-expression. Otherwise, we will alter the AST\n        # of the program.\n        drop_pointless_f_prefix = (\"f\" in prefix) and fstring_contains_expr(\n            LL[string_idx].value\n        )\n\n        first_string_line = True\n\n        string_op_leaves = self._get_string_operator_leaves(LL)\n        string_op_leaves_length = (\n            sum(len(str(prefix_leaf)) for prefix_leaf in string_op_leaves) + 1\n            if string_op_leaves\n            else 0\n        )\n\n        def maybe_append_string_operators(new_line: Line) -> None:\n            \"\"\"\n            Side Effects:\n                If @line starts with a string operator and this is the first\n                line we are constructing, this function appends the string\n                operator to @new_line and replaces the old string operator leaf\n                in the node structure. Otherwise this function does nothing.\n            \"\"\"\n            maybe_prefix_leaves = string_op_leaves if first_string_line else []\n            for i, prefix_leaf in enumerate(maybe_prefix_leaves):\n                replace_child(LL[i], prefix_leaf)\n                new_line.append(prefix_leaf)\n\n        ends_with_comma = (\n            is_valid_index(string_idx + 1) and LL[string_idx + 1].type == token.COMMA\n        )\n\n        def max_last_string_column() -> int:\n            \"\"\"\n            Returns:\n                The max allowed width of the string value used for the last\n                line we will construct.  Note that this value means the width\n                rather than the number of characters (e.g., many East Asian\n                characters expand to two columns).\n            \"\"\"\n            result = self.line_length\n            result -= line.depth * 4\n            result -= 1 if ends_with_comma else 0\n            result -= string_op_leaves_length\n            return result\n\n        # --- Calculate Max Break Width (for string value)\n        # We start with the line length limit\n        max_break_width = self.line_length\n        # The last index of a string of length N is N-1.\n        max_break_width -= 1\n        # Leading whitespace is not present in the string value (e.g. Leaf.value).\n        max_break_width -= line.depth * 4\n        if max_break_width < 0:\n            yield TErr(\n                f\"Unable to split {LL[string_idx].value} at such high of a line depth:\"\n                f\" {line.depth}\"\n            )\n            return\n\n        # Check if StringMerger registered any custom splits.\n        custom_splits = self.pop_custom_splits(LL[string_idx].value)\n        # We use them ONLY if none of them would produce lines that exceed the\n        # line limit.\n        use_custom_breakpoints = bool(\n            custom_splits\n            and all(csplit.break_idx <= max_break_width for csplit in custom_splits)\n        )\n\n        # Temporary storage for the remaining chunk of the string line that\n        # can't fit onto the line currently being constructed.\n        rest_value = LL[string_idx].value\n\n        def more_splits_should_be_made() -> bool:\n            \"\"\"\n            Returns:\n                True iff `rest_value` (the remaining string value from the last\n                split), should be split again.\n            \"\"\"\n            if use_custom_breakpoints:\n                return len(custom_splits) > 1\n            else:\n                return str_width(rest_value) > max_last_string_column()\n\n        string_line_results: list[Ok[Line]] = []\n        while more_splits_should_be_made():\n            if use_custom_breakpoints:\n                # Custom User Split (manual)\n                csplit = custom_splits.pop(0)\n                break_idx = csplit.break_idx\n            else:\n                # Algorithmic Split (automatic)\n                max_bidx = (\n                    count_chars_in_width(rest_value, max_break_width)\n                    - string_op_leaves_length\n                )\n                maybe_break_idx = self._get_break_idx(rest_value, max_bidx)\n                if maybe_break_idx is None:\n                    # If we are unable to algorithmically determine a good split\n                    # and this string has custom splits registered to it, we\n                    # fall back to using them--which means we have to start\n                    # over from the beginning.\n                    if custom_splits:\n                        rest_value = LL[string_idx].value\n                        string_line_results = []\n                        first_string_line = True\n                        use_custom_breakpoints = True\n                        continue\n\n                    # Otherwise, we stop splitting here.\n                    break\n\n                break_idx = maybe_break_idx\n\n            # --- Construct `next_value`\n            next_value = rest_value[:break_idx] + QUOTE\n\n            # HACK: The following 'if' statement is a hack to fix the custom\n            # breakpoint index in the case of either: (a) substrings that were\n            # f-strings but will have the 'f' prefix removed OR (b) substrings\n            # that were not f-strings but will now become f-strings because of\n            # redundant use of the 'f' prefix (i.e. none of the substrings\n            # contain f-expressions but one or more of them had the 'f' prefix\n            # anyway; in which case, we will prepend 'f' to _all_ substrings).\n            #\n            # There is probably a better way to accomplish what is being done\n            # here...\n            #\n            # If this substring is an f-string, we _could_ remove the 'f'\n            # prefix, and the current custom split did NOT originally use a\n            # prefix...\n            if (\n                use_custom_breakpoints\n                and not csplit.has_prefix\n                and (\n                    # `next_value == prefix + QUOTE` happens when the custom\n                    # split is an empty string.\n                    next_value == prefix + QUOTE\n                    or next_value != self._normalize_f_string(next_value, prefix)\n                )\n            ):\n                # Then `csplit.break_idx` will be off by one after removing\n                # the 'f' prefix.\n                break_idx += 1\n                next_value = rest_value[:break_idx] + QUOTE\n\n            if drop_pointless_f_prefix:\n                next_value = self._normalize_f_string(next_value, prefix)\n\n            # --- Construct `next_leaf`\n            next_leaf = Leaf(token.STRING, next_value)\n            insert_str_child(next_leaf)\n            self._maybe_normalize_string_quotes(next_leaf)\n\n            # --- Construct `next_line`\n            next_line = line.clone()\n            maybe_append_string_operators(next_line)\n            next_line.append(next_leaf)\n            string_line_results.append(Ok(next_line))\n\n            rest_value = prefix + QUOTE + rest_value[break_idx:]\n            first_string_line = False\n\n        yield from string_line_results\n\n        if drop_pointless_f_prefix:\n            rest_value = self._normalize_f_string(rest_value, prefix)\n\n        rest_leaf = Leaf(token.STRING, rest_value)\n        insert_str_child(rest_leaf)\n\n        # NOTE: I could not find a test case that verifies that the following\n        # line is actually necessary, but it seems to be. Otherwise we risk\n        # not normalizing the last substring, right?\n        self._maybe_normalize_string_quotes(rest_leaf)\n\n        last_line = line.clone()\n        maybe_append_string_operators(last_line)\n\n        # If there are any leaves to the right of the target string...\n        if is_valid_index(string_idx + 1):\n            # We use `temp_value` here to determine how long the last line\n            # would be if we were to append all the leaves to the right of the\n            # target string to the last string line.\n            temp_value = rest_value\n            for leaf in LL[string_idx + 1 :]:\n                temp_value += str(leaf)\n                if leaf.type == token.LPAR:\n                    break\n\n            # Try to fit them all on the same line with the last substring...\n            if (\n                str_width(temp_value) <= max_last_string_column()\n                or LL[string_idx + 1].type == token.COMMA\n            ):\n                last_line.append(rest_leaf)\n                append_leaves(last_line, line, LL[string_idx + 1 :])\n                yield Ok(last_line)\n            # Otherwise, place the last substring on one line and everything\n            # else on a line below that...\n            else:\n                last_line.append(rest_leaf)\n                yield Ok(last_line)\n\n                non_string_line = line.clone()\n                append_leaves(non_string_line, line, LL[string_idx + 1 :])\n                yield Ok(non_string_line)\n        # Else the target string was the last leaf...\n        else:\n            last_line.append(rest_leaf)\n            last_line.comments = line.comments.copy()\n            yield Ok(last_line)",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 230,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "maybe_append_string_operators",
      "sourceCode": "def maybe_append_string_operators(new_line: Line) -> None:\n            \"\"\"\n            Side Effects:\n                If @line starts with a string operator and this is the first\n                line we are constructing, this function appends the string\n                operator to @new_line and replaces the old string operator leaf\n                in the node structure. Otherwise this function does nothing.\n            \"\"\"\n            maybe_prefix_leaves = string_op_leaves if first_string_line else []\n            for i, prefix_leaf in enumerate(maybe_prefix_leaves):\n                replace_child(LL[i], prefix_leaf)\n                new_line.append(prefix_leaf)",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 11,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "max_last_string_column",
      "sourceCode": "def max_last_string_column() -> int:\n            \"\"\"\n            Returns:\n                The max allowed width of the string value used for the last\n                line we will construct.  Note that this value means the width\n                rather than the number of characters (e.g., many East Asian\n                characters expand to two columns).\n            \"\"\"\n            result = self.line_length\n            result -= line.depth * 4\n            result -= 1 if ends_with_comma else 0\n            result -= string_op_leaves_length\n            return result",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 12,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_iter_nameescape_slices",
      "sourceCode": "def _iter_nameescape_slices(self, string: str) -> Iterator[tuple[Index, Index]]:\n        \"\"\"\n        Yields:\n            All ranges of @string which, if @string were to be split there,\n            would result in the splitting of an \\\\N{...} expression (which is NOT\n            allowed).\n        \"\"\"\n        # True - the previous backslash was unescaped\n        # False - the previous backslash was escaped *or* there was no backslash\n        previous_was_unescaped_backslash = False\n        it = iter(enumerate(string))\n        for idx, c in it:\n            if c == \"\\\\\":\n                previous_was_unescaped_backslash = not previous_was_unescaped_backslash\n                continue\n            if not previous_was_unescaped_backslash or c != \"N\":\n                previous_was_unescaped_backslash = False\n                continue\n            previous_was_unescaped_backslash = False\n\n            begin = idx - 1  # the position of backslash before \\N{...}\n            for idx, c in it:\n                if c == \"}\":\n                    end = idx\n                    break\n            else:\n                # malformed nameescape expression?\n                # should have been detected by AST parsing earlier...\n                raise RuntimeError(f\"{self.__class__.__name__} LOGIC ERROR!\")\n            yield begin, end",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 29,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_get_break_idx",
      "sourceCode": "def _get_break_idx(self, string: str, max_break_idx: int) -> Optional[int]:\n        \"\"\"\n        This method contains the algorithm that StringSplitter uses to\n        determine which character to split each string at.\n\n        Args:\n            @string: The substring that we are attempting to split.\n            @max_break_idx: The ideal break index. We will return this value if it\n            meets all the necessary conditions. In the likely event that it\n            doesn't we will try to find the closest index BELOW @max_break_idx\n            that does. If that fails, we will expand our search by also\n            considering all valid indices ABOVE @max_break_idx.\n\n        Pre-Conditions:\n            * assert_is_leaf_string(@string)\n            * 0 <= @max_break_idx < len(@string)\n\n        Returns:\n            break_idx, if an index is able to be found that meets all of the\n            conditions listed in the 'Transformations' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        is_valid_index = is_valid_index_factory(string)\n\n        assert is_valid_index(max_break_idx)\n        assert_is_leaf_string(string)\n\n        _illegal_split_indices = self._get_illegal_split_indices(string)\n\n        def breaks_unsplittable_expression(i: Index) -> bool:\n            \"\"\"\n            Returns:\n                True iff returning @i would result in the splitting of an\n                unsplittable expression (which is NOT allowed).\n            \"\"\"\n            return i in _illegal_split_indices\n\n        def passes_all_checks(i: Index) -> bool:\n            \"\"\"\n            Returns:\n                True iff ALL of the conditions listed in the 'Transformations'\n                section of this classes' docstring would be met by returning @i.\n            \"\"\"\n            is_space = string[i] == \" \"\n            is_split_safe = is_valid_index(i - 1) and string[i - 1] in SPLIT_SAFE_CHARS\n\n            is_not_escaped = True\n            j = i - 1\n            while is_valid_index(j) and string[j] == \"\\\\\":\n                is_not_escaped = not is_not_escaped\n                j -= 1\n\n            is_big_enough = (\n                len(string[i:]) >= self.MIN_SUBSTR_SIZE\n                and len(string[:i]) >= self.MIN_SUBSTR_SIZE\n            )\n            return (\n                (is_space or is_split_safe)\n                and is_not_escaped\n                and is_big_enough\n                and not breaks_unsplittable_expression(i)\n            )\n\n        # First, we check all indices BELOW @max_break_idx.\n        break_idx = max_break_idx\n        while is_valid_index(break_idx - 1) and not passes_all_checks(break_idx):\n            break_idx -= 1\n\n        if not passes_all_checks(break_idx):\n            # If that fails, we check all indices ABOVE @max_break_idx.\n            #\n            # If we are able to find a valid index here, the next line is going\n            # to be longer than the specified line length, but it's probably\n            # better than doing nothing at all.\n            break_idx = max_break_idx + 1\n            while is_valid_index(break_idx + 1) and not passes_all_checks(break_idx):\n                break_idx += 1\n\n            if not is_valid_index(break_idx) or not passes_all_checks(break_idx):\n                return None\n\n        return break_idx",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 83,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "passes_all_checks",
      "sourceCode": "def passes_all_checks(i: Index) -> bool:\n            \"\"\"\n            Returns:\n                True iff ALL of the conditions listed in the 'Transformations'\n                section of this classes' docstring would be met by returning @i.\n            \"\"\"\n            is_space = string[i] == \" \"\n            is_split_safe = is_valid_index(i - 1) and string[i - 1] in SPLIT_SAFE_CHARS\n\n            is_not_escaped = True\n            j = i - 1\n            while is_valid_index(j) and string[j] == \"\\\\\":\n                is_not_escaped = not is_not_escaped\n                j -= 1\n\n            is_big_enough = (\n                len(string[i:]) >= self.MIN_SUBSTR_SIZE\n                and len(string[:i]) >= self.MIN_SUBSTR_SIZE\n            )\n            return (\n                (is_space or is_split_safe)\n                and is_not_escaped\n                and is_big_enough\n                and not breaks_unsplittable_expression(i)\n            )",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 24,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_normalize_f_string",
      "sourceCode": "def _normalize_f_string(self, string: str, prefix: str) -> str:\n        \"\"\"\n        Pre-Conditions:\n            * assert_is_leaf_string(@string)\n\n        Returns:\n            * If @string is an f-string that contains no f-expressions, we\n            return a string identical to @string except that the 'f' prefix\n            has been stripped and all double braces (i.e. '{{' or '}}') have\n            been normalized (i.e. turned into '{' or '}').\n                OR\n            * Otherwise, we return @string.\n        \"\"\"\n        assert_is_leaf_string(string)\n\n        if \"f\" in prefix and not fstring_contains_expr(string):\n            new_prefix = prefix.replace(\"f\", \"\")\n\n            temp = string[len(prefix) :]\n            temp = re.sub(r\"\\{\\{\", \"{\", temp)\n            temp = re.sub(r\"\\}\\}\", \"}\", temp)\n            new_string = temp\n\n            return f\"{new_prefix}{new_string}\"\n        else:\n            return string",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 25,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_splitter_match",
      "sourceCode": "def do_splitter_match(self, line: Line) -> TMatchResult:\n        LL = line.leaves\n\n        if line.leaves[-1].type in OPENING_BRACKETS:\n            return TErr(\n                \"Cannot wrap parens around a line that ends in an opening bracket.\"\n            )\n\n        string_idx = (\n            self._return_match(LL)\n            or self._else_match(LL)\n            or self._assert_match(LL)\n            or self._assign_match(LL)\n            or self._dict_or_lambda_match(LL)\n            or self._prefer_paren_wrap_match(LL)\n        )\n\n        if string_idx is not None:\n            string_value = line.leaves[string_idx].value\n            # If the string has neither spaces nor East Asian stops...\n            if not any(\n                char == \" \" or char in SPLIT_SAFE_CHARS for char in string_value\n            ):\n                # And will still violate the line length limit when split...\n                max_string_width = self.line_length - ((line.depth + 1) * 4)\n                if str_width(string_value) > max_string_width:\n                    # And has no associated custom splits...\n                    if not self.has_custom_splits(string_value):\n                        # Then we should NOT put this string on its own line.\n                        return TErr(\n                            \"We do not wrap long strings in parentheses when the\"\n                            \" resultant line would still be over the specified line\"\n                            \" length and can't be split further by StringSplitter.\"\n                        )\n            return Ok([string_idx])\n\n        return TErr(\"This line does not contain any non-atomic strings.\")",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 36,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_return_match",
      "sourceCode": "@staticmethod\n    def _return_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the return/yield statement\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of a return/yield statement and the first leaf\n        # contains either the \"return\" or \"yield\" keywords...\n        if parent_type(LL[0]) in [syms.return_stmt, syms.yield_expr] and LL[\n            0\n        ].value in [\"return\", \"yield\"]:\n            is_valid_index = is_valid_index_factory(LL)\n\n            idx = 2 if is_valid_index(1) and is_empty_par(LL[1]) else 1\n            # The next visible leaf MUST contain a string...\n            if is_valid_index(idx) and LL[idx].type == token.STRING:\n                return idx\n\n        return None",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 23,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_else_match",
      "sourceCode": "@staticmethod\n    def _else_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the ternary expression\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of a ternary expression and the first leaf\n        # contains the \"else\" keyword...\n        if (\n            parent_type(LL[0]) == syms.test\n            and LL[0].type == token.NAME\n            and LL[0].value == \"else\"\n        ):\n            is_valid_index = is_valid_index_factory(LL)\n\n            idx = 2 if is_valid_index(1) and is_empty_par(LL[1]) else 1\n            # The next visible leaf MUST contain a string...\n            if is_valid_index(idx) and LL[idx].type == token.STRING:\n                return idx\n\n        return None",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 25,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_assert_match",
      "sourceCode": "@staticmethod\n    def _assert_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the assert statement\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of an assert statement and the first leaf\n        # contains the \"assert\" keyword...\n        if parent_type(LL[0]) == syms.assert_stmt and LL[0].value == \"assert\":\n            is_valid_index = is_valid_index_factory(LL)\n\n            for i, leaf in enumerate(LL):\n                # We MUST find a comma...\n                if leaf.type == token.COMMA:\n                    idx = i + 2 if is_empty_par(LL[i + 1]) else i + 1\n\n                    # That comma MUST be followed by a string...\n                    if is_valid_index(idx) and LL[idx].type == token.STRING:\n                        string_idx = idx\n\n                        # Skip the string trailer, if one exists.\n                        string_parser = StringParser()\n                        idx = string_parser.parse(LL, string_idx)\n\n                        # But no more leaves are allowed...\n                        if not is_valid_index(idx):\n                            return string_idx\n\n        return None",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 33,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_assign_match",
      "sourceCode": "@staticmethod\n    def _assign_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the assignment statement\n            requirements listed in the 'Requirements' section of this classes'\n            docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of an expression statement or is a function\n        # argument AND the first leaf contains a variable name...\n        if (\n            parent_type(LL[0]) in [syms.expr_stmt, syms.argument, syms.power]\n            and LL[0].type == token.NAME\n        ):\n            is_valid_index = is_valid_index_factory(LL)\n\n            for i, leaf in enumerate(LL):\n                # We MUST find either an '=' or '+=' symbol...\n                if leaf.type in [token.EQUAL, token.PLUSEQUAL]:\n                    idx = i + 2 if is_empty_par(LL[i + 1]) else i + 1\n\n                    # That symbol MUST be followed by a string...\n                    if is_valid_index(idx) and LL[idx].type == token.STRING:\n                        string_idx = idx\n\n                        # Skip the string trailer, if one exists.\n                        string_parser = StringParser()\n                        idx = string_parser.parse(LL, string_idx)\n\n                        # The next leaf MAY be a comma iff this line is a part\n                        # of a function argument...\n                        if (\n                            parent_type(LL[0]) == syms.argument\n                            and is_valid_index(idx)\n                            and LL[idx].type == token.COMMA\n                        ):\n                            idx += 1\n\n                        # But no more leaves are allowed...\n                        if not is_valid_index(idx):\n                            return string_idx\n\n        return None",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 45,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_dict_or_lambda_match",
      "sourceCode": "@staticmethod\n    def _dict_or_lambda_match(LL: list[Leaf]) -> Optional[int]:\n        \"\"\"\n        Returns:\n            string_idx such that @LL[string_idx] is equal to our target (i.e.\n            matched) string, if this line matches the dictionary key assignment\n            statement or lambda expression requirements listed in the\n            'Requirements' section of this classes' docstring.\n                OR\n            None, otherwise.\n        \"\"\"\n        # If this line is a part of a dictionary key assignment or lambda expression...\n        parent_types = [parent_type(LL[0]), parent_type(LL[0].parent)]\n        if syms.dictsetmaker in parent_types or syms.lambdef in parent_types:\n            is_valid_index = is_valid_index_factory(LL)\n\n            for i, leaf in enumerate(LL):\n                # We MUST find a colon, it can either be dict's or lambda's colon...\n                if leaf.type == token.COLON and i < len(LL) - 1:\n                    idx = i + 2 if is_empty_par(LL[i + 1]) else i + 1\n\n                    # That colon MUST be followed by a string...\n                    if is_valid_index(idx) and LL[idx].type == token.STRING:\n                        string_idx = idx\n\n                        # Skip the string trailer, if one exists.\n                        string_parser = StringParser()\n                        idx = string_parser.parse(LL, string_idx)\n\n                        # That string MAY be followed by a comma...\n                        if is_valid_index(idx) and LL[idx].type == token.COMMA:\n                            idx += 1\n\n                        # But no more leaves are allowed...\n                        if not is_valid_index(idx):\n                            return string_idx\n\n        return None",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 37,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "do_transform",
      "sourceCode": "def do_transform(\n        self, line: Line, string_indices: list[int]\n    ) -> Iterator[TResult[Line]]:\n        LL = line.leaves\n        assert len(string_indices) == 1, (\n            f\"{self.__class__.__name__} should only find one match at a time, found\"\n            f\" {len(string_indices)}\"\n        )\n        string_idx = string_indices[0]\n\n        is_valid_index = is_valid_index_factory(LL)\n        insert_str_child = insert_str_child_factory(LL[string_idx])\n\n        comma_idx = -1\n        ends_with_comma = False\n        if LL[comma_idx].type == token.COMMA:\n            ends_with_comma = True\n\n        leaves_to_steal_comments_from = [LL[string_idx]]\n        if ends_with_comma:\n            leaves_to_steal_comments_from.append(LL[comma_idx])\n\n        # --- First Line\n        first_line = line.clone()\n        left_leaves = LL[:string_idx]\n\n        # We have to remember to account for (possibly invisible) LPAR and RPAR\n        # leaves that already wrapped the target string. If these leaves do\n        # exist, we will replace them with our own LPAR and RPAR leaves.\n        old_parens_exist = False\n        if left_leaves and left_leaves[-1].type == token.LPAR:\n            old_parens_exist = True\n            leaves_to_steal_comments_from.append(left_leaves[-1])\n            left_leaves.pop()\n\n        append_leaves(first_line, line, left_leaves)\n\n        lpar_leaf = Leaf(token.LPAR, \"(\")\n        if old_parens_exist:\n            replace_child(LL[string_idx - 1], lpar_leaf)\n        else:\n            insert_str_child(lpar_leaf)\n        first_line.append(lpar_leaf)\n\n        # We throw inline comments that were originally to the right of the\n        # target string to the top line. They will now be shown to the right of\n        # the LPAR.\n        for leaf in leaves_to_steal_comments_from:\n            for comment_leaf in line.comments_after(leaf):\n                first_line.append(comment_leaf, preformatted=True)\n\n        yield Ok(first_line)\n\n        # --- Middle (String) Line\n        # We only need to yield one (possibly too long) string line, since the\n        # `StringSplitter` will break it down further if necessary.\n        string_value = LL[string_idx].value\n        string_line = Line(\n            mode=line.mode,\n            depth=line.depth + 1,\n            inside_brackets=True,\n            should_split_rhs=line.should_split_rhs,\n            magic_trailing_comma=line.magic_trailing_comma,\n        )\n        string_leaf = Leaf(token.STRING, string_value)\n        insert_str_child(string_leaf)\n        string_line.append(string_leaf)\n\n        old_rpar_leaf = None\n        if is_valid_index(string_idx + 1):\n            right_leaves = LL[string_idx + 1 :]\n            if ends_with_comma:\n                right_leaves.pop()\n\n            if old_parens_exist:\n                assert right_leaves and right_leaves[-1].type == token.RPAR, (\n                    \"Apparently, old parentheses do NOT exist?!\"\n                    f\" (left_leaves={left_leaves}, right_leaves={right_leaves})\"\n                )\n                old_rpar_leaf = right_leaves.pop()\n            elif right_leaves and right_leaves[-1].type == token.RPAR:\n                # Special case for lambda expressions as dict's value, e.g.:\n                #     my_dict = {\n                #        \"key\": lambda x: f\"formatted: {x},\n                #     }\n                # After wrapping the dict's value with parentheses, the string is\n                # followed by a RPAR but its opening bracket is lambda's, not\n                # the string's:\n                #        \"key\": (lambda x: f\"formatted: {x}),\n                opening_bracket = right_leaves[-1].opening_bracket\n                if opening_bracket is not None and opening_bracket in left_leaves:\n                    index = left_leaves.index(opening_bracket)\n                    if (\n                        0 < index < len(left_leaves) - 1\n                        and left_leaves[index - 1].type == token.COLON\n                        and left_leaves[index + 1].value == \"lambda\"\n                    ):\n                        right_leaves.pop()\n\n            append_leaves(string_line, line, right_leaves)\n\n        yield Ok(string_line)\n\n        # --- Last Line\n        last_line = line.clone()\n        last_line.bracket_tracker = first_line.bracket_tracker\n\n        new_rpar_leaf = Leaf(token.RPAR, \")\")\n        if old_rpar_leaf is not None:\n            replace_child(old_rpar_leaf, new_rpar_leaf)\n        else:\n            insert_str_child(new_rpar_leaf)\n        last_line.append(new_rpar_leaf)\n\n        # If the target string ended with a comma, we place this comma to the\n        # right of the RPAR on the last line.\n        if ends_with_comma:\n            comma_leaf = Leaf(token.COMMA, \",\")\n            replace_child(LL[comma_idx], comma_leaf)\n            last_line.append(comma_leaf)\n\n        yield Ok(last_line)",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 121,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "parse",
      "sourceCode": "def parse(self, leaves: list[Leaf], string_idx: int) -> int:\n        \"\"\"\n        Pre-conditions:\n            * @leaves[@string_idx].type == token.STRING\n\n        Returns:\n            The index directly after the last leaf which is a part of the string\n            trailer, if a \"trailer\" exists.\n            OR\n            @string_idx + 1, if no string \"trailer\" exists.\n        \"\"\"\n        assert leaves[string_idx].type == token.STRING\n\n        idx = string_idx + 1\n        while idx < len(leaves) and self._next_state(leaves[idx]):\n            idx += 1\n        return idx",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 16,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "_next_state",
      "sourceCode": "def _next_state(self, leaf: Leaf) -> bool:\n        \"\"\"\n        Pre-conditions:\n            * On the first call to this function, @leaf MUST be the leaf that\n              was directly after the string leaf in question (e.g. if our target\n              string is `line.leaves[i]` then the first call to this method must\n              be `line.leaves[i + 1]`).\n            * On the next call to this function, the leaf parameter passed in\n              MUST be the leaf directly following @leaf.\n\n        Returns:\n            True iff @leaf is a part of the string's trailer.\n        \"\"\"\n        # We ignore empty LPAR or RPAR leaves.\n        if is_empty_par(leaf):\n            return True\n\n        next_token = leaf.type\n        if next_token == token.LPAR:\n            self._unmatched_lpars += 1\n\n        current_state = self._state\n\n        # The LPAR parser state is a special case. We will return True until we\n        # find the matching RPAR token.\n        if current_state == self.LPAR:\n            if next_token == token.RPAR:\n                self._unmatched_lpars -= 1\n                if self._unmatched_lpars == 0:\n                    self._state = self.RPAR\n        # Otherwise, we use a lookup table to determine the next state.\n        else:\n            # If the lookup table matches the current state to the next\n            # token, we use the lookup table.\n            if (current_state, next_token) in self._goto:\n                self._state = self._goto[current_state, next_token]\n            else:\n                # Otherwise, we check if a the current state was assigned a\n                # default.\n                if (current_state, self.DEFAULT_TOKEN) in self._goto:\n                    self._state = self._goto[current_state, self.DEFAULT_TOKEN]\n                # If no default has been assigned, then this parser has a logic\n                # error.\n                else:\n                    raise RuntimeError(f\"{self.__class__.__name__} LOGIC ERROR!\")\n\n            if self._state == self.DONE:\n                return False\n\n        return True",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 49,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "insert_str_child_factory",
      "sourceCode": "def insert_str_child_factory(string_leaf: Leaf) -> Callable[[LN], None]:\n    \"\"\"\n    Factory for a convenience function that is used to orphan @string_leaf\n    and then insert multiple new leaves into the same part of the node\n    structure that @string_leaf had originally occupied.\n\n    Examples:\n        Let `string_leaf = Leaf(token.STRING, '\"foo\"')` and `N =\n        string_leaf.parent`. Assume the node `N` has the following\n        original structure:\n\n        Node(\n            expr_stmt, [\n                Leaf(NAME, 'x'),\n                Leaf(EQUAL, '='),\n                Leaf(STRING, '\"foo\"'),\n            ]\n        )\n\n        We then run the code snippet shown below.\n        ```\n        insert_str_child = insert_str_child_factory(string_leaf)\n\n        lpar = Leaf(token.LPAR, '(')\n        insert_str_child(lpar)\n\n        bar = Leaf(token.STRING, '\"bar\"')\n        insert_str_child(bar)\n\n        rpar = Leaf(token.RPAR, ')')\n        insert_str_child(rpar)\n        ```\n\n        After which point, it follows that `string_leaf.parent is None` and\n        the node `N` now has the following structure:\n\n        Node(\n            expr_stmt, [\n                Leaf(NAME, 'x'),\n                Leaf(EQUAL, '='),\n                Leaf(LPAR, '('),\n                Leaf(STRING, '\"bar\"'),\n                Leaf(RPAR, ')'),\n            ]\n        )\n    \"\"\"\n    string_parent = string_leaf.parent\n    string_child_idx = string_leaf.remove()\n\n    def insert_str_child(child: LN) -> None:\n        nonlocal string_child_idx\n\n        assert string_parent is not None\n        assert string_child_idx is not None\n\n        string_parent.insert_child(string_child_idx, child)\n        string_child_idx += 1\n\n    return insert_str_child",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 58,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "is_valid_index_factory",
      "sourceCode": "def is_valid_index_factory(seq: Sequence[Any]) -> Callable[[int], bool]:\n    \"\"\"\n    Examples:\n        ```\n        my_list = [1, 2, 3]\n\n        is_valid_index = is_valid_index_factory(my_list)\n\n        assert is_valid_index(0)\n        assert is_valid_index(2)\n\n        assert not is_valid_index(3)\n        assert not is_valid_index(-1)\n        ```\n    \"\"\"\n\n    def is_valid_index(idx: int) -> bool:\n        \"\"\"\n        Returns:\n            True iff @idx is positive AND seq[@idx] does NOT raise an\n            IndexError.\n        \"\"\"\n        return 0 <= idx < len(seq)\n\n    return is_valid_index",
      "importString": "import re\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import Callable, Collection, Iterable, Iterator, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Final, Literal, Optional, TypeVar, Union\n\nfrom mypy_extensions import trait\n\nfrom black.comments import contains_pragma_comment\nfrom black.lines import Line, append_leaves\nfrom black.mode import Feature, Mode, Preview\nfrom black.nodes import (\nCLOSING_BRACKETS\nOPENING_BRACKETS\nSTANDALONE_COMMENT\nis_empty_lpar\nis_empty_par\nis_empty_rpar\nis_part_of_annotation\nparent_type\nreplace_child\nsyms\n)\nfrom black.rusty import Err, Ok, Result\nfrom black.strings import (\nassert_is_leaf_string\ncount_chars_in_width\nget_string_prefix\nhas_triple_quotes\nnormalize_string_quotes\nstr_width\n)\nfrom blib2to3.pgen2 import token\nfrom blib2to3.pytree import Leaf, Node",
      "lineNum": 24,
      "relativeDocumentPath": "src/black/trans.py"
    },
    {
      "symbolName": "main",
      "sourceCode": "@click.command(context_settings={\"help_option_names\": [\"-h\", \"--help\"]})\n@click.option(\n    \"--bind-host\",\n    type=str,\n    help=\"Address to bind the server to.\",\n    default=\"localhost\",\n    show_default=True,\n)\n@click.option(\n    \"--bind-port\", type=int, help=\"Port to listen on\", default=45484, show_default=True\n)\n@click.version_option(version=black.__version__)\ndef main(bind_host: str, bind_port: int) -> None:\n    logging.basicConfig(level=logging.INFO)\n    app = make_app()\n    ver = black.__version__\n    black.out(f\"blackd version {ver} listening on {bind_host} port {bind_port}\")\n    web.run_app(app, host=bind_host, port=bind_port, handle_signals=True, print=None)",
      "importString": "import logging\nfrom concurrent.futures import Executor, ProcessPoolExecutor\nfrom datetime import datetime, timezone\nfrom functools import partial\nfrom multiprocessing import freeze_support\nimport click\nimport black\nfrom _black_version import version as __version__\nfrom black.concurrency import maybe_install_uvloop",
      "lineNum": 17,
      "relativeDocumentPath": "src/blackd/__init__.py"
    },
    {
      "symbolName": "handle",
      "sourceCode": "async def handle(request: web.Request, executor: Executor) -> web.Response:\n    headers = {BLACK_VERSION_HEADER: __version__}\n    try:\n        if request.headers.get(PROTOCOL_VERSION_HEADER, \"1\") != \"1\":\n            return web.Response(\n                status=501, text=\"This server only supports protocol version 1\"\n            )\n\n        fast = False\n        if request.headers.get(FAST_OR_SAFE_HEADER, \"safe\") == \"fast\":\n            fast = True\n        try:\n            mode = parse_mode(request.headers)\n        except HeaderError as e:\n            return web.Response(status=400, text=e.args[0])\n        req_bytes = await request.content.read()\n        charset = request.charset if request.charset is not None else \"utf8\"\n        req_str = req_bytes.decode(charset)\n        then = datetime.now(timezone.utc)\n\n        header = \"\"\n        if mode.skip_source_first_line:\n            first_newline_position: int = req_str.find(\"\\n\") + 1\n            header = req_str[:first_newline_position]\n            req_str = req_str[first_newline_position:]\n\n        loop = asyncio.get_event_loop()\n        formatted_str = await loop.run_in_executor(\n            executor, partial(black.format_file_contents, req_str, fast=fast, mode=mode)\n        )\n\n        # Preserve CRLF line endings\n        nl = req_str.find(\"\\n\")\n        if nl > 0 and req_str[nl - 1] == \"\\r\":\n            formatted_str = formatted_str.replace(\"\\n\", \"\\r\\n\")\n            # If, after swapping line endings, nothing changed, then say so\n            if formatted_str == req_str:\n                raise black.NothingChanged\n\n        # Put the source first line back\n        req_str = header + req_str\n        formatted_str = header + formatted_str\n\n        # Only output the diff in the HTTP response\n        only_diff = bool(request.headers.get(DIFF_HEADER, False))\n        if only_diff:\n            now = datetime.now(timezone.utc)\n            src_name = f\"In\\t{then}\"\n            dst_name = f\"Out\\t{now}\"\n            loop = asyncio.get_event_loop()\n            formatted_str = await loop.run_in_executor(\n                executor,\n                partial(black.diff, req_str, formatted_str, src_name, dst_name),\n            )\n\n        return web.Response(\n            content_type=request.content_type,\n            charset=charset,\n            headers=headers,\n            text=formatted_str,\n        )\n    except black.NothingChanged:\n        return web.Response(status=204, headers=headers)\n    except black.InvalidInput as e:\n        return web.Response(status=400, headers=headers, text=str(e))\n    except Exception as e:\n        logging.exception(\"Exception during handling a request\")\n        return web.Response(status=500, headers=headers, text=str(e))",
      "importString": "import logging\nfrom concurrent.futures import Executor, ProcessPoolExecutor\nfrom datetime import datetime, timezone\nfrom functools import partial\nfrom multiprocessing import freeze_support\nimport click\nimport black\nfrom _black_version import version as __version__\nfrom black.concurrency import maybe_install_uvloop",
      "lineNum": 67,
      "relativeDocumentPath": "src/blackd/__init__.py"
    },
    {
      "symbolName": "parse_mode",
      "sourceCode": "def parse_mode(headers: MultiMapping[str]) -> black.Mode:\n    try:\n        line_length = int(headers.get(LINE_LENGTH_HEADER, black.DEFAULT_LINE_LENGTH))\n    except ValueError:\n        raise HeaderError(\"Invalid line length header value\") from None\n\n    if PYTHON_VARIANT_HEADER in headers:\n        value = headers[PYTHON_VARIANT_HEADER]\n        try:\n            pyi, versions = parse_python_variant_header(value)\n        except InvalidVariantHeader as e:\n            raise HeaderError(\n                f\"Invalid value for {PYTHON_VARIANT_HEADER}: {e.args[0]}\",\n            ) from None\n    else:\n        pyi = False\n        versions = set()\n\n    skip_string_normalization = bool(\n        headers.get(SKIP_STRING_NORMALIZATION_HEADER, False)\n    )\n    skip_magic_trailing_comma = bool(headers.get(SKIP_MAGIC_TRAILING_COMMA, False))\n    skip_source_first_line = bool(headers.get(SKIP_SOURCE_FIRST_LINE, False))\n\n    preview = bool(headers.get(PREVIEW, False))\n    unstable = bool(headers.get(UNSTABLE, False))\n    enable_features: set[black.Preview] = set()\n    enable_unstable_features = headers.get(ENABLE_UNSTABLE_FEATURE, \"\").split(\",\")\n    for piece in enable_unstable_features:\n        piece = piece.strip()\n        if piece:\n            try:\n                enable_features.add(black.Preview[piece])\n            except KeyError:\n                raise HeaderError(\n                    f\"Invalid value for {ENABLE_UNSTABLE_FEATURE}: {piece}\",\n                ) from None\n\n    return black.FileMode(\n        target_versions=versions,\n        is_pyi=pyi,\n        line_length=line_length,\n        skip_source_first_line=skip_source_first_line,\n        string_normalization=not skip_string_normalization,\n        magic_trailing_comma=not skip_magic_trailing_comma,\n        preview=preview,\n        unstable=unstable,\n        enabled_features=enable_features,\n    )",
      "importString": "import logging\nfrom concurrent.futures import Executor, ProcessPoolExecutor\nfrom datetime import datetime, timezone\nfrom functools import partial\nfrom multiprocessing import freeze_support\nimport click\nimport black\nfrom _black_version import version as __version__\nfrom black.concurrency import maybe_install_uvloop",
      "lineNum": 48,
      "relativeDocumentPath": "src/blackd/__init__.py"
    },
    {
      "symbolName": "parse_python_variant_header",
      "sourceCode": "def parse_python_variant_header(value: str) -> tuple[bool, set[black.TargetVersion]]:\n    if value == \"pyi\":\n        return True, set()\n    else:\n        versions = set()\n        for version in value.split(\",\"):\n            if version.startswith(\"py\"):\n                version = version[len(\"py\") :]\n            if \".\" in version:\n                major_str, *rest = version.split(\".\")\n            else:\n                major_str = version[0]\n                rest = [version[1:]] if len(version) > 1 else []\n            try:\n                major = int(major_str)\n                if major not in (2, 3):\n                    raise InvalidVariantHeader(\"major version must be 2 or 3\")\n                if len(rest) > 0:\n                    minor = int(rest[0])\n                    if major == 2:\n                        raise InvalidVariantHeader(\"Python 2 is not supported\")\n                else:\n                    # Default to lowest supported minor version.\n                    minor = 7 if major == 2 else 3\n                version_str = f\"PY{major}{minor}\"\n                if major == 3 and not hasattr(black.TargetVersion, version_str):\n                    raise InvalidVariantHeader(f\"3.{minor} is not supported\")\n                versions.add(black.TargetVersion[version_str])\n            except (KeyError, ValueError):\n                raise InvalidVariantHeader(\"expected e.g. '3.7', 'py3.5'\") from None\n        return False, versions",
      "importString": "import logging\nfrom concurrent.futures import Executor, ProcessPoolExecutor\nfrom datetime import datetime, timezone\nfrom functools import partial\nfrom multiprocessing import freeze_support\nimport click\nimport black\nfrom _black_version import version as __version__\nfrom black.concurrency import maybe_install_uvloop",
      "lineNum": 30,
      "relativeDocumentPath": "src/blackd/__init__.py"
    },
    {
      "symbolName": "cors",
      "sourceCode": "def cors(allow_headers: Iterable[str]) -> Middleware:\n    @middleware\n    async def impl(request: Request, handler: Handler) -> StreamResponse:\n        is_options = request.method == \"OPTIONS\"\n        is_preflight = is_options and \"Access-Control-Request-Method\" in request.headers\n        if is_preflight:\n            resp = StreamResponse()\n        else:\n            resp = await handler(request)\n\n        origin = request.headers.get(\"Origin\")\n        if not origin:\n            return resp\n\n        resp.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n        resp.headers[\"Access-Control-Expose-Headers\"] = \"*\"\n        if is_options:\n            resp.headers[\"Access-Control-Allow-Headers\"] = \", \".join(allow_headers)\n            resp.headers[\"Access-Control-Allow-Methods\"] = \", \".join(\n                (\"OPTIONS\", \"POST\")\n            )\n\n        return resp\n\n    return impl",
      "importString": "from collections.abc import Awaitable, Callable, Iterable\n\nfrom aiohttp.typedefs import Middleware\nfrom aiohttp.web_middlewares import middleware\nfrom aiohttp.web_request import Request\nfrom aiohttp.web_response import StreamResponse",
      "lineNum": 24,
      "relativeDocumentPath": "src/blackd/middlewares.py"
    },
    {
      "symbolName": "impl",
      "sourceCode": "@middleware\n    async def impl(request: Request, handler: Handler) -> StreamResponse:\n        is_options = request.method == \"OPTIONS\"\n        is_preflight = is_options and \"Access-Control-Request-Method\" in request.headers\n        if is_preflight:\n            resp = StreamResponse()\n        else:\n            resp = await handler(request)\n\n        origin = request.headers.get(\"Origin\")\n        if not origin:\n            return resp\n\n        resp.headers[\"Access-Control-Allow-Origin\"] = \"*\"\n        resp.headers[\"Access-Control-Expose-Headers\"] = \"*\"\n        if is_options:\n            resp.headers[\"Access-Control-Allow-Headers\"] = \", \".join(allow_headers)\n            resp.headers[\"Access-Control-Allow-Methods\"] = \", \".join(\n                (\"OPTIONS\", \"POST\")\n            )\n\n        return resp",
      "importString": "from collections.abc import Awaitable, Callable, Iterable\n\nfrom aiohttp.typedefs import Middleware\nfrom aiohttp.web_middlewares import middleware\nfrom aiohttp.web_request import Request\nfrom aiohttp.web_response import StreamResponse",
      "lineNum": 21,
      "relativeDocumentPath": "src/blackd/middlewares.py"
    },
    {
      "symbolName": "parse_graminit_h",
      "sourceCode": "def parse_graminit_h(self, filename):\n        \"\"\"Parse the .h file written by pgen.  (Internal)\n\n        This file is a sequence of #define statements defining the\n        nonterminals of the grammar as numbers.  We build two tables\n        mapping the numbers to names and back.\n\n        \"\"\"\n        try:\n            f = open(filename)\n        except OSError as err:\n            print(f\"Can't open {filename}: {err}\")\n            return False\n        self.symbol2number = {}\n        self.number2symbol = {}\n        lineno = 0\n        for line in f:\n            lineno += 1\n            mo = re.match(r\"^#define\\s+(\\w+)\\s+(\\d+)$\", line)\n            if not mo and line.strip():\n                print(f\"{filename}({lineno}): can't parse {line.strip()}\")\n            else:\n                symbol, number = mo.groups()\n                number = int(number)\n                assert symbol not in self.symbol2number\n                assert number not in self.number2symbol\n                self.symbol2number[symbol] = number\n                self.number2symbol[number] = symbol\n        return True",
      "importString": "import re\nfrom pgen2 import grammar, token",
      "lineNum": 28,
      "relativeDocumentPath": "src/blib2to3/pgen2/conv.py"
    },
    {
      "symbolName": "parse_graminit_c",
      "sourceCode": "def parse_graminit_c(self, filename):\n        \"\"\"Parse the .c file written by pgen.  (Internal)\n\n        The file looks as follows.  The first two lines are always this:\n\n        #include \"pgenheaders.h\"\n        #include \"grammar.h\"\n\n        After that come four blocks:\n\n        1) one or more state definitions\n        2) a table defining dfas\n        3) a table defining labels\n        4) a struct defining the grammar\n\n        A state definition has the following form:\n        - one or more arc arrays, each of the form:\n          static arc arcs_<n>_<m>[<k>] = {\n                  {<i>, <j>},\n                  ...\n          };\n        - followed by a state array, of the form:\n          static state states_<s>[<t>] = {\n                  {<k>, arcs_<n>_<m>},\n                  ...\n          };\n\n        \"\"\"\n        try:\n            f = open(filename)\n        except OSError as err:\n            print(f\"Can't open {filename}: {err}\")\n            return False\n        # The code below essentially uses f's iterator-ness!\n        lineno = 0\n\n        # Expect the two #include lines\n        lineno, line = lineno + 1, next(f)\n        assert line == '#include \"pgenheaders.h\"\\n', (lineno, line)\n        lineno, line = lineno + 1, next(f)\n        assert line == '#include \"grammar.h\"\\n', (lineno, line)\n\n        # Parse the state definitions\n        lineno, line = lineno + 1, next(f)\n        allarcs = {}\n        states = []\n        while line.startswith(\"static arc \"):\n            while line.startswith(\"static arc \"):\n                mo = re.match(r\"static arc arcs_(\\d+)_(\\d+)\\[(\\d+)\\] = {$\", line)\n                assert mo, (lineno, line)\n                n, m, k = list(map(int, mo.groups()))\n                arcs = []\n                for _ in range(k):\n                    lineno, line = lineno + 1, next(f)\n                    mo = re.match(r\"\\s+{(\\d+), (\\d+)},$\", line)\n                    assert mo, (lineno, line)\n                    i, j = list(map(int, mo.groups()))\n                    arcs.append((i, j))\n                lineno, line = lineno + 1, next(f)\n                assert line == \"};\\n\", (lineno, line)\n                allarcs[(n, m)] = arcs\n                lineno, line = lineno + 1, next(f)\n            mo = re.match(r\"static state states_(\\d+)\\[(\\d+)\\] = {$\", line)\n            assert mo, (lineno, line)\n            s, t = list(map(int, mo.groups()))\n            assert s == len(states), (lineno, line)\n            state = []\n            for _ in range(t):\n                lineno, line = lineno + 1, next(f)\n                mo = re.match(r\"\\s+{(\\d+), arcs_(\\d+)_(\\d+)},$\", line)\n                assert mo, (lineno, line)\n                k, n, m = list(map(int, mo.groups()))\n                arcs = allarcs[n, m]\n                assert k == len(arcs), (lineno, line)\n                state.append(arcs)\n            states.append(state)\n            lineno, line = lineno + 1, next(f)\n            assert line == \"};\\n\", (lineno, line)\n            lineno, line = lineno + 1, next(f)\n        self.states = states\n\n        # Parse the dfas\n        dfas = {}\n        mo = re.match(r\"static dfa dfas\\[(\\d+)\\] = {$\", line)\n        assert mo, (lineno, line)\n        ndfas = int(mo.group(1))\n        for i in range(ndfas):\n            lineno, line = lineno + 1, next(f)\n            mo = re.match(r'\\s+{(\\d+), \"(\\w+)\", (\\d+), (\\d+), states_(\\d+),$', line)\n            assert mo, (lineno, line)\n            symbol = mo.group(2)\n            number, x, y, z = list(map(int, mo.group(1, 3, 4, 5)))\n            assert self.symbol2number[symbol] == number, (lineno, line)\n            assert self.number2symbol[number] == symbol, (lineno, line)\n            assert x == 0, (lineno, line)\n            state = states[z]\n            assert y == len(state), (lineno, line)\n            lineno, line = lineno + 1, next(f)\n            mo = re.match(r'\\s+(\"(?:\\\\\\d\\d\\d)*\")},$', line)\n            assert mo, (lineno, line)\n            first = {}\n            rawbitset = eval(mo.group(1))\n            for i, c in enumerate(rawbitset):\n                byte = ord(c)\n                for j in range(8):\n                    if byte & (1 << j):\n                        first[i * 8 + j] = 1\n            dfas[number] = (state, first)\n        lineno, line = lineno + 1, next(f)\n        assert line == \"};\\n\", (lineno, line)\n        self.dfas = dfas\n\n        # Parse the labels\n        labels = []\n        lineno, line = lineno + 1, next(f)\n        mo = re.match(r\"static label labels\\[(\\d+)\\] = {$\", line)\n        assert mo, (lineno, line)\n        nlabels = int(mo.group(1))\n        for i in range(nlabels):\n            lineno, line = lineno + 1, next(f)\n            mo = re.match(r'\\s+{(\\d+), (0|\"\\w+\")},$', line)\n            assert mo, (lineno, line)\n            x, y = mo.groups()\n            x = int(x)\n            if y == \"0\":\n                y = None\n            else:\n                y = eval(y)\n            labels.append((x, y))\n        lineno, line = lineno + 1, next(f)\n        assert line == \"};\\n\", (lineno, line)\n        self.labels = labels\n\n        # Parse the grammar struct\n        lineno, line = lineno + 1, next(f)\n        assert line == \"grammar _PyParser_Grammar = {\\n\", (lineno, line)\n        lineno, line = lineno + 1, next(f)\n        mo = re.match(r\"\\s+(\\d+),$\", line)\n        assert mo, (lineno, line)\n        ndfas = int(mo.group(1))\n        assert ndfas == len(self.dfas)\n        lineno, line = lineno + 1, next(f)\n        assert line == \"\\tdfas,\\n\", (lineno, line)\n        lineno, line = lineno + 1, next(f)\n        mo = re.match(r\"\\s+{(\\d+), labels},$\", line)\n        assert mo, (lineno, line)\n        nlabels = int(mo.group(1))\n        assert nlabels == len(self.labels), (lineno, line)\n        lineno, line = lineno + 1, next(f)\n        mo = re.match(r\"\\s+(\\d+)$\", line)\n        assert mo, (lineno, line)\n        start = int(mo.group(1))\n        assert start in self.number2symbol, (lineno, line)\n        self.start = start\n        lineno, line = lineno + 1, next(f)\n        assert line == \"};\\n\", (lineno, line)\n        try:\n            lineno, line = lineno + 1, next(f)\n        except StopIteration:\n            pass\n        else:\n            assert 0, (lineno, line)",
      "importString": "import re\nfrom pgen2 import grammar, token",
      "lineNum": 161,
      "relativeDocumentPath": "src/blib2to3/pgen2/conv.py"
    },
    {
      "symbolName": "__next__",
      "sourceCode": "def __next__(self) -> Any:\n        # If the current position is already compromised (looked up)\n        # return the eaten token, if not just go further on the given\n        # token producer.\n        for release_range in self._release_ranges:\n            assert release_range.end is not None\n\n            start, end = release_range.start, release_range.end\n            if start <= self._counter < end:\n                token = release_range.tokens[self._counter - start]\n                break\n        else:\n            token = next(self._tokens)\n        self._counter += 1\n        return token",
      "importString": "import io\nimport logging\nimport os\nimport pkgutil\nimport sys\nfrom collections.abc import Iterable, Iterator\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom logging import Logger\nfrom typing import IO, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo\nfrom blib2to3.pytree import NL\nfrom . import grammar, parse, pgen, token, tokenize",
      "lineNum": 14,
      "relativeDocumentPath": "src/blib2to3/pgen2/driver.py"
    },
    {
      "symbolName": "parse_tokens",
      "sourceCode": "def parse_tokens(self, tokens: Iterable[GoodTokenInfo], debug: bool = False) -> NL:\n        \"\"\"Parse a series of tokens and return the syntax tree.\"\"\"\n        # XXX Move the prefix computation into a wrapper around tokenize.\n        proxy = TokenProxy(tokens)\n\n        p = parse.Parser(self.grammar)\n        p.setup(proxy=proxy)\n\n        lineno = 1\n        column = 0\n        indent_columns: list[int] = []\n        type = value = start = end = line_text = None\n        prefix = \"\"\n\n        for quintuple in proxy:\n            type, value, start, end, line_text = quintuple\n            if start != (lineno, column):\n                assert (lineno, column) <= start, ((lineno, column), start)\n                s_lineno, s_column = start\n                if lineno < s_lineno:\n                    prefix += \"\\n\" * (s_lineno - lineno)\n                    lineno = s_lineno\n                    column = 0\n                if column < s_column:\n                    prefix += line_text[column:s_column]\n                    column = s_column\n            if type in (tokenize.COMMENT, tokenize.NL):\n                prefix += value\n                lineno, column = end\n                if value.endswith(\"\\n\"):\n                    lineno += 1\n                    column = 0\n                continue\n            if type == token.OP:\n                type = grammar.opmap[value]\n            if debug:\n                assert type is not None\n                self.logger.debug(\n                    \"%s %r (prefix=%r)\", token.tok_name[type], value, prefix\n                )\n            if type == token.INDENT:\n                indent_columns.append(len(value))\n                _prefix = prefix + value\n                prefix = \"\"\n                value = \"\"\n            elif type == token.DEDENT:\n                _indent_col = indent_columns.pop()\n                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)\n            if p.addtoken(cast(int, type), value, (prefix, start)):\n                if debug:\n                    self.logger.debug(\"Stop.\")\n                break\n            prefix = \"\"\n            if type in {token.INDENT, token.DEDENT}:\n                prefix = _prefix\n            lineno, column = end\n            # FSTRING_MIDDLE is the only token that can end with a newline, and\n            # `end` will point to the next line. For that case, don't increment lineno.\n            if value.endswith(\"\\n\") and type != token.FSTRING_MIDDLE:\n                lineno += 1\n                column = 0\n        else:\n            # We never broke out -- EOF is too soon (how can this happen???)\n            assert start is not None\n            raise parse.ParseError(\"incomplete input\", type, value, (prefix, start))\n        assert p.rootnode is not None\n        return p.rootnode",
      "importString": "import io\nimport logging\nimport os\nimport pkgutil\nimport sys\nfrom collections.abc import Iterable, Iterator\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom logging import Logger\nfrom typing import IO, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo\nfrom blib2to3.pytree import NL\nfrom . import grammar, parse, pgen, token, tokenize",
      "lineNum": 66,
      "relativeDocumentPath": "src/blib2to3/pgen2/driver.py"
    },
    {
      "symbolName": "_partially_consume_prefix",
      "sourceCode": "def _partially_consume_prefix(self, prefix: str, column: int) -> tuple[str, str]:\n        lines: list[str] = []\n        current_line = \"\"\n        current_column = 0\n        wait_for_nl = False\n        for char in prefix:\n            current_line += char\n            if wait_for_nl:\n                if char == \"\\n\":\n                    if current_line.strip() and current_column < column:\n                        res = \"\".join(lines)\n                        return res, prefix[len(res) :]\n\n                    lines.append(current_line)\n                    current_line = \"\"\n                    current_column = 0\n                    wait_for_nl = False\n            elif char in \" \\t\":\n                current_column += 1\n            elif char == \"\\n\":\n                # unexpected empty line\n                current_column = 0\n            elif char == \"\\f\":\n                current_column = 0\n            else:\n                # indent is finished\n                wait_for_nl = True\n        return \"\".join(lines), current_line",
      "importString": "import io\nimport logging\nimport os\nimport pkgutil\nimport sys\nfrom collections.abc import Iterable, Iterator\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom logging import Logger\nfrom typing import IO, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo\nfrom blib2to3.pytree import NL\nfrom . import grammar, parse, pgen, token, tokenize",
      "lineNum": 27,
      "relativeDocumentPath": "src/blib2to3/pgen2/driver.py"
    },
    {
      "symbolName": "load_grammar",
      "sourceCode": "def load_grammar(\n    gt: str = \"Grammar.txt\",\n    gp: Optional[str] = None,\n    save: bool = True,\n    force: bool = False,\n    logger: Optional[Logger] = None,\n) -> Grammar:\n    \"\"\"Load the grammar (maybe from a pickle).\"\"\"\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    gp = _generate_pickle_name(gt) if gp is None else gp\n    if force or not _newer(gp, gt):\n        g: grammar.Grammar = pgen.generate_grammar(gt)\n        if save:\n            try:\n                g.dump(gp)\n            except OSError:\n                # Ignore error, caching is not vital.\n                pass\n    else:\n        g = grammar.Grammar()\n        g.load(gp)\n    return g",
      "importString": "import io\nimport logging\nimport os\nimport pkgutil\nimport sys\nfrom collections.abc import Iterable, Iterator\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom logging import Logger\nfrom typing import IO, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo\nfrom blib2to3.pytree import NL\nfrom . import grammar, parse, pgen, token, tokenize",
      "lineNum": 22,
      "relativeDocumentPath": "src/blib2to3/pgen2/driver.py"
    },
    {
      "symbolName": "load_packaged_grammar",
      "sourceCode": "def load_packaged_grammar(\n    package: str, grammar_source: str, cache_dir: Optional[Path] = None\n) -> grammar.Grammar:\n    \"\"\"Normally, loads a pickled grammar by doing\n        pkgutil.get_data(package, pickled_grammar)\n    where *pickled_grammar* is computed from *grammar_source* by adding the\n    Python version and using a ``.pickle`` extension.\n\n    However, if *grammar_source* is an extant file, load_grammar(grammar_source)\n    is called instead. This facilitates using a packaged grammar file when needed\n    but preserves load_grammar's automatic regeneration behavior when possible.\n\n    \"\"\"\n    if os.path.isfile(grammar_source):\n        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None\n        return load_grammar(grammar_source, gp=gp)\n    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)\n    data = pkgutil.get_data(package, pickled_name)\n    assert data is not None\n    g = grammar.Grammar()\n    g.loads(data)\n    return g",
      "importString": "import io\nimport logging\nimport os\nimport pkgutil\nimport sys\nfrom collections.abc import Iterable, Iterator\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom logging import Logger\nfrom typing import IO, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo\nfrom blib2to3.pytree import NL\nfrom . import grammar, parse, pgen, token, tokenize",
      "lineNum": 21,
      "relativeDocumentPath": "src/blib2to3/pgen2/driver.py"
    },
    {
      "symbolName": "main",
      "sourceCode": "def main(*args: str) -> bool:\n    \"\"\"Main program, when run as a script: produce grammar pickle files.\n\n    Calls load_grammar for each argument, a path to a grammar text file.\n    \"\"\"\n    if not args:\n        args = tuple(sys.argv[1:])\n    logging.basicConfig(level=logging.INFO, stream=sys.stdout, format=\"%(message)s\")\n    for gt in args:\n        load_grammar(gt, save=True, force=True)\n    return True",
      "importString": "import io\nimport logging\nimport os\nimport pkgutil\nimport sys\nfrom collections.abc import Iterable, Iterator\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom logging import Logger\nfrom typing import IO, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo\nfrom blib2to3.pytree import NL\nfrom . import grammar, parse, pgen, token, tokenize",
      "lineNum": 10,
      "relativeDocumentPath": "src/blib2to3/pgen2/driver.py"
    },
    {
      "symbolName": "__init__",
      "sourceCode": "def __init__(self) -> None:\n        self.symbol2number: dict[str, int] = {}\n        self.number2symbol: dict[int, str] = {}\n        self.states: list[DFA] = []\n        self.dfas: dict[int, DFAS] = {}\n        self.labels: list[Label] = [(0, \"EMPTY\")]\n        self.keywords: dict[str, int] = {}\n        self.soft_keywords: dict[str, int] = {}\n        self.tokens: dict[int, int] = {}\n        self.symbol2label: dict[str, int] = {}\n        self.version: tuple[int, int] = (0, 0)\n        self.start = 256\n        # Python 3.7+ parses async as a keyword, not an identifier\n        self.async_keywords = False",
      "importString": "import os\nimport pickle\nimport tempfile\nfrom typing import Any, Optional, TypeVar, Union\nfrom . import token",
      "lineNum": 13,
      "relativeDocumentPath": "src/blib2to3/pgen2/grammar.py"
    },
    {
      "symbolName": "dump",
      "sourceCode": "def dump(self, filename: Path) -> None:\n        \"\"\"Dump the grammar tables to a pickle file.\"\"\"\n\n        # mypyc generates objects that don't have a __dict__, but they\n        # do have __getstate__ methods that will return an equivalent\n        # dictionary\n        if hasattr(self, \"__dict__\"):\n            d = self.__dict__\n        else:\n            d = self.__getstate__()  # type: ignore\n\n        with tempfile.NamedTemporaryFile(\n            dir=os.path.dirname(filename), delete=False\n        ) as f:\n            pickle.dump(d, f, pickle.HIGHEST_PROTOCOL)\n        os.replace(f.name, filename)",
      "importString": "import os\nimport pickle\nimport tempfile\nfrom typing import Any, Optional, TypeVar, Union\nfrom . import token",
      "lineNum": 15,
      "relativeDocumentPath": "src/blib2to3/pgen2/grammar.py"
    },
    {
      "symbolName": "copy",
      "sourceCode": "def copy(self: _P) -> _P:\n        \"\"\"\n        Copy the grammar.\n        \"\"\"\n        new = self.__class__()\n        for dict_attr in (\n            \"symbol2number\",\n            \"number2symbol\",\n            \"dfas\",\n            \"keywords\",\n            \"soft_keywords\",\n            \"tokens\",\n            \"symbol2label\",\n        ):\n            setattr(new, dict_attr, getattr(self, dict_attr).copy())\n        new.labels = self.labels[:]\n        new.states = self.states[:]\n        new.start = self.start\n        new.version = self.version\n        new.async_keywords = self.async_keywords\n        return new",
      "importString": "import os\nimport pickle\nimport tempfile\nfrom typing import Any, Optional, TypeVar, Union\nfrom . import token",
      "lineNum": 20,
      "relativeDocumentPath": "src/blib2to3/pgen2/grammar.py"
    },
    {
      "symbolName": "report",
      "sourceCode": "def report(self) -> None:\n        \"\"\"Dump the grammar tables to standard output, for debugging.\"\"\"\n        from pprint import pprint\n\n        print(\"s2n\")\n        pprint(self.symbol2number)\n        print(\"n2s\")\n        pprint(self.number2symbol)\n        print(\"states\")\n        pprint(self.states)\n        print(\"dfas\")\n        pprint(self.dfas)\n        print(\"labels\")\n        pprint(self.labels)\n        print(\"start\", self.start)",
      "importString": "import os\nimport pickle\nimport tempfile\nfrom typing import Any, Optional, TypeVar, Union\nfrom . import token",
      "lineNum": 14,
      "relativeDocumentPath": "src/blib2to3/pgen2/grammar.py"
    },
    {
      "symbolName": "escape",
      "sourceCode": "def escape(m: re.Match[str]) -> str:\n    all, tail = m.group(0, 1)\n    assert all.startswith(\"\\\\\")\n    esc = simple_escapes.get(tail)\n    if esc is not None:\n        return esc\n    if tail.startswith(\"x\"):\n        hexes = tail[1:]\n        if len(hexes) < 2:\n            raise ValueError(\"invalid hex string escape ('\\\\%s')\" % tail)\n        try:\n            i = int(hexes, 16)\n        except ValueError:\n            raise ValueError(\"invalid hex string escape ('\\\\%s')\" % tail) from None\n    else:\n        try:\n            i = int(tail, 8)\n        except ValueError:\n            raise ValueError(\"invalid octal string escape ('\\\\%s')\" % tail) from None\n    return chr(i)",
      "importString": "import re",
      "lineNum": 19,
      "relativeDocumentPath": "src/blib2to3/pgen2/literals.py"
    },
    {
      "symbolName": "backtrack",
      "sourceCode": "@contextmanager\n    def backtrack(self) -> Iterator[None]:\n        \"\"\"\n        Use the node-level invariant ones for basic parsing operations (push/pop/shift).\n        These still will operate on the stack; but they won't create any new nodes, or\n        modify the contents of any other existing nodes.\n\n        This saves us a ton of time when we are backtracking, since we\n        want to restore to the initial state as quick as possible, which\n        can only be done by having as little mutatations as possible.\n        \"\"\"\n        is_backtracking = self.parser.is_backtracking\n        try:\n            self.parser.is_backtracking = True\n            yield\n        finally:\n            self.parser.is_backtracking = is_backtracking",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 16,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "add_token",
      "sourceCode": "def add_token(self, tok_type: int, tok_val: str, raw: bool = False) -> None:\n        func: Callable[..., Any]\n        if raw:\n            func = self.parser._addtoken\n        else:\n            func = self.parser.addtoken\n\n        for ilabel in self.ilabels:\n            with self.switch_to(ilabel):\n                args = [tok_type, tok_val, self.context]\n                if raw:\n                    args.insert(0, ilabel)\n                func(*args)",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 12,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "determine_route",
      "sourceCode": "def determine_route(\n        self, value: Optional[str] = None, force: bool = False\n    ) -> Optional[int]:\n        alive_ilabels = self.ilabels\n        if len(alive_ilabels) == 0:\n            *_, most_successful_ilabel = self._dead_ilabels\n            raise ParseError(\"bad input\", most_successful_ilabel, value, self.context)\n\n        ilabel, *rest = alive_ilabels\n        if force or not rest:\n            return ilabel\n        else:\n            return None",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 12,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "__init__",
      "sourceCode": "def __init__(self, grammar: Grammar, convert: Optional[Convert] = None) -> None:\n        \"\"\"Constructor.\n\n        The grammar argument is a grammar.Grammar instance; see the\n        grammar module for more information.\n\n        The parser is not ready yet for parsing; you must call the\n        setup() method to get it started.\n\n        The optional convert argument is a function mapping concrete\n        syntax tree nodes to abstract syntax tree nodes.  If not\n        given, no conversion is done and the syntax tree produced is\n        the concrete syntax tree.  If given, it must be a function of\n        two arguments, the first being the grammar (a grammar.Grammar\n        instance), and the second being the concrete syntax tree node\n        to be converted.  The syntax tree is converted from the bottom\n        up.\n\n        **post-note: the convert argument is ignored since for Black's\n        usage, convert will always be blib2to3.pytree.convert. Allowing\n        this to be dynamic hurts mypyc's ability to use early binding.\n        These docs are left for historical and informational value.\n\n        A concrete syntax tree node is a (type, value, context, nodes)\n        tuple, where type is the node type (a token or symbol number),\n        value is None for symbols and a string for tokens, context is\n        None or an opaque value used for error reporting (typically a\n        (lineno, offset) pair), and nodes is a list of children for\n        symbols, and None for tokens.\n\n        An abstract syntax tree node may be anything; this is entirely\n        up to the converter function.\n\n        \"\"\"\n        self.grammar = grammar\n        # See note in docstring above. TL;DR this is ignored.\n        self.convert = convert or lam_sub\n        self.is_backtracking = False\n        self.last_token: Optional[int] = None",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 38,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "setup",
      "sourceCode": "def setup(self, proxy: \"TokenProxy\", start: Optional[int] = None) -> None:\n        \"\"\"Prepare for parsing.\n\n        This *must* be called before starting to parse.\n\n        The optional argument is an alternative start symbol; it\n        defaults to the grammar's start symbol.\n\n        You can use a Parser instance to parse any number of programs;\n        each time you call setup() the parser is reset to an initial\n        state determined by the (implicit or explicit) start symbol.\n\n        \"\"\"\n        if start is None:\n            start = self.grammar.start\n        # Each stack entry is a tuple: (dfa, state, node).\n        # A node is a tuple: (type, value, context, children),\n        # where children is a list of nodes or None, and context may be None.\n        newnode: RawNode = (start, None, None, [])\n        stackentry = (self.grammar.dfas[start], 0, newnode)\n        self.stack: list[tuple[DFAS, int, RawNode]] = [stackentry]\n        self.rootnode: Optional[NL] = None\n        self.used_names: set[str] = set()\n        self.proxy = proxy\n        self.last_token = None",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 24,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "addtoken",
      "sourceCode": "def addtoken(self, type: int, value: str, context: Context) -> bool:\n        \"\"\"Add a token; return True iff this is the end of the program.\"\"\"\n        # Map from token to label\n        ilabels = self.classify(type, value, context)\n        assert len(ilabels) >= 1\n\n        # If we have only one state to advance, we'll directly\n        # take it as is.\n        if len(ilabels) == 1:\n            [ilabel] = ilabels\n            return self._addtoken(ilabel, type, value, context)\n\n        # If there are multiple states which we can advance (only\n        # happen under soft-keywords), then we will try all of them\n        # in parallel and as soon as one state can reach further than\n        # the rest, we'll choose that one. This is a pretty hacky\n        # and hopefully temporary algorithm.\n        #\n        # For a more detailed explanation, check out this post:\n        # https://tree.science/what-the-backtracking.html\n\n        with self.proxy.release() as proxy:\n            counter, force = 0, False\n            recorder = Recorder(self, ilabels, context)\n            recorder.add_token(type, value, raw=True)\n\n            next_token_value = value\n            while recorder.determine_route(next_token_value) is None:\n                if not proxy.can_advance(counter):\n                    force = True\n                    break\n\n                next_token_type, next_token_value, *_ = proxy.eat(counter)\n                if next_token_type in (tokenize.COMMENT, tokenize.NL):\n                    counter += 1\n                    continue\n\n                if next_token_type == tokenize.OP:\n                    next_token_type = grammar.opmap[next_token_value]\n\n                recorder.add_token(next_token_type, next_token_value)\n                counter += 1\n\n            ilabel = cast(int, recorder.determine_route(next_token_value, force=force))\n            assert ilabel is not None\n\n        return self._addtoken(ilabel, type, value, context)",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 46,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "_addtoken",
      "sourceCode": "def _addtoken(self, ilabel: int, type: int, value: str, context: Context) -> bool:\n        # Loop until the token is shifted; may raise exceptions\n        while True:\n            dfa, state, node = self.stack[-1]\n            states, first = dfa\n            arcs = states[state]\n            # Look for a state with this label\n            for i, newstate in arcs:\n                t = self.grammar.labels[i][0]\n                if t >= 256:\n                    # See if it's a symbol and if we're in its first set\n                    itsdfa = self.grammar.dfas[t]\n                    itsstates, itsfirst = itsdfa\n                    if ilabel in itsfirst:\n                        # Push a symbol\n                        self.push(t, itsdfa, newstate, context)\n                        break  # To continue the outer while loop\n\n                elif ilabel == i:\n                    # Look it up in the list of labels\n                    # Shift a token; we're done with it\n                    self.shift(type, value, newstate, context)\n                    # Pop while we are in an accept-only state\n                    state = newstate\n                    while states[state] == [(0, state)]:\n                        self.pop()\n                        if not self.stack:\n                            # Done parsing!\n                            return True\n                        dfa, state, node = self.stack[-1]\n                        states, first = dfa\n                    # Done with this token\n                    self.last_token = type\n                    return False\n\n            else:\n                if (0, state) in arcs:\n                    # An accepting state, pop it and try something else\n                    self.pop()\n                    if not self.stack:\n                        # Done parsing, but another token is input\n                        raise ParseError(\"too much input\", type, value, context)\n                else:\n                    # No success finding a transition\n                    raise ParseError(\"bad input\", type, value, context)",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 44,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "classify",
      "sourceCode": "def classify(self, type: int, value: str, context: Context) -> list[int]:\n        \"\"\"Turn a token into a label.  (Internal)\n\n        Depending on whether the value is a soft-keyword or not,\n        this function may return multiple labels to choose from.\"\"\"\n        if type == token.NAME:\n            # Keep a listing of all used names\n            self.used_names.add(value)\n            # Check for reserved words\n            if value in self.grammar.keywords:\n                return [self.grammar.keywords[value]]\n            elif value in self.grammar.soft_keywords:\n                assert type in self.grammar.tokens\n                # Current soft keywords (match, case, type) can only appear at the\n                # beginning of a statement. So as a shortcut, don't try to treat them\n                # like keywords in any other context.\n                # ('_' is also a soft keyword in the real grammar, but for our grammar\n                # it's just an expression, so we don't need to treat it specially.)\n                if self.last_token not in (\n                    None,\n                    token.INDENT,\n                    token.DEDENT,\n                    token.NEWLINE,\n                    token.SEMI,\n                    token.COLON,\n                ):\n                    return [self.grammar.tokens[type]]\n                return [\n                    self.grammar.tokens[type],\n                    self.grammar.soft_keywords[value],\n                ]\n\n        ilabel = self.grammar.tokens.get(type)\n        if ilabel is None:\n            raise ParseError(\"bad token\", type, value, context)\n        return [ilabel]",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 35,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "shift",
      "sourceCode": "def shift(self, type: int, value: str, newstate: int, context: Context) -> None:\n        \"\"\"Shift a token.  (Internal)\"\"\"\n        if self.is_backtracking:\n            dfa, state, _ = self.stack[-1]\n            self.stack[-1] = (dfa, newstate, DUMMY_NODE)\n        else:\n            dfa, state, node = self.stack[-1]\n            rawnode: RawNode = (type, value, context, None)\n            newnode = convert(self.grammar, rawnode)\n            assert node[-1] is not None\n            node[-1].append(newnode)\n            self.stack[-1] = (dfa, newstate, node)",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 11,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "push",
      "sourceCode": "def push(self, type: int, newdfa: DFAS, newstate: int, context: Context) -> None:\n        \"\"\"Push a nonterminal.  (Internal)\"\"\"\n        if self.is_backtracking:\n            dfa, state, _ = self.stack[-1]\n            self.stack[-1] = (dfa, newstate, DUMMY_NODE)\n            self.stack.append((newdfa, 0, DUMMY_NODE))\n        else:\n            dfa, state, node = self.stack[-1]\n            newnode: RawNode = (type, None, context, [])\n            self.stack[-1] = (dfa, newstate, node)\n            self.stack.append((newdfa, 0, newnode))",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 10,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "pop",
      "sourceCode": "def pop(self) -> None:\n        \"\"\"Pop a nonterminal.  (Internal)\"\"\"\n        if self.is_backtracking:\n            self.stack.pop()\n        else:\n            popdfa, popstate, popnode = self.stack.pop()\n            newnode = convert(self.grammar, popnode)\n            if self.stack:\n                dfa, state, node = self.stack[-1]\n                assert node[-1] is not None\n                node[-1].append(newnode)\n            else:\n                self.rootnode = newnode\n                self.rootnode.used_names = self.used_names",
      "importString": "from collections.abc import Callable, Iterator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pytree import NL, Context, Leaf, Node, RawNode, convert\nfrom . import grammar, token, tokenize",
      "lineNum": 13,
      "relativeDocumentPath": "src/blib2to3/pgen2/parse.py"
    },
    {
      "symbolName": "__init__",
      "sourceCode": "def __init__(self, filename: Path, stream: Optional[IO[str]] = None) -> None:\n        close_stream = None\n        if stream is None:\n            stream = open(filename, encoding=\"utf-8\")\n            close_stream = stream.close\n        self.filename = filename\n        self.stream = stream\n        self.generator = tokenize.generate_tokens(stream.readline)\n        self.gettoken()  # Initialize lookahead\n        self.dfas, self.startsymbol = self.parse()\n        if close_stream is not None:\n            close_stream()\n        self.first = {}  # map from symbol name to set of tokens\n        self.addfirstsets()",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 13,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "make_grammar",
      "sourceCode": "def make_grammar(self) -> PgenGrammar:\n        c = PgenGrammar()\n        names = list(self.dfas.keys())\n        names.sort()\n        names.remove(self.startsymbol)\n        names.insert(0, self.startsymbol)\n        for name in names:\n            i = 256 + len(c.symbol2number)\n            c.symbol2number[name] = i\n            c.number2symbol[i] = name\n        for name in names:\n            dfa = self.dfas[name]\n            states = []\n            for state in dfa:\n                arcs = []\n                for label, next in sorted(state.arcs.items()):\n                    arcs.append((self.make_label(c, label), dfa.index(next)))\n                if state.isfinal:\n                    arcs.append((0, dfa.index(state)))\n                states.append(arcs)\n            c.states.append(states)\n            c.dfas[c.symbol2number[name]] = (states, self.make_first(c, name))\n        c.start = c.symbol2number[self.startsymbol]\n        return c",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 23,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "make_label",
      "sourceCode": "def make_label(self, c: PgenGrammar, label: str) -> int:\n        # XXX Maybe this should be a method on a subclass of converter?\n        ilabel = len(c.labels)\n        if label[0].isalpha():\n            # Either a symbol name or a named token\n            if label in c.symbol2number:\n                # A symbol name (a non-terminal)\n                if label in c.symbol2label:\n                    return c.symbol2label[label]\n                else:\n                    c.labels.append((c.symbol2number[label], None))\n                    c.symbol2label[label] = ilabel\n                    return ilabel\n            else:\n                # A named token (NAME, NUMBER, STRING)\n                itoken = getattr(token, label, None)\n                assert isinstance(itoken, int), label\n                assert itoken in token.tok_name, label\n                if itoken in c.tokens:\n                    return c.tokens[itoken]\n                else:\n                    c.labels.append((itoken, None))\n                    c.tokens[itoken] = ilabel\n                    return ilabel\n        else:\n            # Either a keyword or an operator\n            assert label[0] in ('\"', \"'\"), label\n            value = eval(label)\n            if value[0].isalpha():\n                if label[0] == '\"':\n                    keywords = c.soft_keywords\n                else:\n                    keywords = c.keywords\n\n                # A keyword\n                if value in keywords:\n                    return keywords[value]\n                else:\n                    c.labels.append((token.NAME, value))\n                    keywords[value] = ilabel\n                    return ilabel\n            else:\n                # An operator (any non-numeric token)\n                itoken = grammar.opmap[value]  # Fails if unknown token\n                if itoken in c.tokens:\n                    return c.tokens[itoken]\n                else:\n                    c.labels.append((itoken, None))\n                    c.tokens[itoken] = ilabel\n                    return ilabel",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 49,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "calcfirst",
      "sourceCode": "def calcfirst(self, name: str) -> None:\n        dfa = self.dfas[name]\n        self.first[name] = None  # dummy to detect left recursion\n        state = dfa[0]\n        totalset: dict[str, int] = {}\n        overlapcheck = {}\n        for label in state.arcs:\n            if label in self.dfas:\n                if label in self.first:\n                    fset = self.first[label]\n                    if fset is None:\n                        raise ValueError(\"recursion for rule %r\" % name)\n                else:\n                    self.calcfirst(label)\n                    fset = self.first[label]\n                    assert fset is not None\n                totalset.update(fset)\n                overlapcheck[label] = fset\n            else:\n                totalset[label] = 1\n                overlapcheck[label] = {label: 1}\n        inverse: dict[str, str] = {}\n        for label, itsfirst in overlapcheck.items():\n            for symbol in itsfirst:\n                if symbol in inverse:\n                    raise ValueError(\n                        \"rule %s is ambiguous; %s is in the first sets of %s as well\"\n                        \" as %s\" % (name, symbol, label, inverse[symbol])\n                    )\n                inverse[symbol] = label\n        self.first[name] = totalset",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 30,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "parse",
      "sourceCode": "def parse(self) -> tuple[dict[str, list[\"DFAState\"]], str]:\n        dfas = {}\n        startsymbol: Optional[str] = None\n        # MSTART: (NEWLINE | RULE)* ENDMARKER\n        while self.type != token.ENDMARKER:\n            while self.type == token.NEWLINE:\n                self.gettoken()\n            # RULE: NAME ':' RHS NEWLINE\n            name = self.expect(token.NAME)\n            self.expect(token.OP, \":\")\n            a, z = self.parse_rhs()\n            self.expect(token.NEWLINE)\n            # self.dump_nfa(name, a, z)\n            dfa = self.make_dfa(a, z)\n            # self.dump_dfa(name, dfa)\n            # oldlen = len(dfa)\n            self.simplify_dfa(dfa)\n            # newlen = len(dfa)\n            dfas[name] = dfa\n            # print name, oldlen, newlen\n            if startsymbol is None:\n                startsymbol = name\n        assert startsymbol is not None\n        return dfas, startsymbol",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 23,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "make_dfa",
      "sourceCode": "def make_dfa(self, start: \"NFAState\", finish: \"NFAState\") -> list[\"DFAState\"]:\n        # To turn an NFA into a DFA, we define the states of the DFA\n        # to correspond to *sets* of states of the NFA.  Then do some\n        # state reduction.  Let's represent sets as dicts with 1 for\n        # values.\n        assert isinstance(start, NFAState)\n        assert isinstance(finish, NFAState)\n\n        def closure(state: NFAState) -> dict[NFAState, int]:\n            base: dict[NFAState, int] = {}\n            addclosure(state, base)\n            return base\n\n        def addclosure(state: NFAState, base: dict[NFAState, int]) -> None:\n            assert isinstance(state, NFAState)\n            if state in base:\n                return\n            base[state] = 1\n            for label, next in state.arcs:\n                if label is None:\n                    addclosure(next, base)\n\n        states = [DFAState(closure(start), finish)]\n        for state in states:  # NB states grows while we're iterating\n            arcs: dict[str, dict[NFAState, int]] = {}\n            for nfastate in state.nfaset:\n                for label, next in nfastate.arcs:\n                    if label is not None:\n                        addclosure(next, arcs.setdefault(label, {}))\n            for label, nfaset in sorted(arcs.items()):\n                for st in states:\n                    if st.nfaset == nfaset:\n                        break\n                else:\n                    st = DFAState(nfaset, finish)\n                    states.append(st)\n                state.addarc(st, label)\n        return states",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 37,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "dump_nfa",
      "sourceCode": "def dump_nfa(self, name: str, start: \"NFAState\", finish: \"NFAState\") -> None:\n        print(\"Dump of NFA for\", name)\n        todo = [start]\n        for i, state in enumerate(todo):\n            print(\"  State\", i, state is finish and \"(final)\" or \"\")\n            for label, next in state.arcs:\n                if next in todo:\n                    j = todo.index(next)\n                else:\n                    j = len(todo)\n                    todo.append(next)\n                if label is None:\n                    print(\"    -> %d\" % j)\n                else:\n                    print(\"    %s -> %d\" % (label, j))",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 14,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "simplify_dfa",
      "sourceCode": "def simplify_dfa(self, dfa: list[\"DFAState\"]) -> None:\n        # This is not theoretically optimal, but works well enough.\n        # Algorithm: repeatedly look for two states that have the same\n        # set of arcs (same labels pointing to the same nodes) and\n        # unify them, until things stop changing.\n\n        # dfa is a list of DFAState instances\n        changes = True\n        while changes:\n            changes = False\n            for i, state_i in enumerate(dfa):\n                for j in range(i + 1, len(dfa)):\n                    state_j = dfa[j]\n                    if state_i == state_j:\n                        # print \"  unify\", i, j\n                        del dfa[j]\n                        for state in dfa:\n                            state.unifystate(state_j, state_i)\n                        changes = True\n                        break",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 19,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "parse_rhs",
      "sourceCode": "def parse_rhs(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # RHS: ALT ('|' ALT)*\n        a, z = self.parse_alt()\n        if self.value != \"|\":\n            return a, z\n        else:\n            aa = NFAState()\n            zz = NFAState()\n            aa.addarc(a)\n            z.addarc(zz)\n            while self.value == \"|\":\n                self.gettoken()\n                a, z = self.parse_alt()\n                aa.addarc(a)\n                z.addarc(zz)\n            return aa, zz",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 15,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "parse_item",
      "sourceCode": "def parse_item(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # ITEM: '[' RHS ']' | ATOM ['+' | '*']\n        if self.value == \"[\":\n            self.gettoken()\n            a, z = self.parse_rhs()\n            self.expect(token.OP, \"]\")\n            a.addarc(z)\n            return a, z\n        else:\n            a, z = self.parse_atom()\n            value = self.value\n            if value not in (\"+\", \"*\"):\n                return a, z\n            self.gettoken()\n            z.addarc(a)\n            if value == \"+\":\n                return a, z\n            else:\n                return a, a",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 18,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "parse_atom",
      "sourceCode": "def parse_atom(self) -> tuple[\"NFAState\", \"NFAState\"]:\n        # ATOM: '(' RHS ')' | NAME | STRING\n        if self.value == \"(\":\n            self.gettoken()\n            a, z = self.parse_rhs()\n            self.expect(token.OP, \")\")\n            return a, z\n        elif self.type in (token.NAME, token.STRING):\n            a = NFAState()\n            z = NFAState()\n            a.addarc(z, self.value)\n            self.gettoken()\n            return a, z\n        else:\n            self.raise_error(\n                \"expected (...) or NAME or STRING, got %s/%s\", self.type, self.value\n            )\n            raise AssertionError",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 17,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "__eq__",
      "sourceCode": "def __eq__(self, other: Any) -> bool:\n        # Equality test -- ignore the nfaset instance variable\n        assert isinstance(other, DFAState)\n        if self.isfinal != other.isfinal:\n            return False\n        # Can't just return self.arcs == other.arcs, because that\n        # would invoke this method recursively, with cycles...\n        if len(self.arcs) != len(other.arcs):\n            return False\n        for label, next in self.arcs.items():\n            if next is not other.arcs.get(label):\n                return False\n        return True",
      "importString": "import os\nfrom collections.abc import Iterator, Sequence\nfrom typing import IO, Any, NoReturn, Optional, Union\n\nfrom blib2to3.pgen2 import grammar, token, tokenize\nfrom blib2to3.pgen2.tokenize import GoodTokenInfo",
      "lineNum": 12,
      "relativeDocumentPath": "src/blib2to3/pgen2/pgen.py"
    },
    {
      "symbolName": "tokenize",
      "sourceCode": "def tokenize(readline: Callable[[], str], tokeneater: TokenEater = printtoken) -> None:\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass",
      "importString": "import builtins\nimport sys\nfrom collections.abc import Callable, Iterable, Iterator\nfrom re import Pattern\nfrom typing import Final, Optional, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.token import (\nASYNC\nAWAIT\nCOMMENT\nDEDENT\nENDMARKER\nERRORTOKEN\nFSTRING_END\nFSTRING_MIDDLE\nFSTRING_START\nINDENT\nLBRACE\nNAME\nNEWLINE\nNL\nNUMBER\nOP\nRBRACE\nSTRING\ntok_name\n)\nimport re\nfrom codecs import BOM_UTF8, lookup\n\nfrom . import token",
      "lineNum": 16,
      "relativeDocumentPath": "src/blib2to3/pgen2/tokenize.py"
    },
    {
      "symbolName": "untokenize",
      "sourceCode": "def untokenize(self, iterable: Iterable[TokenInfo]) -> str:\n        for t in iterable:\n            if len(t) == 2:\n                self.compat(t, iterable)\n                break\n            tok_type, token, start, end, line = t\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)",
      "importString": "import builtins\nimport sys\nfrom collections.abc import Callable, Iterable, Iterator\nfrom re import Pattern\nfrom typing import Final, Optional, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.token import (\nASYNC\nAWAIT\nCOMMENT\nDEDENT\nENDMARKER\nERRORTOKEN\nFSTRING_END\nFSTRING_MIDDLE\nFSTRING_START\nINDENT\nLBRACE\nNAME\nNEWLINE\nNL\nNUMBER\nOP\nRBRACE\nSTRING\ntok_name\n)\nimport re\nfrom codecs import BOM_UTF8, lookup\n\nfrom . import token",
      "lineNum": 12,
      "relativeDocumentPath": "src/blib2to3/pgen2/tokenize.py"
    },
    {
      "symbolName": "compat",
      "sourceCode": "def compat(self, token: tuple[int, str], iterable: Iterable[TokenInfo]) -> None:\n        startline = False\n        indents = []\n        toks_append = self.tokens.append\n        toknum, tokval = token\n        if toknum in (NAME, NUMBER):\n            tokval += \" \"\n        if toknum in (NEWLINE, NL):\n            startline = True\n        for tok in iterable:\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER, ASYNC, AWAIT):\n                tokval += \" \"\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)",
      "importString": "import builtins\nimport sys\nfrom collections.abc import Callable, Iterable, Iterator\nfrom re import Pattern\nfrom typing import Final, Optional, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.token import (\nASYNC\nAWAIT\nCOMMENT\nDEDENT\nENDMARKER\nERRORTOKEN\nFSTRING_END\nFSTRING_MIDDLE\nFSTRING_START\nINDENT\nLBRACE\nNAME\nNEWLINE\nNL\nNUMBER\nOP\nRBRACE\nSTRING\ntok_name\n)\nimport re\nfrom codecs import BOM_UTF8, lookup\n\nfrom . import token",
      "lineNum": 26,
      "relativeDocumentPath": "src/blib2to3/pgen2/tokenize.py"
    },
    {
      "symbolName": "_get_normal_name",
      "sourceCode": "def _get_normal_name(orig_enc: str) -> str:\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or enc.startswith(\n        (\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")\n    ):\n        return \"iso-8859-1\"\n    return orig_enc",
      "importString": "import builtins\nimport sys\nfrom collections.abc import Callable, Iterable, Iterator\nfrom re import Pattern\nfrom typing import Final, Optional, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.token import (\nASYNC\nAWAIT\nCOMMENT\nDEDENT\nENDMARKER\nERRORTOKEN\nFSTRING_END\nFSTRING_MIDDLE\nFSTRING_START\nINDENT\nLBRACE\nNAME\nNEWLINE\nNL\nNUMBER\nOP\nRBRACE\nSTRING\ntok_name\n)\nimport re\nfrom codecs import BOM_UTF8, lookup\n\nfrom . import token",
      "lineNum": 10,
      "relativeDocumentPath": "src/blib2to3/pgen2/tokenize.py"
    },
    {
      "symbolName": "detect_encoding",
      "sourceCode": "def detect_encoding(readline: Callable[[], bytes]) -> tuple[str, list[bytes]]:\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, readline,\n    in the same way as the tokenize() generator.\n\n    It will call readline a maximum of twice, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present, but\n    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid\n    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n    'utf-8-sig' is returned.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n    encoding = None\n    default = \"utf-8\"\n\n    def read_or_stop() -> bytes:\n        try:\n            return readline()\n        except StopIteration:\n            return b\"\"\n\n    def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding\n\n    first = read_or_stop()\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n        default = \"utf-8-sig\"\n    if not first:\n        return default, []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n    if not blank_re.match(first):\n        return default, [first]\n\n    second = read_or_stop()\n    if not second:\n        return default, [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return default, [first, second]",
      "importString": "import builtins\nimport sys\nfrom collections.abc import Callable, Iterable, Iterator\nfrom re import Pattern\nfrom typing import Final, Optional, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.token import (\nASYNC\nAWAIT\nCOMMENT\nDEDENT\nENDMARKER\nERRORTOKEN\nFSTRING_END\nFSTRING_MIDDLE\nFSTRING_START\nINDENT\nLBRACE\nNAME\nNEWLINE\nNL\nNUMBER\nOP\nRBRACE\nSTRING\ntok_name\n)\nimport re\nfrom codecs import BOM_UTF8, lookup\n\nfrom . import token",
      "lineNum": 72,
      "relativeDocumentPath": "src/blib2to3/pgen2/tokenize.py"
    },
    {
      "symbolName": "find_cookie",
      "sourceCode": "def find_cookie(line: bytes) -> Optional[str]:\n        try:\n            line_string = line.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n        match = cookie_re.match(line_string)\n        if not match:\n            return None\n        encoding = _get_normal_name(match.group(1))\n        try:\n            codec = lookup(encoding)\n        except LookupError:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding)\n\n        if bom_found:\n            if codec.name != \"utf-8\":\n                # This behaviour mimics the Python interpreter\n                raise SyntaxError(\"encoding problem: utf-8\")\n            encoding += \"-sig\"\n        return encoding",
      "importString": "import builtins\nimport sys\nfrom collections.abc import Callable, Iterable, Iterator\nfrom re import Pattern\nfrom typing import Final, Optional, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.token import (\nASYNC\nAWAIT\nCOMMENT\nDEDENT\nENDMARKER\nERRORTOKEN\nFSTRING_END\nFSTRING_MIDDLE\nFSTRING_START\nINDENT\nLBRACE\nNAME\nNEWLINE\nNL\nNUMBER\nOP\nRBRACE\nSTRING\ntok_name\n)\nimport re\nfrom codecs import BOM_UTF8, lookup\n\nfrom . import token",
      "lineNum": 20,
      "relativeDocumentPath": "src/blib2to3/pgen2/tokenize.py"
    },
    {
      "symbolName": "untokenize",
      "sourceCode": "def untokenize(iterable: Iterable[TokenInfo]) -> str:\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited input:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tokin generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)",
      "importString": "import builtins\nimport sys\nfrom collections.abc import Callable, Iterable, Iterator\nfrom re import Pattern\nfrom typing import Final, Optional, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.token import (\nASYNC\nAWAIT\nCOMMENT\nDEDENT\nENDMARKER\nERRORTOKEN\nFSTRING_END\nFSTRING_MIDDLE\nFSTRING_START\nINDENT\nLBRACE\nNAME\nNEWLINE\nNL\nNUMBER\nOP\nRBRACE\nSTRING\ntok_name\n)\nimport re\nfrom codecs import BOM_UTF8, lookup\n\nfrom . import token",
      "lineNum": 19,
      "relativeDocumentPath": "src/blib2to3/pgen2/tokenize.py"
    },
    {
      "symbolName": "generate_tokens",
      "sourceCode": "def generate_tokens(\n    readline: Callable[[], str], grammar: Optional[Grammar] = None\n) -> Iterator[GoodTokenInfo]:\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    parenlev_stack: list[int] = []\n    fstring_state = FStringState()\n    formatspec = \"\"\n    numchars: Final[str] = \"0123456789\"\n    contstr, needcont = \"\", 0\n    contline: Optional[str] = None\n    indents = [0]\n\n    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n    # `await` as keywords.\n    async_keywords = False if grammar is None else grammar.async_keywords\n    # 'stashed' and 'async_*' are used for async/await parsing\n    stashed: Optional[GoodTokenInfo] = None\n    async_def = False\n    async_def_indent = 0\n    async_def_nl = False\n\n    strstart: tuple[int, int]\n    endprog_stack: list[Pattern[str]] = []\n    formatspec_start: tuple[int, int]\n\n    while 1:  # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = \"\"\n        lnum += 1\n\n        # skip lines that are just indent characters ending with a slash\n        # to avoid storing that line's indent information.\n        if not contstr and line.rstrip(\"\\n\").strip(\" \\t\\f\") == \"\\\\\":\n            continue\n\n        pos, max = 0, len(line)\n\n        if contstr:  # continued string\n            assert contline is not None\n            if not line:\n                raise TokenError(\"EOF in multi-line string\", strstart)\n            endprog = endprog_stack[-1]\n            endmatch = endprog.match(line)\n            if endmatch:\n                end = endmatch.end(0)\n                token = contstr + line[:end]\n                spos = strstart\n                epos = (lnum, end)\n                tokenline = contline + line\n                if fstring_state.current() in (\n                    STATE_NOT_FSTRING,\n                    STATE_IN_BRACES,\n                ) and not is_fstring_start(token):\n                    yield (STRING, token, spos, epos, tokenline)\n                    endprog_stack.pop()\n                    parenlev = parenlev_stack.pop()\n                else:\n                    if is_fstring_start(token):\n                        fstring_start, token = _split_fstring_start_and_middle(token)\n                        fstring_start_epos = (spos[0], spos[1] + len(fstring_start))\n                        yield (\n                            FSTRING_START,\n                            fstring_start,\n                            spos,\n                            fstring_start_epos,\n                            tokenline,\n                        )\n                        fstring_state.enter_fstring()\n                        # increase spos to the end of the fstring start\n                        spos = fstring_start_epos\n\n                    if token.endswith(\"{\"):\n                        fstring_middle, lbrace = token[:-1], token[-1]\n                        fstring_middle_epos = lbrace_spos = (lnum, end - 1)\n                        yield (\n                            FSTRING_MIDDLE,\n                            fstring_middle,\n                            spos,\n                            fstring_middle_epos,\n                            line,\n                        )\n                        yield (LBRACE, lbrace, lbrace_spos, epos, line)\n                        fstring_state.consume_lbrace()\n                    else:\n                        if token.endswith(('\"\"\"', \"'''\")):\n                            fstring_middle, fstring_end = token[:-3], token[-3:]\n                            fstring_middle_epos = end_spos = (lnum, end - 3)\n                        else:\n                            fstring_middle, fstring_end = token[:-1], token[-1]\n                            fstring_middle_epos = end_spos = (lnum, end - 1)\n                        yield (\n                            FSTRING_MIDDLE,\n                            fstring_middle,\n                            spos,\n                            fstring_middle_epos,\n                            line,\n                        )\n                        yield (\n                            FSTRING_END,\n                            fstring_end,\n                            end_spos,\n                            epos,\n                            line,\n                        )\n                        fstring_state.leave_fstring()\n                        endprog_stack.pop()\n                        parenlev = parenlev_stack.pop()\n                pos = end\n                contstr, needcont = \"\", 0\n                contline = None\n            elif needcont and line[-2:] != \"\\\\\\n\" and line[-3:] != \"\\\\\\r\\n\":\n                yield (\n                    ERRORTOKEN,\n                    contstr + line,\n                    strstart,\n                    (lnum, len(line)),\n                    contline,\n                )\n                contstr = \"\"\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        # new statement\n        elif (\n            parenlev == 0\n            and not continued\n            and not fstring_state.is_in_fstring_expression()\n        ):\n            if not line:\n                break\n            column = 0\n            while pos < max:  # measure leading whitespace\n                if line[pos] == \" \":\n                    column += 1\n                elif line[pos] == \"\\t\":\n                    column = (column // tabsize + 1) * tabsize\n                elif line[pos] == \"\\f\":\n                    column = 0\n                else:\n                    break\n                pos += 1\n            if pos == max:\n                break\n\n            if stashed:\n                yield stashed\n                stashed = None\n\n            if line[pos] in \"\\r\\n\":  # skip blank lines\n                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if line[pos] == \"#\":  # skip comments\n                comment_token = line[pos:].rstrip(\"\\r\\n\")\n                nl_pos = pos + len(comment_token)\n                yield (\n                    COMMENT,\n                    comment_token,\n                    (lnum, pos),\n                    (lnum, nl_pos),\n                    line,\n                )\n                yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:  # count indents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n\n            while column < indents[-1]:  # count dedents\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line),\n                    )\n                indents = indents[:-1]\n\n                if async_def and async_def_indent >= indents[-1]:\n                    async_def = False\n                    async_def_nl = False\n                    async_def_indent = 0\n\n                yield (DEDENT, \"\", (lnum, pos), (lnum, pos), line)\n\n            if async_def and async_def_nl and async_def_indent >= indents[-1]:\n                async_def = False\n                async_def_nl = False\n                async_def_indent = 0\n\n        else:  # continued statement\n            if not line:\n                raise TokenError(\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            if fstring_state.current() == STATE_MIDDLE:\n                endprog = endprog_stack[-1]\n                endmatch = endprog.match(line, pos)\n                if endmatch:  # all on one line\n                    start, end = endmatch.span(0)\n                    token = line[start:end]\n                    if token.endswith(('\"\"\"', \"'''\")):\n                        middle_token, end_token = token[:-3], token[-3:]\n                        middle_epos = end_spos = (lnum, end - 3)\n                    else:\n                        middle_token, end_token = token[:-1], token[-1]\n                        middle_epos = end_spos = (lnum, end - 1)\n                    # TODO: unsure if this can be safely removed\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (\n                        FSTRING_MIDDLE,\n                        middle_token,\n                        (lnum, pos),\n                        middle_epos,\n                        line,\n                    )\n                    if not token.endswith(\"{\"):\n                        yield (\n                            FSTRING_END,\n                            end_token,\n                            end_spos,\n                            (lnum, end),\n                            line,\n                        )\n                        fstring_state.leave_fstring()\n                        endprog_stack.pop()\n                        parenlev = parenlev_stack.pop()\n                    else:\n                        yield (LBRACE, \"{\", (lnum, end - 1), (lnum, end), line)\n                        fstring_state.consume_lbrace()\n                    pos = end\n                    continue\n                else:  # multiple lines\n                    strstart = (lnum, end)\n                    contstr = line[end:]\n                    contline = line\n                    break\n\n            if fstring_state.current() == STATE_IN_COLON:\n                match = fstring_middle_after_colon.match(line, pos)\n                if match is None:\n                    formatspec += line[pos:]\n                    pos = max\n                    continue\n\n                start, end = match.span(1)\n                token = line[start:end]\n                formatspec += token\n\n                brace_start, brace_end = match.span(2)\n                brace_or_nl = line[brace_start:brace_end]\n                if brace_or_nl == \"\\n\":\n                    pos = brace_end\n\n                yield (FSTRING_MIDDLE, formatspec, formatspec_start, (lnum, end), line)\n                formatspec = \"\"\n\n                if brace_or_nl == \"{\":\n                    yield (LBRACE, \"{\", (lnum, brace_start), (lnum, brace_end), line)\n                    fstring_state.consume_lbrace()\n                    end = brace_end\n                elif brace_or_nl == \"}\":\n                    yield (RBRACE, \"}\", (lnum, brace_start), (lnum, brace_end), line)\n                    fstring_state.consume_rbrace()\n                    end = brace_end\n                    formatspec_start = (lnum, brace_end)\n\n                pos = end\n                continue\n\n            if fstring_state.current() == STATE_IN_BRACES and parenlev == 0:\n                match = bang.match(line, pos)\n                if match:\n                    start, end = match.span(1)\n                    yield (OP, \"!\", (lnum, start), (lnum, end), line)\n                    pos = end\n                    continue\n\n                match = colon.match(line, pos)\n                if match:\n                    start, end = match.span(1)\n                    yield (OP, \":\", (lnum, start), (lnum, end), line)\n                    fstring_state.consume_colon()\n                    formatspec_start = (lnum, end)\n                    pos = end\n                    continue\n\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:  # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or (\n                    initial == \".\" and token != \".\"\n                ):  # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in \"\\r\\n\":\n                    newline = NEWLINE\n                    if parenlev > 0 or fstring_state.is_in_fstring_expression():\n                        newline = NL\n                    elif async_def:\n                        async_def_nl = True\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (newline, token, spos, epos, line)\n\n                elif initial == \"#\":\n                    assert not token.endswith(\"\\n\")\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endprog_stack.append(endprog)\n                    parenlev_stack.append(parenlev)\n                    parenlev = 0\n                    if is_fstring_start(token):\n                        yield (FSTRING_START, token, spos, epos, line)\n                        fstring_state.enter_fstring()\n\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:  # all on one line\n                        if stashed:\n                            yield stashed\n                            stashed = None\n                        if not is_fstring_start(token):\n                            pos = endmatch.end(0)\n                            token = line[start:pos]\n                            epos = (lnum, pos)\n                            yield (STRING, token, spos, epos, line)\n                            endprog_stack.pop()\n                            parenlev = parenlev_stack.pop()\n                        else:\n                            end = endmatch.end(0)\n                            token = line[pos:end]\n                            spos, epos = (lnum, pos), (lnum, end)\n                            if not token.endswith(\"{\"):\n                                fstring_middle, fstring_end = token[:-3], token[-3:]\n                                fstring_middle_epos = fstring_end_spos = (lnum, end - 3)\n                                yield (\n                                    FSTRING_MIDDLE,\n                                    fstring_middle,\n                                    spos,\n                                    fstring_middle_epos,\n                                    line,\n                                )\n                                yield (\n                                    FSTRING_END,\n                                    fstring_end,\n                                    fstring_end_spos,\n                                    epos,\n                                    line,\n                                )\n                                fstring_state.leave_fstring()\n                                endprog_stack.pop()\n                                parenlev = parenlev_stack.pop()\n                            else:\n                                fstring_middle, lbrace = token[:-1], token[-1]\n                                fstring_middle_epos = lbrace_spos = (lnum, end - 1)\n                                yield (\n                                    FSTRING_MIDDLE,\n                                    fstring_middle,\n                                    spos,\n                                    fstring_middle_epos,\n                                    line,\n                                )\n                                yield (LBRACE, lbrace, lbrace_spos, epos, line)\n                                fstring_state.consume_lbrace()\n                            pos = end\n                    else:\n                        # multiple lines\n                        if is_fstring_start(token):\n                            strstart = (lnum, pos)\n                            contstr = line[pos:]\n                        else:\n                            strstart = (lnum, start)\n                            contstr = line[start:]\n                        contline = line\n                        break\n                elif (\n                    initial in single_quoted\n                    or token[:2] in single_quoted\n                    or token[:3] in single_quoted\n                ):\n                    maybe_endprog = (\n                        endprogs.get(initial)\n                        or endprogs.get(token[:2])\n                        or endprogs.get(token[:3])\n                    )\n                    assert maybe_endprog is not None, f\"endprog not found for {token}\"\n                    endprog = maybe_endprog\n                    if token[-1] == \"\\n\":  # continued string\n                        endprog_stack.append(endprog)\n                        parenlev_stack.append(parenlev)\n                        parenlev = 0\n                        strstart = (lnum, start)\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:  # ordinary string\n                        if stashed:\n                            yield stashed\n                            stashed = None\n\n                        if not is_fstring_start(token):\n                            yield (STRING, token, spos, epos, line)\n                        else:\n                            if pseudomatch[20] is not None:\n                                fstring_start = pseudomatch[20]\n                                offset = pseudomatch.end(20) - pseudomatch.start(1)\n                            elif pseudomatch[22] is not None:\n                                fstring_start = pseudomatch[22]\n                                offset = pseudomatch.end(22) - pseudomatch.start(1)\n                            elif pseudomatch[24] is not None:\n                                fstring_start = pseudomatch[24]\n                                offset = pseudomatch.end(24) - pseudomatch.start(1)\n                            else:\n                                fstring_start = pseudomatch[26]\n                                offset = pseudomatch.end(26) - pseudomatch.start(1)\n\n                            start_epos = (lnum, start + offset)\n                            yield (FSTRING_START, fstring_start, spos, start_epos, line)\n                            fstring_state.enter_fstring()\n                            endprog = endprogs[fstring_start]\n                            endprog_stack.append(endprog)\n                            parenlev_stack.append(parenlev)\n                            parenlev = 0\n\n                            end_offset = pseudomatch.end(1) - 1\n                            fstring_middle = line[start + offset : end_offset]\n                            middle_spos = (lnum, start + offset)\n                            middle_epos = (lnum, end_offset)\n                            yield (\n                                FSTRING_MIDDLE,\n                                fstring_middle,\n                                middle_spos,\n                                middle_epos,\n                                line,\n                            )\n                            if not token.endswith(\"{\"):\n                                end_spos = (lnum, end_offset)\n                                end_epos = (lnum, end_offset + 1)\n                                yield (FSTRING_END, token[-1], end_spos, end_epos, line)\n                                fstring_state.leave_fstring()\n                                endprog_stack.pop()\n                                parenlev = parenlev_stack.pop()\n                            else:\n                                end_spos = (lnum, end_offset)\n                                end_epos = (lnum, end_offset + 1)\n                                yield (LBRACE, \"{\", end_spos, end_epos, line)\n                                fstring_state.consume_lbrace()\n\n                elif initial.isidentifier():  # ordinary name\n                    if token in (\"async\", \"await\"):\n                        if async_keywords or async_def:\n                            yield (\n                                ASYNC if token == \"async\" else AWAIT,\n                                token,\n                                spos,\n                                epos,\n                                line,\n                            )\n                            continue\n\n                    tok = (NAME, token, spos, epos, line)\n                    if token == \"async\" and not stashed:\n                        stashed = tok\n                        continue\n\n                    if token in (\"def\", \"for\"):\n                        if stashed and stashed[0] == NAME and stashed[1] == \"async\":\n                            if token == \"def\":\n                                async_def = True\n                                async_def_indent = indents[-1]\n\n                            yield (\n                                ASYNC,\n                                stashed[1],\n                                stashed[2],\n                                stashed[3],\n                                stashed[4],\n                            )\n                            stashed = None\n\n                    if stashed:\n                        yield stashed\n                        stashed = None\n\n                    yield tok\n                elif initial == \"\\\\\":  # continued stmt\n                    # This yield is new; needed for better idempotency:\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (NL, token, spos, (lnum, pos), line)\n                    continued = 1\n                elif (\n                    initial == \"}\"\n                    and parenlev == 0\n                    and fstring_state.is_in_fstring_expression()\n                ):\n                    yield (RBRACE, token, spos, epos, line)\n                    fstring_state.consume_rbrace()\n                    formatspec_start = epos\n                else:\n                    if initial in \"([{\":\n                        parenlev += 1\n                    elif initial in \")]}\":\n                        parenlev -= 1\n                    if stashed:\n                        yield stashed\n                        stashed = None\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)\n                pos += 1\n\n    if stashed:\n        yield stashed\n        stashed = None\n\n    for _indent in indents[1:]:  # pop remaining indent levels\n        yield (DEDENT, \"\", (lnum, 0), (lnum, 0), \"\")\n    yield (ENDMARKER, \"\", (lnum, 0), (lnum, 0), \"\")\n    assert len(endprog_stack) == 0\n    assert len(parenlev_stack) == 0",
      "importString": "import builtins\nimport sys\nfrom collections.abc import Callable, Iterable, Iterator\nfrom re import Pattern\nfrom typing import Final, Optional, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nfrom blib2to3.pgen2.token import (\nASYNC\nAWAIT\nCOMMENT\nDEDENT\nENDMARKER\nERRORTOKEN\nFSTRING_END\nFSTRING_MIDDLE\nFSTRING_START\nINDENT\nLBRACE\nNAME\nNEWLINE\nNL\nNUMBER\nOP\nRBRACE\nSTRING\ntok_name\n)\nimport re\nfrom codecs import BOM_UTF8, lookup\n\nfrom . import token",
      "lineNum": 551,
      "relativeDocumentPath": "src/blib2to3/pgen2/tokenize.py"
    },
    {
      "symbolName": "initialize",
      "sourceCode": "def initialize(cache_dir: Union[str, \"os.PathLike[str]\", None] = None) -> None:\n    global python_grammar\n    global python_grammar_async_keywords\n    global python_grammar_soft_keywords\n    global python_symbols\n    global pattern_grammar\n    global pattern_symbols\n\n    # The grammar file\n    _GRAMMAR_FILE = os.path.join(os.path.dirname(__file__), \"Grammar.txt\")\n    _PATTERN_GRAMMAR_FILE = os.path.join(\n        os.path.dirname(__file__), \"PatternGrammar.txt\"\n    )\n\n    python_grammar = driver.load_packaged_grammar(\"blib2to3\", _GRAMMAR_FILE, cache_dir)\n    assert \"print\" not in python_grammar.keywords\n    assert \"exec\" not in python_grammar.keywords\n\n    soft_keywords = python_grammar.soft_keywords.copy()\n    python_grammar.soft_keywords.clear()\n\n    python_symbols = _python_symbols(python_grammar)\n\n    # Python 3.0-3.6\n    python_grammar.version = (3, 0)\n\n    # Python 3.7+\n    python_grammar_async_keywords = python_grammar.copy()\n    python_grammar_async_keywords.async_keywords = True\n    python_grammar_async_keywords.version = (3, 7)\n\n    # Python 3.10+\n    python_grammar_soft_keywords = python_grammar_async_keywords.copy()\n    python_grammar_soft_keywords.soft_keywords = soft_keywords\n    python_grammar_soft_keywords.version = (3, 10)\n\n    pattern_grammar = driver.load_packaged_grammar(\n        \"blib2to3\", _PATTERN_GRAMMAR_FILE, cache_dir\n    )\n    pattern_symbols = _pattern_symbols(pattern_grammar)",
      "importString": "import os\nfrom typing import Union\nfrom .pgen2 import driver\nfrom .pgen2.grammar import Grammar",
      "lineNum": 39,
      "relativeDocumentPath": "src/blib2to3/pygram.py"
    },
    {
      "symbolName": "type_repr",
      "sourceCode": "def type_repr(type_num: int) -> Union[str, int]:\n    global _type_reprs\n    if not _type_reprs:\n        from . import pygram\n\n        if not hasattr(pygram, \"python_symbols\"):\n            pygram.initialize(cache_dir=None)\n\n        # printing tokens is possible but not as useful\n        # from .pgen2 import token // token.__dict__.items():\n        for name in dir(pygram.python_symbols):\n            val = getattr(pygram.python_symbols, name)\n            if type(val) == int:\n                _type_reprs[val] = name\n    return _type_reprs.setdefault(type_num, type_num)",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 14,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "replace",
      "sourceCode": "def replace(self, new: Union[NL, list[NL]]) -> None:\n        \"\"\"Replace this node with a new one in the parent.\"\"\"\n        assert self.parent is not None, str(self)\n        assert new is not None\n        if not isinstance(new, list):\n            new = [new]\n        l_children = []\n        found = False\n        for ch in self.parent.children:\n            if ch is self:\n                assert not found, (self.parent.children, self, new)\n                if new is not None:\n                    l_children.extend(new)\n                found = True\n            else:\n                l_children.append(ch)\n        assert found, (self.children, self, new)\n        self.parent.children = l_children\n        self.parent.changed()\n        self.parent.invalidate_sibling_maps()\n        for x in new:\n            x.parent = self.parent\n        self.parent = None",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 22,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "remove",
      "sourceCode": "def remove(self) -> Optional[int]:\n        \"\"\"\n        Remove the node from the tree. Returns the position of the node in its\n        parent's children before it was removed.\n        \"\"\"\n        if self.parent:\n            for i, node in enumerate(self.parent.children):\n                if node is self:\n                    del self.parent.children[i]\n                    self.parent.changed()\n                    self.parent.invalidate_sibling_maps()\n                    self.parent = None\n                    return i\n        return None",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 13,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "next_sibling",
      "sourceCode": "@property\n    def next_sibling(self) -> Optional[NL]:\n        \"\"\"\n        The node immediately following the invocant in their parent's children\n        list. If the invocant does not have a next sibling, it is None\n        \"\"\"\n        if self.parent is None:\n            return None\n\n        if self.parent.next_sibling_map is None:\n            self.parent.update_sibling_maps()\n        assert self.parent.next_sibling_map is not None\n        return self.parent.next_sibling_map[id(self)]",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 12,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "prev_sibling",
      "sourceCode": "@property\n    def prev_sibling(self) -> Optional[NL]:\n        \"\"\"\n        The node immediately preceding the invocant in their parent's children\n        list. If the invocant does not have a previous sibling, it is None.\n        \"\"\"\n        if self.parent is None:\n            return None\n\n        if self.parent.prev_sibling_map is None:\n            self.parent.update_sibling_maps()\n        assert self.parent.prev_sibling_map is not None\n        return self.parent.prev_sibling_map[id(self)]",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 12,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "__init__",
      "sourceCode": "def __init__(\n        self,\n        type: int,\n        children: list[NL],\n        context: Optional[Any] = None,\n        prefix: Optional[str] = None,\n        fixers_applied: Optional[list[Any]] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Takes a type constant (a symbol number >= 256), a sequence of\n        child nodes, and an optional context keyword argument.\n\n        As a side effect, the parent pointers of the children are updated.\n        \"\"\"\n        assert type >= 256, type\n        self.type = type\n        self.children = list(children)\n        for ch in self.children:\n            assert ch.parent is None, repr(ch)\n            ch.parent = self\n        self.invalidate_sibling_maps()\n        if prefix is not None:\n            self.prefix = prefix\n        if fixers_applied:\n            self.fixers_applied = fixers_applied[:]\n        else:\n            self.fixers_applied = None",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 28,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "update_sibling_maps",
      "sourceCode": "def update_sibling_maps(self) -> None:\n        _prev: dict[int, Optional[NL]] = {}\n        _next: dict[int, Optional[NL]] = {}\n        self.prev_sibling_map = _prev\n        self.next_sibling_map = _next\n        previous: Optional[NL] = None\n        for current in self.children:\n            _prev[id(current)] = previous\n            _next[id(previous)] = current\n            previous = current\n        _next[id(current)] = None",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 10,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "__init__",
      "sourceCode": "def __init__(\n        self,\n        type: int,\n        value: str,\n        context: Optional[Context] = None,\n        prefix: Optional[str] = None,\n        fixers_applied: list[Any] = [],\n        opening_bracket: Optional[\"Leaf\"] = None,\n        fmt_pass_converted_first_leaf: Optional[\"Leaf\"] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Takes a type constant (a token number < 256), a string value, and an\n        optional context keyword argument.\n        \"\"\"\n\n        assert 0 <= type < 256, type\n        if context is not None:\n            self._prefix, (self.lineno, self.column) = context\n        self.type = type\n        self.value = value\n        if prefix is not None:\n            self._prefix = prefix\n        self.fixers_applied: Optional[list[Any]] = fixers_applied[:]\n        self.children = []\n        self.opening_bracket = opening_bracket\n        self.fmt_pass_converted_first_leaf = fmt_pass_converted_first_leaf",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 27,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "convert",
      "sourceCode": "def convert(gr: Grammar, raw_node: RawNode) -> NL:\n    \"\"\"\n    Convert raw node information to a Node or Leaf instance.\n\n    This is passed to the parser driver which calls it whenever a reduction of a\n    grammar rule produces a new complete node, so that the tree is build\n    strictly bottom-up.\n    \"\"\"\n    type, value, context, children = raw_node\n    if children or type in gr.number2symbol:\n        # If there's exactly one child, return that child instead of\n        # creating a new node.\n        assert children is not None\n        if len(children) == 1:\n            return children[0]\n        return Node(type, children, context=context)\n    else:\n        return Leaf(type, value or \"\", context=context)",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 17,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "match",
      "sourceCode": "def match(self, node: NL, results: Optional[_Results] = None) -> bool:\n        \"\"\"\n        Does this pattern exactly match a node?\n\n        Returns True if it matches, False if not.\n\n        If results is not None, it must be a dict which will be\n        updated with the nodes matching named subpatterns.\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n        if self.type is not None and node.type != self.type:\n            return False\n        if self.content is not None:\n            r: Optional[_Results] = None\n            if results is not None:\n                r = {}\n            if not self._submatch(node, r):\n                return False\n            if r:\n                assert results is not None\n                results.update(r)\n        if results is not None and self.name:\n            results[self.name] = node\n        return True",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 24,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "__init__",
      "sourceCode": "def __init__(\n        self,\n        type: Optional[int] = None,\n        content: Optional[str] = None,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.  Takes optional type, content, and name.\n\n        The type, if given must be a token type (< 256).  If not given,\n        this matches any *leaf* node; the content may still be required.\n\n        The content, if given, must be a string.\n\n        If a name is given, the matching node is stored in the results\n        dict under that key.\n        \"\"\"\n        if type is not None:\n            assert 0 <= type < 256, type\n        if content is not None:\n            assert isinstance(content, str), repr(content)\n        self.type = type\n        self.content = content\n        self.name = name",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 23,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "_submatch",
      "sourceCode": "def _submatch(self, node, results=None):\n        \"\"\"\n        Match the pattern's content to the node's children.\n\n        This assumes the node type matches and self.content is not None.\n\n        Returns True if it matches, False if not.\n\n        If results is not None, it must be a dict which will be\n        updated with the nodes matching named subpatterns.\n\n        When returning False, the results dict may still be updated.\n        \"\"\"\n        return self.content == node.value",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 13,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "__init__",
      "sourceCode": "def __init__(\n        self,\n        type: Optional[int] = None,\n        content: Optional[Iterable[str]] = None,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.  Takes optional type, content, and name.\n\n        The type, if given, must be a symbol type (>= 256).  If the\n        type is None this matches *any* single node (leaf or not),\n        except if content is not None, in which it only matches\n        non-leaf nodes that also match the content pattern.\n\n        The content, if not None, must be a sequence of Patterns that\n        must match the node's children exactly.  If the content is\n        given, the type must not be None.\n\n        If a name is given, the matching node is stored in the results\n        dict under that key.\n        \"\"\"\n        if type is not None:\n            assert type >= 256, type\n        if content is not None:\n            assert not isinstance(content, str), repr(content)\n            newcontent = list(content)\n            for i, item in enumerate(newcontent):\n                assert isinstance(item, BasePattern), (i, item)\n                # I don't even think this code is used anywhere, but it does cause\n                # unreachable errors from mypy. This function's signature does look\n                # odd though *shrug*.\n                if isinstance(item, WildcardPattern):  # type: ignore[unreachable]\n                    self.wildcards = True  # type: ignore[unreachable]\n        self.type = type\n        self.content = newcontent  # TODO: this is unbound when content is None\n        self.name = name",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 35,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "_submatch",
      "sourceCode": "def _submatch(self, node, results=None) -> bool:\n        \"\"\"\n        Match the pattern's content to the node's children.\n\n        This assumes the node type matches and self.content is not None.\n\n        Returns True if it matches, False if not.\n\n        If results is not None, it must be a dict which will be\n        updated with the nodes matching named subpatterns.\n\n        When returning False, the results dict may still be updated.\n        \"\"\"\n        if self.wildcards:\n            for c, r in generate_matches(self.content, node.children):\n                if c == len(node.children):\n                    if results is not None:\n                        results.update(r)\n                    return True\n            return False\n        if len(self.content) != len(node.children):\n            return False\n        for subpattern, child in zip(self.content, node.children):\n            if not subpattern.match(child, results):\n                return False\n        return True",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 25,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "__init__",
      "sourceCode": "def __init__(\n        self,\n        content: Optional[str] = None,\n        min: int = 0,\n        max: int = HUGE,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Initializer.\n\n        Args:\n            content: optional sequence of subsequences of patterns;\n                     if absent, matches one node;\n                     if present, each subsequence is an alternative [*]\n            min: optional minimum number of times to match, default 0\n            max: optional maximum number of times to match, default HUGE\n            name: optional name assigned to this match\n\n        [*] Thus, if content is [[a, b, c], [d, e], [f, g, h]] this is\n            equivalent to (a b c | d e | f g h); if content is None,\n            this is equivalent to '.' in regular expression terms.\n            The min and max parameters work as follows:\n                min=0, max=maxint: .*\n                min=1, max=maxint: .+\n                min=0, max=1: .?\n                min=1, max=1: .\n            If content is not None, replace the dot with the parenthesized\n            list of alternatives, e.g. (a b c | d e | f g h)*\n        \"\"\"\n        assert 0 <= min <= max <= HUGE, (min, max)\n        if content is not None:\n            f = lambda s: tuple(s)\n            wrapped_content = tuple(map(f, content))  # Protect against alterations\n            # Check sanity of alternatives\n            assert len(wrapped_content), repr(\n                wrapped_content\n            )  # Can't have zero alternatives\n            for alt in wrapped_content:\n                assert len(alt), repr(alt)  # Can have empty alternatives\n        self.content = wrapped_content\n        self.min = min\n        self.max = max\n        self.name = name",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 42,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "optimize",
      "sourceCode": "def optimize(self) -> Any:\n        \"\"\"Optimize certain stacked wildcard patterns.\"\"\"\n        subpattern = None\n        if (\n            self.content is not None\n            and len(self.content) == 1\n            and len(self.content[0]) == 1\n        ):\n            subpattern = self.content[0][0]\n        if self.min == 1 and self.max == 1:\n            if self.content is None:\n                return NodePattern(name=self.name)\n            if subpattern is not None and self.name == subpattern.name:\n                return subpattern.optimize()\n        if (\n            self.min <= 1\n            and isinstance(subpattern, WildcardPattern)\n            and subpattern.min <= 1\n            and self.name == subpattern.name\n        ):\n            return WildcardPattern(\n                subpattern.content,\n                self.min * subpattern.min,\n                self.max * subpattern.max,\n                subpattern.name,\n            )\n        return self",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 26,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "generate_matches",
      "sourceCode": "def generate_matches(self, nodes) -> Iterator[tuple[int, _Results]]:\n        \"\"\"\n        Generator yielding matches for a sequence of nodes.\n\n        Args:\n            nodes: sequence of nodes\n\n        Yields:\n            (count, results) tuples where:\n            count: the match comprises nodes[:count];\n            results: dict containing named submatches.\n        \"\"\"\n        if self.content is None:\n            # Shortcut for special case (see __init__.__doc__)\n            for count in range(self.min, 1 + min(len(nodes), self.max)):\n                r = {}\n                if self.name:\n                    r[self.name] = nodes[:count]\n                yield count, r\n        elif self.name == \"bare_name\":\n            yield self._bare_name_matches(nodes)\n        else:\n            # The reason for this is that hitting the recursion limit usually\n            # results in some ugly messages about how RuntimeErrors are being\n            # ignored. We only have to do this on CPython, though, because other\n            # implementations don't have this nasty bug in the first place.\n            if hasattr(sys, \"getrefcount\"):\n                save_stderr = sys.stderr\n                sys.stderr = StringIO()\n            try:\n                for count, r in self._recursive_matches(nodes, 0):\n                    if self.name:\n                        r[self.name] = nodes[:count]\n                    yield count, r\n            except RuntimeError:\n                # We fall back to the iterative pattern matching scheme if the recursive\n                # scheme hits the recursion limit.\n                for count, r in self._iterative_matches(nodes):\n                    if self.name:\n                        r[self.name] = nodes[:count]\n                    yield count, r\n            finally:\n                if hasattr(sys, \"getrefcount\"):\n                    sys.stderr = save_stderr",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 43,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "_iterative_matches",
      "sourceCode": "def _iterative_matches(self, nodes) -> Iterator[tuple[int, _Results]]:\n        \"\"\"Helper to iteratively yield the matches.\"\"\"\n        nodelen = len(nodes)\n        if 0 >= self.min:\n            yield 0, {}\n\n        results = []\n        # generate matches that use just one alt from self.content\n        for alt in self.content:\n            for c, r in generate_matches(alt, nodes):\n                yield c, r\n                results.append((c, r))\n\n        # for each match, iterate down the nodes\n        while results:\n            new_results = []\n            for c0, r0 in results:\n                # stop if the entire set of nodes has been matched\n                if c0 < nodelen and c0 <= self.max:\n                    for alt in self.content:\n                        for c1, r1 in generate_matches(alt, nodes[c0:]):\n                            if c1 > 0:\n                                r = {}\n                                r.update(r0)\n                                r.update(r1)\n                                yield c0 + c1, r\n                                new_results.append((c0 + c1, r))\n            results = new_results",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 27,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "_bare_name_matches",
      "sourceCode": "def _bare_name_matches(self, nodes) -> tuple[int, _Results]:\n        \"\"\"Special optimized matcher for bare_name.\"\"\"\n        count = 0\n        r = {}  # type: _Results\n        done = False\n        max = len(nodes)\n        while not done and count < max:\n            done = True\n            for leaf in self.content:\n                if leaf[0].match(nodes[count], r):\n                    count += 1\n                    done = False\n                    break\n        assert self.name is not None\n        r[self.name] = nodes[:count]\n        return count, r",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 15,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "_recursive_matches",
      "sourceCode": "def _recursive_matches(self, nodes, count) -> Iterator[tuple[int, _Results]]:\n        \"\"\"Helper to recursively yield the matches.\"\"\"\n        assert self.content is not None\n        if count >= self.min:\n            yield 0, {}\n        if count < self.max:\n            for alt in self.content:\n                for c0, r0 in generate_matches(alt, nodes):\n                    for c1, r1 in self._recursive_matches(nodes[c0:], count + 1):\n                        r = {}\n                        r.update(r0)\n                        r.update(r1)\n                        yield c0 + c1, r",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 12,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "__init__",
      "sourceCode": "def __init__(self, content: Optional[BasePattern] = None) -> None:\n        \"\"\"\n        Initializer.\n\n        The argument is either a pattern or None.  If it is None, this\n        only matches an empty sequence (effectively '$' in regex\n        lingo).  If it is not None, this matches whenever the argument\n        pattern doesn't have any matches.\n        \"\"\"\n        if content is not None:\n            assert isinstance(content, BasePattern), repr(content)\n        self.content = content",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 11,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    },
    {
      "symbolName": "generate_matches",
      "sourceCode": "def generate_matches(\n    patterns: list[BasePattern], nodes: list[NL]\n) -> Iterator[tuple[int, _Results]]:\n    \"\"\"\n    Generator yielding matches for a sequence of patterns and nodes.\n\n    Args:\n        patterns: a sequence of patterns\n        nodes: a sequence of nodes\n\n    Yields:\n        (count, results) tuples where:\n        count: the entire sequence of patterns matches nodes[:count];\n        results: dict containing named submatches.\n    \"\"\"\n    if not patterns:\n        yield 0, {}\n    else:\n        p, rest = patterns[0], patterns[1:]\n        for c0, r0 in p.generate_matches(nodes):\n            if not rest:\n                yield c0, r0\n            else:\n                for c1, r1 in generate_matches(rest, nodes[c0:]):\n                    r = {}\n                    r.update(r0)\n                    r.update(r1)\n                    yield c0 + c1, r",
      "importString": "from collections.abc import Iterable, Iterator\nfrom typing import Any, Optional, TypeVar, Union\n\nfrom blib2to3.pgen2.grammar import Grammar\nimport sys\nfrom io import StringIO",
      "lineNum": 27,
      "relativeDocumentPath": "src/blib2to3/pytree.py"
    }
  ]